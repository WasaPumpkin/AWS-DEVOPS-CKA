<!DOCTYPE html>
<html lang="en">

<head>
  <title>Search in Questions</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 20px;
      background-color: #f5f5f5;
    }

    .container {
      background: #fff;
      padding: 20px;
      border-radius: 10px;
      width: 100%;
      margin: 0 auto;
    }

    .search-container {
      margin-bottom: 20px;
      width: 100%;
      max-width: 1900px;
      position: relative;
    }

    .search-bar {
      width: 100%;
      padding: 10px;
      padding-right: 70px;
      /* Space for buttons */
      font-size: 16px;
      border: 1px solid #ccc;
      border-radius: 5px;
      box-sizing: border-box;
    }

    .search-buttons {
      position: absolute;
      right: 10px;
      top: 50%;
      transform: translateY(-50%);
      display: flex;
      gap: 5px;
    }

    .search-button {
      background: #ddd;
      border: none;
      border-radius: 3px;
      padding: 3px 8px;
      cursor: pointer;
      font-size: 12px;
    }

    .search-button:hover {
      background: #ccc;
    }

    .question-block, .question-form {
      margin-bottom: 20px;
      padding: 15px;
      border: 1px solid #eee;
      border-radius: 5px;
    }

    .question {
      color: #333;
      margin: 0;
      font-size: 30px;
      background-color: yellow;
      padding: 10px;
    }

    img {
      width: 40%;
      height: auto;
      border-radius: 10px;
      margin: 10px;
    }

    .question-form {
      margin-top: 10px;
    }
    
    /* Remove redundant styling from form if it's inside a question-block */
    .question-block .question-form {
        margin-top: 10px;
        padding: 0;
        border: none;
    }

    .question-form label {
      display: block;
      margin: 5px 0;
      padding: 10px;
      border-radius: 5px;
      background-color: #f9f9f9;
      transition: background-color 0.3s;
    }

    .correct-answer {
      font-weight: bold;
      color: green;
      background-color: #e0ffe0;
      border: 1px solid green;
    }

    .scroll-top {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background: #333;
      color: white;
      border: none;
      border-radius: 50%;
      width: 50px;
      height: 50px;
      font-size: 20px;
      cursor: pointer;
      display: none;
      z-index: 99;
    }

    .scroll-top:hover {
      background: #555;
    }
  </style>
</head>

<body>
  <div class="container">
    <div class="search-container">
      <input type="text" id="search" class="search-bar" placeholder="Search questions...">
      <div class="search-buttons">
        <button class="search-button" id="clear-search">X</button>
      </div>
    </div>

    <form class="question-form">
  <h1 class="question">
    What is a possible reason you would need to edit claims issued in a SAML token?
  </h1>

  <label for="optionA">A. The NameIdentifier claim must be the same as the username stored in AD..</label>
  <label for="optionB">B. Authentication fails consistently.</label>
  <label for="optionC">C. The NameIdentifier claim cannot be the same as the claim URI.</label>
  <label for="optionD" class="correct-answer">D. The NameIdentifier claim cannot be the same as the username stored in AD ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON format in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to perform the analysis with minimal changes to the existing architecture.<br><br>
    What should the solutions architect do to meet these requirements with the LEAST amount of operational overhead?
  </h1>

  <label for="optionA">A. Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.</label>
  <label for="optionB">B. Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon CloudWatch console.</label>
  <label for="optionC" class="correct-answer">C. Use Amazon Athena directly with Amazon S3 to run the queries as needed. ✓✓</label>
  <label for="optionD">D. Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company uses AWS Organizations to manage multiple AWS accounts for different departments. 
    The management account has an Amazon S3 bucket that contains project reports. 
    The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.
    Which solution meets these requirements with the LEAST amount of operational overhead?
  </h1>

  <label for="optionA" class="correct-answer">
    A. Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy. ✓✓
  </label>
  <label for="optionB">
    B. Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.
  </label>
  <label for="optionC">
    C. Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.
  </label>
  <label for="optionD">
    D. Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    An application runs on an Amazon EC2 instance in a VPC. The application processes logs that are stored in an Amazon S3 bucket. 
    The EC2 instance needs to access the S3 bucket without connectivity to the internet.
    Which solution will provide private network connectivity to Amazon S3?
  </h1>

  <label for="optionA" class="correct-answer">
    A. Create a gateway VPC endpoint to the S3 bucket. ✓✓
  </label>
  <label for="optionB">
    B. Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.
  </label>
  <label for="optionC">
    C. Create an instance profile on Amazon EC2 to allow S3 access.
  </label>
  <label for="optionD">
    D. Create an Amazon API Gateway API with a private link to access the S3 endpoint.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon EBS volume. For better scalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind an Application Load Balancer. After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or the other, but never all of the documents at the same time.
  </h1>
  <h1 class="question">
    What should a solutions architect propose to ensure users see all of their documents at once?
  </h1>

  <label for="optionA">
    A. Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS. 
  </label><br>

  <label for="optionB">
    B. Configure the Application Load Balancer to direct a user to the server with the documents 
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Copy the data so both EBS volumes contain all the documents ✓✓
  </label><br>

  <label for="optionD">
    D. Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as soon as possible while using the least possible network bandwidth. 
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">
    A. Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the AWS CLI to copy all files locally to the S3 bucket.
  </label>
  <label for="optionB" class="correct-answer">
    B. Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3 File Gateway on premises. Create a public virtual interface (VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway. ✓✓
  </label>
  <label for="optionC">
    C. Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.
  </label>
  <label for="optionD">
    D.  Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device so that AWS can import the data into Amazon S3.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has an application that ingests incoming messages. Dozens of other applications and microservices then quickly consume these messages. The number of messages varies drastically and sometimes increases suddenly to 100,000 each second. The company wants to decouple the solution and increase scalability.
    Which solution meets these requirements?
  </h1>

  <label for="optionA">
    A. Persist the messages to Amazon Kinesis Data Analytics. Configure the consumer applications to read and process the messages.
  </label>
  <label for="optionB">
    B. Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale the number of EC2 instances based on CPU metrics.
  </label>
  <label for="optionC">
    C. Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to preprocess messages and store them in Amazon DynamoDB. Configure the consumer applications to read from DynamoDB to process the messages.
  </label>
  <label for="optionD" class="correct-answer">
    D. Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SQS) subscriptions. Configure the consumer applications to process the messages from the queues. ✓✓
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is migrating a distributed application to AWS. The application serves variable workloads. The legacy platform consists of a primary server that coordinates jobs across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resiliency and scalability.
    How should a solutions architect design the architecture to meet these requirements?
  </h1>

  <label for="optionA">
    A. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.
  </label>
  <label for="optionB" class="correct-answer">
    B. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue. ✓✓
  </label>
  <label for="optionC">
    C. Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure AWS CloudTrail as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the primary server.
  </label>
  <label for="optionD">
    D. Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure Amazon EventBridge (Amazon CloudWatch Events) as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed.
    The total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">
    A. Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.
  </label>
  <label for="optionB" class="correct-answer">
    B. Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days. ✓✓
  </label>
  <label for="optionC">
    C. Create an Amazon FSx for Windows File Server file system to extend the company's storage space.
  </label>
  <label for="optionD">
    D. Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is building an ecommerce web application on AWS. The application sends information about new orders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are processed in the order that they are received.
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">
    A. Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to the topic to perform processing.
  </label>
  <label for="optionB" class="correct-answer">
    B. Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing. ✓✓
  </label>
  <label for="optionC">
    C. Use an API Gateway authorizer to block any requests while the application processes an order.
  </label>
  <label for="optionD">
    D. Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the application receives an order. Configure the SQS standard queue to invoke an AWS Lambda function for processing.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The EC2 instances connect to the database by using user names and passwords that are stored locally in a file. The company wants to minimize the operational overhead of credential management.
    What should a solutions architect do to accomplish this goal?
  </h1>

  <label for="optionA" class="correct-answer">
    A. Use AWS Secrets Manager. Turn on automatic rotation. ✓✓
  </label>
  <label for="optionB">
    B. Use AWS Systems Manager Parameter Store. Turn on automatic rotation.
  </label>
  <label for="optionC">
    C. Create an Amazon S3 bucket to store objects that are encrypted with an AWS Key Management Service (AWS KMS) encryption key. Migrate the credential file to the S3 bucket. Point the application to the S3 bucket.
  </label>
  <label for="optionD">
    D. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume for each EC2 instance. Attach the new EBS volume to each EC2 instance. Migrate the credential file to the new EBS volume. Point the application to the new EBS volume.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has static data and dynamic data. The company stores its static data in an Amazon S3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic data. The company is using its own domain name registered with Amazon Route 53.
    What should a solutions architect do to meet these requirements?
  </h1>

  <label for="optionA" class="correct-answer">
    A. Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure Route 53 to route traffic to the CloudFront distribution. ✓✓
  </label>
  <label for="optionB">
    B. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint Configure Route 53 to route traffic to the CloudFront distribution.
  </label>
  <label for="optionC">
    C. Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for the web application.
  </label>
  <label for="optionD">
    D. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNS name for static content. Use the domain names as endpoints for the web application.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the company needs to rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS Regions.
    Which solution will meet these requirements with the LEAST operational overhead?
  </h1>

  <label for="optionA" class="correct-answer">
    A. Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the required Regions. Configure Secrets Manager to rotate the secrets on a schedule. ✓✓
  </label>
  <label for="optionB">
    B. Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter. Use multi-Region secret replication for the required Regions. Configure Systems Manager to rotate the secrets on a schedule.
  </label>
  <label for="optionC">
    C. Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke an AWS Lambda function to rotate the credentials.
  </label>
  <label for="optionD">
    D. Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Store the secrets in an Amazon DynamoDB global table. Use an AWS Lambda function to retrieve the secrets from DynamoDB. Use the RDS API to rotate the secrets.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a large EC2 instance.
    The database's performance degrades quickly as application load increases. The application handles more read requests than write transactions. The company wants a solution that will automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability.
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">
    A. Use Amazon Redshift with a single node for leader and compute functionality.
  </label>
  <label for="optionB">
    B. Use Amazon RDS with a Single-AZ deployment. Configure Amazon RDS to add reader instances in a different Availability Zone.
  </label>
  <label for="optionC" class="correct-answer">
    C. Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas. ✓✓
  </label>
  <label for="optionD">
    D. Use Amazon ElastiCache for Memcached with EC2 Spot Instances.
  </label>
</form>
 <form class="question-form">
        <h1 class="question">
            A company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in and out of the production VPC. The company had an inspection server in its on-premises data center. The inspection server performed specific operations such as traffic flow inspection and traffic filtering. The company wants to have the same functionalities in the AWS Cloud.
            <br><br>
            Which solution will meet these requirements?
        </h1>
        
        <label for="optionA">
            A. Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC.
        </label>
        
        <label for="optionB">
            B. Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.
        </label>
        
        <label for="optionC" class="correct-answer">
            C. Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC.
            <span class="checkmark">✓✓</span>
        </label>
        
        <label for="optionD">
            D. Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for the production VPC.
        </label>
    </form>
    <form class="question-form">
  <h1 class="question">
    A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.
    <br><br>
    Which solution will meet these requirements?
  </h1>
  
  <label for="optionA" class="correct-answer">
    A. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.
    <span class="checkmark">✓✓</span>
  </label>
  
  <label for="optionB">
    B. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.
  </label>
  
  <label for="optionC">
    C. Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.
  </label>
  
  <label for="optionD">
    D. Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is implementing a new business application. The application runs on two Amazon EC2 instances and uses an Amazon S3 bucket for document storage. A solutions architect needs to ensure that the EC2 instances can access the S3 bucket.
    <br><br>
    What should the solutions architect do to meet this requirement?
  </h1>
  
  <label for="optionA" class="correct-answer">
    A. Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.
    <span class="checkmark">✓✓</span>
  </label>
  
  <label for="optionB">
    B. Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.
  </label>
  
  <label for="optionC">
    C. Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.
  </label>
  
  <label for="optionD">
    D. Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    An application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the web interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in its compressed form in a different S3 bucket.
    <br><br>
    A solutions architect needs to design a solution that uses durable, stateless components to process the images automatically.
    <br><br>
    Which combination of actions will meet these requirements? (Choose two.)
  </h1>
  
  <label for="optionA" class="correct-answer">
    A. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.
    <span class="checkmark">✓✓</span>
  </label>
  
  <label for="optionB" class="correct-answer">
    B. Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.
    <span class="checkmark">✓✓</span>
  </label>
  
  <label for="optionC">
    C. Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, write the file name to a text file in memory and use the file to keep track of the images that were processed.
  </label>
  
  <label for="optionD">
    D. Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.
  </label>
  
  <label for="optionE">
    E. Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon ample Notification Service (Amazon SNS) topic with the application owner's email address for further processing.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets.
    <br><br>
    A solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.
    <br><br>
    Which solution will meet these requirements with the LEAST operational overhead?
  </h1>
  
  <label for="optionA">
    A. Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.
  </label>
  
  <label for="optionB">
    B. Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.
  </label>
  
  <label for="optionC">
    C. Deploy a transit gateway in the inspection VPC. Configure route tables to route the incoming packets through the transit gateway.
  </label>
  
  <label for="optionD" class="correct-answer">
    D. Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.
    <span class="checkmark">✓✓</span>
  </label>
</form>
<form class="question-form">
        <h1 class="question">
            An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one product on sale for a period of 24 hours. The company wants to be able to handle millions of requests each hour with millisecond latency during peak hours.
            <br><br>
            Which solution will meet these requirements with the LEAST operational overhead?
        </h1>
        
        <label for="optionA">
            <input type="radio" id="optionA" name="answer" value="A">
            A. Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store the order data in Amazon S3.
        </label>
        
        <label for="optionB">
            <input type="radio" id="optionB" name="answer" value="B">
            B. Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple Availability Zones. Add an Application Load Balancer (ALB) to distribute the website traffic. Add another ALB for the backend APIs. Store the data in Amazon RDS for MySQL.
        </label>
        
        <label for="optionC">
            <input type="radio" id="optionC" name="answer" value="C">
            C. Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscaler to increase and decrease the number of pods to process bursts in traffic. Store the data in Amazon RDS for MySQL.
        </label>
        
        <label for="optionD" class="correct-answer">
            <input type="radio" id="optionD" name="answer" value="D">
            D. Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB.
            <span class="checkmark">✓✓</span>
        </label>
    </form>
 <form class="question-form">
        <h1 class="question">
            A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that accesses this data requires consistently high I/O performance.
            <br><br>
            A solutions architect needs to minimize the time that is required to clone the production data into the test environment.
            <br><br>
            Which solution will meet these requirements?
        </h1>
        
        <label for="optionA">
            <input type="radio" id="optionA" name="answer" value="A">
            A. Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment.
        </label>
        
        <label for="optionB">
            <input type="radio" id="optionB" name="answer" value="B">
            B. Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment.
        </label>
        
        <label for="optionC" class="correct-answer">
            <input type="radio" id="optionC" name="answer" value="C">
            C. Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.
            <span class="checkmark">✓✓</span>
        </label>
        
        <label for="optionD">
            <input type="radio" id="optionD" name="answer" value="D">
            D. Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.
        </label>
    </form>
<form class="question-form">
  <h1 class="question">
    Which EC2 Purchasing Option can provide you the biggest discount, but it is not suitable for critical jobs or databases?
  </h1>

  <label for="optionA">A. Convertible Reserved Instances</label>
  <label for="optionB">B. Dedicated Hosts</label>
  <label for="optionC" class="correct-answer">C. Spot Instances ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    What should you use to control traffic in and out of EC2 instances?
  </h1>

  <label for="optionA" class="correct-answer">A. Security Groups ✓✓</label>
  <label for="optionB">B. NACLs</label>
  <label for="optionC">C. IAM Policies</label>
</form>
<form class="question-form">
  <h1 class="question">
    How long can you reserve an EC2 Reserved Instance?
  </h1>

  <label for="optionA" class="correct-answer">A. 1 or 3 years ✓✓</label>
  <label for="optionB">B. 2 or 4 years</label>
  <label for="optionC">C. 6 months or 1 year</label>
  <label for="optionD">D. Anytime between 1 or 3 years</label>
</form>
<form class="question-form">
  <h1 class="question">
    You would like to deploy a High-Performance Computing (HPC) application on EC2 instances. Which EC2 instance type should you choose?
  </h1>

  <label for="optionA">A. Storage optimized</label>
  <label for="optionB" class="correct-answer">B. Compute optimized ✓✓</label>
  <label for="optionC">C. Memory optimized</label>
  <label for="optionD">D. General purpose</label>
</form>
<form class="question-form">
  <h1 class="question">
    Which EC2 Purchasing Option should you use for an application you plan to run on a server continuously for 1 year?
  </h1>

  <label for="optionA" class="correct-answer">A. Reserved Instances ✓✓</label>
  <label for="optionB">B. Stop Instances</label>
  <label for="optionC">C. On-Demand Instances</label>
</form>
<form class="question-form">
  <h1 class="question">
    Which EC2 Instance Type should you choose for a critical application that uses an in-memory database?
  </h1>

  <label for="optionA">A. Compute Optimized</label>
  <label for="optionB">B. Storage Optimized</label>
  <label for="optionC" class="correct-answer">C. Memory Optimized ✓✓</label>
  <label for="optionD">D. General Purpose</label>
</form>
<form class="question-form">
  <h1 class="question">
    You have an e-commerce application with an OLTP database hosted on-premises. This application has popularity which results in its database having thousands of requests per second. You want to migrate the database to an EC2 instance. Which EC2 Instance Type should you choose to handle this high-frequency OLTP database?
  </h1>

  <label for="optionA">A. Compute Optimized</label>
  <label for="optionB" class="correct-answer">B. Storage Optimized ✓✓</label>
  <label for="optionC">C. Memory Optimized</label>
  <label for="optionD">D. General Purpose</label>
</form>
<form class="question-form"> 
  <h1 class="question">
    Security Groups can be attached to only one EC2 instance.
  </h1>

  <label for="optionA" class="correct-answer">A. False ✓✓</label>
  <label for="optionB">B. True</label>
</form>
<form class="question-form"> 
  <h1 class="question">
    You're planning to migrate on-premises applications to AWS. Your company has strict compliance requirements that require your applications to run on dedicated servers. You also need to use your own server-bound software license to reduce costs. Which EC2 Purchasing Option is suitable for you?
  </h1>

  <label for="optionA">A. Convertible Reserved Instances</label>
  <label for="optionB" class="correct-answer">B. Dedicated Hosts ✓✓</label>
  <label for="optionC">C. Spot Instances</label>
</form>
<form class="question-form"> 
  <h1 class="question">
    You would like to deploy a database technology on an EC2 instance and the vendor license bills you based on the physical cores and underlying network socket visibility. Which EC2 Purchasing Option allows you to get visibility into them?
  </h1>

  <label for="optionA">A. Spot Instances</label>
  <label for="optionB">B. On-Demand</label>
  <label for="optionC" class="correct-answer">C. Dedicated Hosts ✓✓</label>
  <label for="optionD">D. Reserved Instances</label>
</form>
<form class="question-form"> 
  <h1 class="question">
    Spot Fleet is a set of Spot Instances and optionally ................
  </h1>

  <label for="optionA">A. Reserved Instances</label>
  <label for="optionB" class="correct-answer">B. On-Demand Instances ✓✓</label>
  <label for="optionC">C. Dedicated Hosts</label>
  <label for="optionD">D. Dedicated Instances</label>
</form>
<form class="question-form"> 
  <h1 class="question">
    A company's data center is connected to the AWS Cloud over a minimally used 10-Gbps AWS Direct Connect connection with a private virtual interface to its virtual private cloud (VPC). The company internet connection is 200 Mbps, and the company has a 150-TB dataset that is created each Friday. The data must be transferred and available in Amazon S3 on Monday morning.
    Which is the LEAST expensive way to meet the requirements while allowing for data transfer growth?
  </h1>

  <label for="optionA" class="correct-answer">A. Create a VPC endpoint for Amazon S3. Copy the data to Amazon S3 by using the VPC endpoint, forcing the transfer to use the Direct Connect connection . ✓✓</label>
  <label for="optionB">B. Order two 80-GB AWS Snowball appliances. Offload the data to the appliances and ship them to AWS. AWS will copy the data from the Snowball appliances to Amazon S3.</label>
  <label for="optionC">C. Create a VPC endpoint for Amazon S3. Set up a reverse proxy farm behind a Classic Load Balancer in the VPC. Copy the data to Amazon S3 using the proxy.</label>
  <label for="optionD">D. Create a public virtual interface on a Direct Connect connection, and copy the data to Amazon S3 over the connection.</label>
</form>
<form class="question-form">
  <h1 class="question">
    An Auto Scaling group is running at the desired capacity of 5 instances and receives a trigger from the CloudWatch Alarm to increase the capacity by 1. The cooldown period is 5 minutes.
    CloudWatch sends another trigger after 2 minutes to decrease the desired capacity by 1. What will be the count of instances at the end of 4 minutes?
  </h1>

  <label for="optionA">A. 4</label>
  <label for="optionB">B. 5</label>
  <label for="optionC" class="correct-answer">C. 6 ✓✓</label>
  <label for="optionD">D. 7</label>
</form>
<form class="question-form">
  <h1 class="question">
    An organization (account ID 123412341234) has configured the IAM policy to allow the user to modify his credentials.
    What will the below mentioned statement allow the user to perform?
    <pre>
{
"Version": "2012-10-17",
"Statement": [{
"Effect": "Allow",
"Action": [
"iam:AddUserToGroup",
"iam:RemoveUserFromGroup",
"iam:GetGroup"
],
"Resource": "arn:aws:iam::123412341234:group/TestingGroup"
}]
}
    </pre>
  </h1>

  <label for="optionA" class="correct-answer">A. Allow the IAM user to update the membership of the group called TestingGroup ✓✓</label>
  <label for="optionB">B. The IAM policy will throw an error due to an invalid resource name</label>
  <label for="optionC">C. The IAM policy will allow the user to subscribe to any IAM group</label>
  <label for="optionD">D. Allow the IAM user to delete the TestingGroup</label>
</form>
<form class="question-form">
  <h1 class="question">
    After launching an instance that you intend to serve as a NAT (Network Address Translation) device in a public subnet you modify your route tables to have the NAT device be the target of internet bound traffic of your private subnet. When you try and make an outbound connection to the internet from an instance in the private subnet, you are not successful. Which of the following steps could resolve the issue?
  </h1>

  <label for="optionA">A. Attaching a second Elastic Network Interface (ENI) to the NAT instance, and placing it in the private subnet</label>
  <label for="optionB">B. Attaching a second Elastic Network Interface (ENI) to the instance in the private subnet, and placing it in the public subnet</label>
  <label for="optionC">C. Attaching an Elastic IP address to the instance in the private subnet</label>
  <label for="optionD" class="correct-answer">D. Disabling the Source/Destination Check attribute on the NAT instance ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    True or False: The Amazon ElastiCache clusters are not available for use in VPC at this time.
  </h1>

  <label for="optionA">A. TRUE</label>
  <label for="optionB">B. True, but they are available only in the GovCloud.</label>
  <label for="optionC">C. True, but they are available only on request.</label>
  <label for="optionD" class="correct-answer">D. FALSE ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has a data center that must be migrated to AWS as quickly as possible. The data center has a 500 Mbps AWS Direct Connect link and a separate, fully available 1 Gbps ISP connection. A Solutions Architect must transfer 20 TB of data from the data center to an Amazon S3 bucket.<br>
    What is the FASTEST way to transfer the data?
  </h1>

  <label for="optionA">A. Upload the data to the S3 bucket using the existing DX link.</label>
  <label for="optionB">B. Send the data to AWS using the AWS Import/Export service.</label>
  <label for="optionC" class="correct-answer">C. Upload the data to the S3 bucket using S3 Transfer Acceleration  ✓✓</label>
  <label for="optionD">D. Upload the data using an 80 TB AWS Snowball device.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A user has created an AWS AMI. The user wants the AMI to be available only to his friend and not anyone else. How can the user manage this?
  </h1>

  <label for="optionA">A. Share the AMI with the community and setup the approval workflow before anyone launches it.</label>
  <label for="optionB">B. It is not possible to share the AMI with the selected user.</label>
  <label for="optionC" class="correct-answer">C. Share the AMI with a friend's AWS account ID ✓✓</label>
  <label for="optionD">D. Share the AMI with a friend's AWS login ID.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A user is using CloudFormation to launch an EC2 instance and then configure an application after the instance is launched. The user wants the stack creation of ELB and AutoScaling to wait until the EC2 instance is launched and configured properly. How can the user configure this?
  </h1>

  <label for="optionA">A. The user can use the DependentCondition resource to hold the creation of the other dependent resources.</label>
  <label for="optionB">B. It is not possible that the stack creation will wait until one service is created and launched.</label>
  <label for="optionC">C. The user can use the HoldCondition resource to wait for the creation of the other dependent resources.</label>
  <label for="optionD" class="correct-answer">D. The user can use the WaitCondition resource to hold the creation of the other dependent resource ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    An organization is setting up a backup and restore system in AWS of their in premise system. 
    The organization needs High Availability (HA) and Disaster Recovery (DR) but is okay to have a longer recovery time to save costs. 
    Which of the below mentioned setup options helps achieve the objective of cost saving as well as DR in the most effective way?
  </h1>

  <label for="optionA">A. Setup pre-configured servers and create AMIs. Use EIP and Route 53 to quickly switch over to AWS from in premise.</label>
  <label for="optionB">B. Setup the backup data on S3 and transfer data to S3 regularly using the storage gateway.</label>
  <label for="optionC">C. Setup a small instance with AutoScaling; in case of DR start diverting all the load to AWS from on premise.</label>
  <label for="optionD" class="correct-answer">D. Replicate on premise DB to EC2 at regular intervals and setup a scenario similar to the pilot light ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    When using the AWS CLI for AWS CloudFormation, which of the following commands returns a description of the specified resource in the specified stack?
  </h1>

  <label for="optionA">A. describe-stack-events</label>
  <label for="optionB" class="correct-answer">B. describe-stack-resource ✓✓</label>
  <label for="optionC">C. create-stack-resource</label>
  <label for="optionD">D. describe-stack-returns</label>
</form>
<form class="question-form">
  <h1 class="question">
    Can Provisioned IOPS be used on RDS instances launched in a VPC?
  </h1>

  <label for="optionA">A. Yes, they can be used only with Oracle based instances.</label>
  <label for="optionB" class="correct-answer">B. Yes, they can be used for all RDS instances ✓✓</label>
  <label for="optionC">C. No</label>
  <label for="optionD">D. Yes, they can be used only with MySQL based instances.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A Solutions Architect must build a highly available infrastructure for a popular global video game that runs on a mobile phone platform. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The database tier is an Amazon RDS MySQL Multi-AZ instance. The entire application stack is deployed in both us-east-1 and eu-central-1. Amazon Route 53 is used to route traffic to the two installations using a latency-based routing policy. A weighted routing policy is configured in Route 53 as a failover to another region in case the installation in a region becomes unresponsive.
    <br><br>
    During the testing of disaster recovery scenarios, after blocking access to the Amazon RDS MySQL instance in eu-central-1 from all the application instances running in that region, Route 53 does not automatically failover all traffic to us-east-1.
    <br><br>
    Based on this situation, which changes would allow the infrastructure to failover to us-east-1? (Choose two.)
  </h1>

  <label for="optionA" class="correct-answer">
    A. Write a URL in the application that performs a health check on the database layer. Add it as a health check within the weighted routing policy in both regions. ✓✓
  </label>
  <label for="optionB" class="correct-answer">
    B. Set the value of Evaluate Target Health to Yes on the latency alias resources for both eu-central-1 and us-east-1. ✓✓
  </label>
  <label for="optionC">
    C. Specify a weight of 100 for the record pointing to the primary Application Load Balancer in us-east-1 and a weight of 0 for the record pointing to the primary Application Load Balancer in eu-central-1.
  </label>
  <label for="optionD">
    D. Specify a weight of 100 for the record pointing to the primary Application Load Balancer in us-east-1 and a weight of 60 for the pointing to the primary Application Load Balancer in eu-central-1.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A user is configuring MySQL RDS with PIOPS. What should be the minimum PIOPS that the user should provision?
  </h1>

  <label for="optionA">
    A. 10200
  </label>
  <label for="optionB">
    B. 200
  </label>
  <label for="optionC" class="correct-answer">
    C. 1000 ✓✓
  </label>
  <label for="optionD">
    D. 500
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is running multiple applications on Amazon EC2. Each application is deployed and managed by multiple business units. All applications are deployed on a single AWS account but on different virtual private clouds (VPCs). The company uses a separate VPC in the same account for test and development purposes.
    <br><br>
    Production applications suffered multiple outages when users accidentally terminated and modified resources that belonged to another business unit. A Solutions Architect has been asked to improve the availability of the company applications while allowing the Developers access to the resources they need.
    <br><br>
    Which option meets the requirements with the LEAST disruption?
  </h1>

  <label for="optionA">
    A. Create an AWS account for each business unit. Move each business unit's instances to its own account and set up a federation to allow users to access their business unit's account.
  </label>
  <label for="optionB">
    B. Set up a federation to allow users to use their corporate credentials, and lock the users down to their own VPC. Use a network ACL to block each VPC from accessing other VPCs.
  </label>
  <label for="optionC" class="correct-answer">
    C. Implement a tagging policy based on business units. Create an IAM policy so that each user can terminate instances belonging to their own business units only. ✓✓
  </label>
  <label for="optionD">
    D. Set up role-based access for each user and provide limited permissions based on individual roles and the services for which each user is responsible.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    One of the AWS account owners faced a major challenge in June as his account was hacked and the hacker deleted all the data from his AWS account. This resulted in a major blow to the business. Which of the below mentioned steps would not have helped in preventing this action?
  </h1>

  <label for="optionA">
    A. Setup an MFA for each user as well as for the root account user.
  </label>
  <label for="optionB" class="correct-answer">
    B. Create an AMI and a snapshot of the data at regular intervals as well as keep a copy to separate regions.  ✓✓
  </label>
  <label for="optionC">
    C. Take a backup of the critical data to offsite / on premise.
  </label>
  <label for="optionD">
    D. Do not share the AWS access and secret access keys with others as well do not store it inside programs, instead use IAM roles.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A user has launched two EBS backed EC2 instances in the US-East-1a region. The user wants to change the zone of one of the instances. How can the user change it?
  </h1>

  <label for="optionA" class="correct-answer">
    A. It is not possible to change the zone of an instance after it is launched. ✓✓
  </label>
  <label for="optionB">
    B. From the AWS EC2 console, select the Actions -> Change zones and specify the new zone.
  </label>
  <label for="optionC">
    C. The zone can only be modified using the AWS CLI.
  </label>
  <label for="optionD">
    D. Stop one of the instances and change the availability zone.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    Which of the following are associated with using the "HLS" method of viewing the Kinesis video stream? (Select TWO)
  </h1>

  <label for="optionA">
    A. In order to process Kinesis video streams, a SAAS provider needs to build a new video player which is integrated into their major online product.
  </label>
  <label for="optionB" class="correct-answer">
    B. Playback video by typing in the HLS streaming session URL in the location bar of the Apple Safari browser for debug purpose. ✓✓
  </label>
  <label for="optionC">
    C. Able to view only live video, not archived video.
  </label>
  <label for="optionD" class="correct-answer">
    D. A web application that is able to display the video stream using the third-party player Video.js. ✓✓
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company collects data for temperature, humidity, and atmospheric pressure in cities across multiple continents. 
    The company wants to aggregate the data from all these global sites as quickly as possible in a single Amazon S3 bucket, 
    minimizing operational complexity. Which solution meets these requirements?
  </h1>

  <label for="optionA" class="correct-answer">
    A. Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket. ✓✓
  </label>
  <label for="optionB">
    B. Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket. Then remove the data from the origin S3 bucket.
  </label>
  <label for="optionC">
    C. Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site to the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket.
  </label>
  <label for="optionD">
    D. Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. At regular intervals, take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. Restore the EBS volume in that Region.
  </label>
</form>


<form class="question-form">
  <h1 class="question">
    You are preparing to launch an application that will be hosted on a set of EC2 instances. This application needs some software installation and some OS packages need to be updated during the first launch. What is the best way to achieve this when you launch the EC2 instances?
  </h1>

  <label for="optionA">A. Connect to each EC2 instance using SSH, then install the required software and update your OS packages manually</label>
  <label for="optionB">B. Write a bash script that installs the required software and updates to your OS, then contact AWS Support and provide them with the script. They will run it on your EC2 instances at launch</label>
  <label for="optionC" class="correct-answer">C. Write a bash script that installs the required software and updates to your OS, then use this script in EC2 User Data when you launch your EC2 instances ✓✓</label>
</form>

<form class="question-form">
  <h1 class="question">
    A big company has a service to process gigantic clickstream data sets which are often the result of holiday shopping traffic on a retail website, or sudden dramatic growth on the data network of a media or social networking site. It is becoming more and more expensive to analyze these clickstream datasets for its on-premise infrastructure. As the sample data set keeps growing, fewer applications are available to provide a timely response. The service is using a Hadoop cluster with Cascading. How can they migrate the applications to AWS in the best way?
  </h1>

  <label for="optionA">A. Put the source data to a Kinesis stream and migrate the processing service to an AWS EMR cluster with Cascading. Enable EMR to directly read and query data from Kinesis streams. Write the output to Redshift.</label>
  <label for="optionB">B. Put the source data to a Kinesis stream and migrate the processing service to AWS Lambda to utilize its scaling feature. Enable Lambda to directly read and query data from Kinesis stream. Write the output to RDS database.</label>
  <label for="optionC" class="correct-answer">C. Put the source data to S3 and migrate the processing service to an AWS EMR Hadoop cluster with Cascading. Enable EMR to directly read and query data from S3 buckets. Write the output to RDS database ✓✓</label>
  <label for="optionD">D. Put the source data to a S3 bucket and migrate the processing service to AWS EC2 with auto scaling. Ensure that the auto scaling configuration has proper maximum and minimum number of instances. Monitor the performance in CloudWatch dashboard. Write the output to DynamoDB table for downstream to process.</label>
</form>
<form class="question-form">
  <h1 class="question">
    What is the name of licensing model in which I can use your existing Oracle Database licenses to run Oracle deployments on Amazon RDS?
  </h1>

  <label for="optionA" class="correct-answer">A. Bring Your Own License ✓✓</label>
  <label for="optionB">B. Role Bases License</label>
  <label for="optionC">C. Enterprise License</label>
  <label for="optionD">D. License Included</label>
</form>
<form class="question-form">
  <h1 class="question">
    You have launched an EC2 instance with four (4) 500 GB EBS Provisioned IOPS volumes attached. The EC2 instance is EBS-Optimized and supports 500 Mbps throughput between EC2 and EBS. The four EBS volumes are configured as a single RAID 0 device, and each Provisioned IOPS volume is provisioned with 4,000 IOPS (4,000 16KB reads or writes), for a total of 16,000 random IOPS on the instance. The EC2 instance initially delivers the expected 16,000 IOPS random read and write performance. Sometime later, in order to increase the total random I/O performance of the instance, you add an additional two 500 GB EBS Provisioned IOPS volumes to the RAID. Each volume is provisioned to 4,000 IOPS like the original four, for a total of 24,000 IOPS on the EC2 instance. Monitoring shows that the EC2 instance CPU utilization increased from 50% to 70%, but the total random IOPS measured at the instance level does not increase at all.
    <br><br>
    What is the problem and a valid solution?
  </h1>

  <label for="optionA">A. Small block sizes cause performance degradation, limiting the I/O throughput; configure the instance device driver and filesystem to use 64KB blocks to increase throughput.</label>
  <label for="optionB" class="correct-answer">B. The EBS-Optimized throughput limits the total IOPS that can be utilized; use an EBS-Optimized instance that provides larger throughput ✓✓</label>
  <label for="optionC">C. The standard EBS Instance root volume limits the total IOPS rate; change the instance root volume to also be a 500GB 4,000 Provisioned IOPS volume.</label>
  <label for="optionD">D. Larger storage volumes support higher Provisioned IOPS rates; increase the provisioned volume storage of each of the 6 EBS volumes to 1TB.</label>
  <label for="optionE">E. RAID 0 only scales linearly to about 4 devices; use RAID 0 with 4 EBS Provisioned IOPS volumes, but increase each Provisioned IOPS EBS volume to 6,000 IOPS.</label>
</form>
<form class="question-form">
  <h1 class="question">
    Which of the following commands accepts binary data as parameters?
  </h1>

  <label for="optionA">A. --ciphertext-key</label>
  <label for="optionB" class="correct-answer">B. --user-data  ✓✓</label>
  <label for="optionC">C. --aws-customer-key</label>
  <label for="optionD">D. --describe-instances-user</label>
</form>
<form class="question-form">
  <h1 class="question">
    A Solutions Architect needs to migrate a legacy application from on premises to AWS. On premises, the application runs on two Linux servers behind a load balancer and accesses a database that is master-master on two servers. Each application server requires a license file that is tied to the MAC address of the server's network adapter. It takes the software vendor 12 hours to send new license files through email. The application requires configuration files to use static IPv4 addresses to access the database servers, not DNS.
    <br><br>
    Given these requirements, which steps should be taken together to enable a scalable architecture for the application servers? (Choose two.)
  </h1>

  <label for="optionA" class="correct-answer">A. Create bootstrap automation to attach an ENI from the pool, read the database IP addresses from AWS Systems Manager Parameter Store, and inject those parameters into the local configuration files. Keep SSM up to date using a Lambda function . ✓✓</label>
  <label for="optionB" class="correct-answer">B. Create a pool of ENIs, request license files from the vendor for the pool, and store the license files within Amazon S3. Create automation to download an unused license, and attach the corresponding ENI at boot time. ✓✓</label>
  <label for="optionC">C. Create a bootstrap automation to request a new license file from the vendor with a unique return email. Have the server configure itself with the received license file.</label>
  <label for="optionD">D. Install the application on an EC2 instance, configure the application, and configure the IP address information. Create an AMI from this instance and use it for all instances.</label>
  <label for="optionE">E. Create a pool of ENIs, request license files from the vendor for the pool, store the license files on an Amazon EC2 instance, modify the configuration files, and create an AMI from the instance. Use this AMI for all instances .</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is migrating its on-premises build artifact server to an AWS solution. The current system consists of an Apache HTTP server that serves artifacts to clients on the local network, restricted by the perimeter firewall. The artifact consumers are largely build automation scripts that download artifacts via anonymous HTTP, which the company will be unable to modify within its migration timetable.
    <br><br>
    The company decides to move the solution to Amazon S3 static website hosting. The artifact consumers will be migrated to Amazon EC2 instances located within both public and private subnets in a virtual private cloud (VPC).
    <br><br>
    Which solution will permit the artifact consumers to download artifacts without modifying the existing automation scripts?
  </h1>

  <label for="optionA">A. Create a NAT gateway within a public subnet of the VPC. Add a default route pointing to the NAT gateway into the route table associated with the subnets containing consumers. Configure the bucket policy to allow the s3:ListBucket and s3:GetObject actions using the condition IpAddress and the condition key aws:SourceIp matching the elastic IP address of the NAT gateway.</label>
  <label for="optionB" class="correct-answer">B. Create a VPC endpoint and add it to the route table associated with subnets containing consumers. Configure the bucket policy to allow s3:ListBucket and s3:GetObject actions using the condition and the condition key aws:sourceVpce matching the identification of the VPC StringEquals endpoint. ✓✓</label>
  <label for="optionC">C. Create an IAM role and instance profile for Amazon EC2 and attach it to the instances that consume build artifacts. Configure the bucket policy to allow the s3:ListBucket and s3:GetObjects actions for the principal matching the IAM role created.</label>
  <label for="optionD">D. Create a VPC endpoint and add it to the route table associated with subnets containing consumers. Configure the bucket policy to allow s3:ListBucket and s3:GetObject actions using the condition and the condition key aws:SourceIp matching the VPC CIDR block. IpAddress</label>
</form>
<form class="question-form">
  <h1 class="question">
    A user is planning to host a web server as well as an app server on a single EC2 instance which is a part of the public subnet of a VPC. How can the user set up to have two separate public IPs and separate security groups for both the application as well as the web server?
  </h1>

  <label for="optionA">A. Launch VPC with two separate subnets and make the instance a part of both the subnets.</label>
  <label for="optionB" class="correct-answer">B. Launch a VPC instance with two network interfaces. Assign a separate security group and elastic IP to them. ✓✓</label>
  <label for="optionC">C. Launch a VPC instance with two network interfaces. Assign a separate security group to each and AWS will assign a separate public IP to them.</label>
  <label for="optionD">D. Launch a VPC with ELB such that it redirects requests to separate VPC instances of the public subnet.</label>
</form>
<form class="question-form">
  <h1 class="question">
    An AWS customer runs a public blogging website. The site users upload two million blog entries a month. The average blog entry size is 200 KB. The access rate to blog entries drops to negligible 6 months after publication and users rarely access a blog entry 1 year after publication.
    <br><br>
    Additionally, blog entries have a high update rate during the first 3 months following publication, this drops to no updates after 6 months. The customer wants to use CloudFront to improve his user's load times.
    <br><br>
    Which of the following recommendations would you make to the customer?
  </h1>

  <label for="optionA">A. Create a CloudFront distribution with "US/Europe" price class for US/Europe users and a different CloudFront distribution with "All Edge Locations" for the remaining users.</label>
  <label for="optionB" class="correct-answer">B. Create a CloudFront distribution with S3 access restricted only to the CloudFront identity and partition the blog entry's location in S3 according to the month it was uploaded to be used with CloudFront behaviors. ✓✓</label>
  <label for="optionC">C. Create a CloudFront distribution with Restrict Viewer Access, Forward Query String set to true and minimum TTL of 0.</label>
  <label for="optionD">D. Duplicate entries into two different buckets and create two separate CloudFront distributions where S3 access is restricted only to CloudFront identity.</label>
</form>
<form class="question-form">
  <h1 class="question">
    To ensure failover capabilities on an elastic network interface (ENI), what should you use for incoming traffic?
  </h1>

  <label for="optionA">A. A Route53 A record</label>
  <label for="optionB" class="correct-answer">B. A secondary private IP ✓✓</label>
  <label for="optionC">C. A secondary public IP</label>
  <label for="optionD">D. A secondary ENI</label>
</form>
<form class="question-form">
  <h1 class="question">
    An organization is planning to host an application on the AWS VPC. The organization wants dedicated instances. However, an AWS consultant advised the organization not to use dedicated instances with VPC as the design has a few limitations. Which of the below mentioned statements is not a limitation of dedicated instances with VPC?
  </h1>

  <label for="optionA">A. All instances launched with this VPC will always be dedicated instances and the user cannot use a default tenancy model for them.</label>
  <label for="optionB">B. It does not support the AWS RDS with a dedicated tenancy VPC.</label>
  <label for="optionC" class="correct-answer">C. The user cannot use Reserved Instances with a dedicated tenancy model. ✓✓</label>
  <label for="optionD">D. The EBS volume will not be on the same tenant hardware as the EC2 instance though the user has configured dedicated tenancy.</label>
</form>
<form class="question-form">
  <h1 class="question">
    An organization is setting up their website on AWS. The organization is working on various security measures to be performed on the AWS EC2 instances. Which of the below mentioned security mechanisms will not help the organization to avoid future data leaks and identify security weaknesses?
  </h1>

  <label for="optionA">A. Run penetration testing on AWS with prior approval from Amazon.</label>
  <label for="optionB">B. Perform SQL injection for application testing.</label>
  <label for="optionC" class="correct-answer">C. Perform a Code Check for any memory leaks. ✓✓</label>
  <label for="optionD">D. Perform a hardening test on the AWS instance.</label>
</form>


<form class="question-form">
  <h1 class="question">
    A user is creating a Provisioned IOPS volume.<br><br>
    What is the maximum ratio the user should configure between Provisioned IOPS and the volume size?
  </h1>

  <label for="optionA">
    A. 30 to 1
  </label>

  <label for="optionB">
    B. 150 to 1
  </label>

  <label for="optionC" class="correct-answer">
    C. 50 to 1 ✓✓
  </label>

  <label for="optionD">
    D. 20 to 1
  </label>
</form>

<form class="question-form">
  <h1 class="question">
    API Gateway and Lambda non-proxy integrations have been chosen to implement an application by a software engineer.
    <br><br>
    The application is a data analysis tool that returns some statistic results when the HTTP endpoint is called. The Lambda function needs to communicate with some back-end data services such as Keen.io. However, there are chances that errors happen (e.g., wrong data requested, bad communication, etc).
    <br><br>
    The Lambda is written in Java and may return two exceptions: <strong>BadRequestException</strong> and <strong>InternalErrorException</strong>.
    <br><br>
    What should the software engineer do to map these two exceptions in API Gateway with proper HTTP return codes?
    <br><br>
    For example, <strong>BadRequestException</strong> and <strong>InternalErrorException</strong> are mapped to HTTP return codes <strong>400</strong> and <strong>500</strong> respectively. (Select 2)
  </h1>

  <label for="optionA" class="correct-answer">
    A. Add the corresponding error codes (400 and 500) on the Method Response in API Gateway. ✓✓✓
  </label>

  <label for="optionB" class="correct-answer">
    B. Add Integration Responses where regular expression patterns are set such as BadRequest or InternalError. Associate them with HTTP status codes  . ✓✓✓
  </label>

  <label for="optionC">
    C. Add Method Responses where regular expression patterns are set such as BadRequest or InternalError. Associate them with HTTP status codes 400 and 500.
  </label>

  <label for="optionD">
    D. Put the mapping logic into Lambda itself so that when exception happens, error codes are returned at the same time in a JSON body.
  </label>

  <label for="optionE">
    E. Add the corresponding error codes (400 and 500) on the Integration Response in API Gateway
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company runs a video processing platform. Files are uploaded by users who connect to a web server, which stores them on an Amazon EFS share. This web server is running on a single Amazon EC2 instance.
    <br><br>
    A different group of instances, running in an Auto Scaling group, scans the EFS share directory structure for new files to process and generates new videos (thumbnails, different resolution, compression, etc.) according to the instructions file, which is uploaded along with the video files.
    <br><br>
    A different application running on a group of instances managed by an Auto Scaling group processes the video files and then deletes them from the EFS share. The results are stored in an S3 bucket. Links to the processed video files are emailed to the customer.
    <br><br>
    The company has recently discovered that as they add more instances to the Auto Scaling Group, many files are processed twice, so image processing speed is not improved. The maximum size of these video files is 2GB.
    <br><br>
    What should the Solutions Architect do to improve reliability and reduce the redundant processing of video files?
  </h1>

  <label for="optionA" class="correct-answer">
    A.  Rewrite the application to run from Amazon S3 and upload the video files to an S3 bucket. Each time a new file is uploaded, trigger an AWS Lambda function to put a message in an SQS queue containing the link and the instructions. Modify the video processing application to read from the SQS queue and the S3 bucket. Use the queue depth metric to adjust the size of the Auto Scaling group for video processing instances.   ✓✓✓
  </label>

  <label for="optionB">
    B. Set up a cron job on the web server instance to synchronize the contents of the EFS share into Amazon S3. Trigger an AWS Lambda function every time a file is uploaded to process the video file and store the results in Amazon S3. Using Amazon CloudWatch Events trigger an Amazon SES job to send an email to the customer containing the link to the processed file.
  </label>

  <label for="optionC">
    C. Rewrite the web application to run directly from Amazon S3 and use Amazon API Gateway to upload the video files to an S3 bucket. Use an S3 trigger to run an AWS Lambda function each time a file is uploaded to process and store new video files in a different bucket. Using CloudWatch Events, trigger an SES job to send an email to the customer containing the link to the processed file.
  </label>

  <label for="optionD">
    D. Modify the web application to upload the video files directly to Amazon S3. Use Amazon CloudWatch Events to trigger an AWS Lambda function every time a file is uploaded, and have this Lambda function put a message into an Amazon queue for new files and use the queue depth metric to scale instances in the video processing Auto Scaling group.
  </label>
</form>



<form class="question-form">
  <h1 class="question">
    Refer to the architecture diagram above of a batch processing solution using Simple Queue Service (SQS) to set up a message queue between EC2 instances which are used as batch processors. CloudWatch monitors the number of job requests (queued messages) and an Auto Scaling group adds or deletes batch servers automatically based on parameters set in CloudWatch alarms.
    <br><br>
    You can use this architecture to implement which of the following features in a cost effective and efficient manner?
  </h1>

  <label for="optionA" class="correct-answer">A. Coordinate number of EC2 instances with number of Job requests automatically, thus improving cost effectiveness. ✓✓</label>
  <label for="optionB">B. Reduce the overall time for executing Jobs through parallel processing by allowing a busy EC2 instance that receives a message to pass it to the next instance in a daisy-chain setup.</label>
  <label for="optionC">C. Implement fault tolerance against EC2 instance failure since messages would remain in SQS and work can continue with recovery of EC2 instances. Implement fault tolerance against SQS failure by backing up messages to S3.</label>
  <label for="optionD">D. Handle high priority Jobs before lower priority Jobs by assigning a priority metadata field to SQS messages.</label>
  <label for="optionE">E. Implement message passing between EC2 instances within a batch by exchanging messages through SQS.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A read-only news reporting site with a combined web and application tier and a database tier that receives large and unpredictable traffic demands must be able to respond to these traffic fluctuations automatically. What AWS services should be used to meet these requirements?
  </h1>

  <label for="optionA" class="correct-answer">A. Stateless instances for the web and application tier synchronized using ElastiCache Memcached in an autoscaling group monitored with CloudWatch, and RDS with read replicas. ✓✓</label>
  <label for="optionB">B. Stateful instances for the web and application tier in an autoscaling group monitored with CloudWatch, and multi-AZ RDS.</label>
  <label for="optionC">C. Stateful instances for the web and application tier in an autoscaling group monitored with CloudWatch, and RDS with read replicas.</label>
  <label for="optionD">D. Stateless instances for the web and application tier synchronized using ElastiCache Memcached in an autoscaling group monitored with CloudWatch, and multi-AZ RDS.</label>
</form>
  <form class="question-form">
  <h1 class="question">
    A company wants to ensure that the workloads for each of its business units have complete autonomy and a minimal blast radius in AWS. The Security team must be able to control access to the resources and services in the account to ensure that particular services are not used by the business units.
    How can a Solutions Architect achieve the isolation requirements?
  </h1>

  <label for="optionA" class="correct-answer">A. Create individual accounts for each business unit and add the account to an OU in AWS Organizations. Modify the OU to ensure that the particular services are blocked. Federate each account with an IdP, and create separate roles for the business units and the Security team. ✓✓</label>
  <label for="optionB">B. Create individual accounts for each business unit. Federate each account with an IdP and create separate roles and policies for business units and the Security team.</label>
  <label for="optionC">C. Create one shared account for the entire company. Create separate VPCs for each business unit. Create individual IAM policies and resource tags for each business unit. Federate each account with an IdP, and create separate roles for the business units and the Security team.</label>
  <label for="optionD">D. Create one shared account for the entire company. Create individual IAM policies and resource tags for each business unit. Federate the account with an IdP, and create separate roles for the business units and the Security team.</label>
</form>
<form class="question-form">
  <h1 class="question">
    Your company currently has a 2-tier web application running in an on-premises data center. 
    You have experienced several infrastructure failures in the past few months resulting in significant financial losses. Your CIO is strongly considering moving the application to AWS. 
    While working on achieving buy-in from the other company executives, he asks you to develop a disaster recovery plan to help improve business continuity in the short term. 
    He specifies a target Recovery Time Objective (RTO) of 4 hours and a Recovery Point Objective (RPO) of 1 hour or less. 
    He also asks you to implement the solution within 2 weeks.
    Your database is 200GB in size and you have a 20Mbps Internet connection.
    How would you do this while minimizing costs?
  </h1>

  <label for="optionA">A. Create an EBS backed private AMI which includes a fresh install of your application. Setup a script in your data center to backup the local database every 1 hour and to encrypt and copy the resulting file to an S3 bucket using multi-part upload. </label>

  <label for="optionB" class="correct-answer">B. Create an EBS backed private AMI which includes a fresh install of your application. Develop a CloudFormation template which includes your AMI and the required EC2, AutoScaling, and ELB resources to support deploying the application across Multiple-Availability-Zones. Asynchronously replicate transactions from your on-premises database to a database instance in AWS across a secure VPN connection. ✓✓</label>

  <label for="optionC">C. Install your application on a compute-optimized EC2 instance capable of supporting the application's average load. Synchronously replicate transactions from your on-premises database to a database instance in AWS across a secure Direct Connect connection.</label>

  <label for="optionD">D. Deploy your application on EC2 instances within an Auto Scaling group across multiple availability zones. Asynchronously replicate transactions from your on-premises database to a database instance in AWS across a secure VPN connection.</label>
</form>

<form class="question-form">
  <h1 class="question">
    You have been asked to set up a public website on AWS with the following criteria:
    <ul>
      <li>The database and the application server must run in an Amazon VPC.</li>
      <li>The database must be able to connect to the Internet for automatic patch updates.</li>
      <li>The database must not receive any incoming traffic from the Internet.</li>
    </ul>
    Which solutions would be the best to satisfy all the above requirements for your planned public website on AWS? (Choose 2 answers)
  </h1>

  <label for="optionA">A. Set up both the public website and the database on a public subnet and block all incoming requests from the Internet with a Network Access Control List (NACL)</label>
  <label for="optionB">B. Set up both the public website and the database on a private subnet and block all incoming requests from the Internet with a Network Access Control List (NACL). Set up a Security group between the public website and the database which only allows access via port 80 .</label>
  <label for="optionC" class="correct-answer">C. Set up the public website on a public subnet and set up the database in a private subnet which connects to the Internet via a NAT instance ✓✓</label>
  <label for="optionD" class="correct-answer">D. Set up both the public website and the database on a public subnet, and block all incoming requests from the Internet with a security group which only allows access from the IP of the public website   ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A web-startup runs its very successful social news application on Amazon EC2 with an Elastic Load Balancer, an Auto-Scaling group of Java/Tomcat application-servers, and DynamoDB as data store. The main web-application best runs on m2.xlarge instances since it is highly memory-bound. Each new deployment requires semi-automated creation and testing of a new AMI for the application servers, which takes quite a while and is therefore only done once per week.

    Recently, a new chat feature has been implemented in Node.js and waits to be integrated in the architecture. First tests show that the new component is CPU-bound. Because the company has some experience with using Chef, they decided to streamline the deployment process and use AWS OpsWorks as an application life cycle tool to simplify management of the application and reduce the deployment cycles.

    What configuration in AWS OpsWorks is necessary to integrate the new chat module in the most cost-efficient and flexible way?
  </h1>

  <label for="optionA">A. Create two AWS OpsWorks stacks, create two AWS OpsWorks layers, create two custom recipes</label>
  <label for="optionB" class="correct-answer">B. Create one AWS OpsWorks stack, create two AWS OpsWorks layers, create one custom recipe ✓✓</label>
  <label for="optionC">C. Create one AWS OpsWorks stack, create one AWS OpsWorks layer, create one custom recipe</label>
  <label for="optionD">D. Create two AWS OpsWorks stacks, create two AWS OpsWorks layers, create one custom recipe</label>
</form>
<form class="question-form">
  <h1 class="question">
    A user has created a VPC with public and private subnets using the VPC Wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24. Which of the below mentioned entries are required in the main route table to allow the instances in VPC to communicate with each other?
  </h1>

  <label for="optionA">A. Destination : 20.0.0.0/0 and Target : ALL</label>
  <label for="optionB" class="correct-answer">B. Destination : 20.0.0.0/16 and Target : Local ✓✓</label>
  <label for="optionC">C. Destination : 20.0.0.0/24 and Target : Local</label>
  <label for="optionD">D. Destination : 20.0.0.0/16 and Target : ALL</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has an application that generates a weather forecast that is updated every 15 minutes with an output resolution of 1 billion unique positions, each approximately 20 bytes in size (20 Gigabytes per forecast). Every hour, the forecast data is globally accessed approximately 5 million times (1,400 requests per second), and up to 10 times more during weather events. The forecast data is overwritten every update. Users of the current weather forecast application expect responses to queries to be returned in less than two seconds for each request.
    <br><br>
    Which design meets the required request rate and response time?
  </h1>

  <label for="optionA">A.  Store forecast locations in an Amazon ES cluster. Use an Amazon CloudFront distribution targeting an API Gateway endpoint with AWS Lambda functions responding to queries as the origin. Create an Amazon Lambda@Edge function that caches the data locally at edge locations for 15 minutes.</label>

  <label for="optionB">B. Store forecast locations in an Amazon EFS volume. Create an Amazon CloudFront distribution that targets an Elastic Load Balancing group of an Auto Scaling fleet of Amazon EC2 instances that have mounted the Amazon EFS volume. Set the set cache-control timeout for 15 minutes in the CloudFront distribution.</label>

  <label for="optionC" class="correct-answer">C. Store forecast locations in an Amazon ES cluster. Use an Amazon CloudFront distribution targeting an Amazon API Gateway endpoint with AWS Lambda functions responding to queries as the origin. Enable API caching on the API Gateway stage with a cache-control timeout set for 15 minutes . ✓✓</label>

  <label for="optionD">D. Store forecast locations in an Amazon S3 as individual objects. Create an Amazon CloudFront distribution targeting an Elastic Load Balancing group of an Auto Scaling fleet of EC2 instances, querying the origin of the S3 object. Set the cache-control timeout for 15 minutes in the CloudFront distribution.</label>
</form>
<form class="question-form">
  <h1 class="question">
    What types of identities do Amazon Cognito identity pools support?
  </h1>

  <label for="optionA" class="correct-answer">A. They support both authenticated and unauthenticated identities. ✓✓</label>
  <label for="optionB">B. They support only unauthenticated identities.</label>
  <label for="optionC">C. They support neither authenticated nor unauthenticated identities.</label>
  <label for="optionD">D. They support only authenticated identities.</label>
</form>
<form class="question-form">
  <h1 class="question">
    Which of the following AWS services can be used to define alarms to trigger on a certain activity, such as activity success, failure, or delay in AWS Data Pipeline?
  </h1>

  <label for="optionA">A. Amazon SES</label>
  <label for="optionB">B. Amazon CodeDeploy</label>
  <label for="optionC" class="correct-answer">C. Amazon SNS ✓✓</label>
  <label for="optionD">D. Amazon SQS</label>
</form>
<form class="question-form">
  <h1 class="question">
    In DynamoDB, to get a detailed listing of secondary indexes on a table, you can use the _____ action.
  </h1>

  <label for="optionA">A. BatchGetItem</label>
  <label for="optionB">B. TableName</label>
  <label for="optionC" class="correct-answer">C. DescribeTable ✓✓</label>
  <label for="optionD">D. GetItem</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company's application is increasingly popular and experiencing latency because of high volume reads on the database server. The service has the following properties:
    <ul>
      <li>A highly available REST API hosted in one region using Application Load Balancer (ALB) with auto scaling.</li>
      <li>A MySQL database hosted on an Amazon EC2 instance in a single Availability Zone.</li>
      <li>The company wants to reduce latency, increase in-region database read performance, and have multi-region disaster recovery capabilities that can perform a live recovery automatically without any data or performance loss (HA/DR).</li>
    </ul>
    Which deployment strategy will meet these requirements?
  </h1>

  <label for="optionA" class="correct-answer">
    A. Use AWS CloudFormation StackSets to deploy the API layer in two regions. Migrate the database to an Amazon Aurora with MySQL database cluster with multiple read replicas in one region and a read replica in a different region than the source database cluster. Use Amazon Route 53 health checks to trigger a DNS failover to the standby region if the health checks to the primary load balancer fail. In the event of Route 53 failover, promote the cross-region database replica to be the master and build out new read replicas in the standby region. ✓✓
  </label>

  <label for="optionB">
    B. Use Amazon ElastiCache for Redis Multi-AZ with an automatic failover to cache the database read queries. Use AWS OpsWorks to deploy the API layer, cache layer, and existing database layer in two regions. In the event of failure, use Amazon Route 53 health checks on the database to trigger a DNS failover to the standby region if the health checks in the primary region fail. Back up the MySQL database frequently, and in the event of a failure in an active region, copy the backup to the standby region and restore the standby database.
  </label>

  <label for="optionC">
    C. Use AWS CloudFormation StackSets to deploy the API layer in two regions. Add the database to an Auto Scaling group. Add a read replica to the database in the second region. Use Amazon Route 53 health checks in the primary region fail. Promote the cross-region database replica to be the master and build out new read replicas in the standby region.
  </label>

  <label for="optionD">
    D. Use Amazon ElastiCache for Redis Multi-AZ with an automatic failover to cache the database read queries. Use AWS OpsWorks to deploy the API layer, cache layer, and existing database layer in two regions. Use Amazon Route 53 health checks on the ALB to trigger a DNS failover to the standby region if the health checks in the primary region fail. Back up the MySQL database frequently, and in the event of a failure in an active region, copy the backup to the standby region and restore the standby database.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has a requirement that only allows specially hardened AMIs to be launched into public subnets in a VPC, and for the AMIs to be associated with a specific security group.
    <br><br>
    Allowing non-compliant instances to launch into the public subnet could present a significant security risk if they are allowed to operate.
    <br><br>
    A mapping of approved AMIs to subnets to security groups exists in an Amazon DynamoDB table in the same AWS account. The company created an AWS Lambda function that, when invoked, will terminate a given Amazon EC2 instance if the combination of AMI, subnet, and security group are not approved in the DynamoDB table.
    <br><br>
    What should the Solutions Architect do to MOST quickly mitigate the risk of compliance deviations?
  </h1>

  <label for="optionA">
    A. Create an Amazon CloudWatch Events rule that matches each time an EC2 instance is launched using one of the allowed AMIs, and associate it with the Lambda function as the target.
  </label>

  <label for="optionB">
    B. For the Amazon S3 bucket receiving the AWS CloudTrail logs, create an S3 event notification configuration with a filter to match when logs contain the ec2:RunInstances action, and associate it with the Lambda function as the target.
  </label>

  <label for="optionC">
    C. Enable AWS CloudTrail and configure it to stream to an Amazon CloudWatch Logs group. Create a metric filter in CloudWatch to match when the ec2:RunInstances action occurs, and trigger the Lambda function when the metric is greater than 0.
  </label>

  <label for="optionD" class="correct-answer">
    D. Create an Amazon CloudWatch Events rule that matches each time an EC2 instance is launched, and associate it with the Lambda function as the target. ✓✓
  </label>
</form>

















<form class="question-form">
  <h1 class="question">
    A user has created a VPC with a public subnet. The user has terminated all the instances which are part of the subnet. 
    Which of the below mentioned statements is true with respect to this scenario?
  </h1>

  <label for="optionA">A. The subnet to which the instances were launched with will be deleted</label>

  <label for="optionB">B. When the user launches a new instance it cannot use the same subnet</label>

  <label for="optionC">C. The user cannot delete the VPC since the subnet is not deleted</label>

  <label for="optionD" class="correct-answer">D. Secondary network interfaces attached to the terminated instances may persist ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A media storage application uploads user photos to Amazon S3 for processing. End users are reporting that some uploaded photos are not being processed properly. The Application Developers trace the logs and find that AWS Lambda is experiencing execution issues when thousands of users are on the system simultaneously. Issues are caused by:
    <br>
    - Limits around concurrent executions.
    <br>
    - The performance of Amazon DynamoDB when saving data.
    <br><br>
    Which actions can be taken to increase the performance and reliability of the application? (Choose two.)
  </h1>

  <label for="optionA" class="correct-answer">A. Configure a dead letter queue that will reprocess failed or timed-out Lambda functions ✓✓</label>

  <label for="optionB" class="correct-answer">B. Evaluate and adjust the write capacity units (WCUs) for the DynamoDB tables. ✓✓</label>

  <label for="optionC">C. Add an Amazon ElastiCache layer to increase the performance of Lambda functions.</label>

  <label for="optionD">D. Evaluate and adjust the read capacity units (RCUs) for the DynamoDB tables. .</label>

  <label for="optionE">E. Use S3 Transfer Acceleration to provide lower-latency access to end users.</label>
</form>

<form class="question-form">
  <h1 class="question">
    You have deployed a web application, targeting a global audience across multiple AWS Regions under the domain name example.com.
    <br><br>
    You decide to use Route53 Latency-Based Routing to serve web requests to users from the region closest to the user. To provide business continuity in the event of server downtime you configure weighted record sets associated with two web servers in separate Availability Zones per region.
    <br><br>
    During a DR test you notice that when you disable all web servers in one of the regions Route53 does not automatically direct all users to the other region.
    <br><br>
    What could be happening? (Choose 2 answers)
  </h1>

  <label for="optionA" class="correct-answer">A. You did not set "Evaluate Target Health" to 'Yes" on the latency alias resource record set associated with example.com in the region where you disabled the servers. ✓✓</label>

  <label for="optionB">B. The value of the weight associated with the latency alias resource record set in the region with the disabled servers is higher than the weight for the other region.</label>

  <label for="optionC">C. One of the two working web servers in the other region did not pass its HTTP health check.</label>

  <label for="optionD">D. Latency resource record sets cannot be used in combination with weighted resource record sets.</label>

  <label for="optionE" class="correct-answer">E. You did not setup an HTTP health check for one or more of the weighted resource record sets associated with the disabled web servers. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    In DynamoDB, which of the following operations is not possible by the console?
  </h1>

  <label for="optionA">A. Updating an item</label>
  <label for="optionB">B. Copying an item</label>
  <label for="optionC" class="correct-answer">C. Blocking an item ✓✓</label>
  <label for="optionD">D. Deleting an item</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has decided to move some workloads onto AWS to create a grid environment to run market analytics.
    The grid will consist of many similar instances, spun-up by a job-scheduling function. Each time a large analytics
    workload is completed, a new VPC is deployed along with job scheduler and grid nodes. Multiple grids could be
    running in parallel. Key requirements are:
    <ul>
      <li>Grid instances must communicate with Amazon S3 to retrieve data to be processed.</li>
      <li>Grid instances must communicate with Amazon DynamoDB to track intermediate data.</li>
      <li>The job scheduler needs only to communicate with the Amazon EC2 API to start new grid nodes.</li>
      <li>No internet access is allowed, either directly or via the on-premises proxy.</li>
      <li>The application must seamlessly communicate with Amazon S3, DynamoDB, and EC2 API without needing reconfiguration for each deployment.</li>
    </ul>
    Which of the following should the Solutions Architect do to achieve this target architecture? (Choose three.)
  </h1>

  <label for="optionA" class="correct-answer">A. Enable VPC endpoints for Amazon S3 and DynamoDB ✓✓</label>
  <label for="optionB">B. Disable Private DNS Name Support</label>
  <label for="optionC">C. Configure the application on the grid instances to use the private DNS name of the Amazon S3 endpoint</label>
  <label for="optionD">D. Populate the on-premises DNS server with the private IP addresses of the EC2 endpoint</label>
  <label for="optionE" class="correct-answer">E. Enable an interface VPC endpoint for EC2 ✓✓</label>
  <label for="optionF" class="correct-answer">F. Configure Amazon S3 endpoint policy to permit access only from the grid nodes ✓✓</label>
</form>







<form class="question-form">
  <h1 class="question">
    Someone is creating a VPC for their application hosting. He has created two private subnets in the same availability zone and created one subnet in a separate availability zone. He wants to make a High Availability system with an internal Elastic Load Balancer.
  </h1>
  <p>Which choice is true regarding internal ELBs in this scenario? (Choose 2 answers)</p>

  <label for="optionA" class="correct-answer">A. Internal ELBs should only be launched within private subnets. ✓✓</label>
  <label for="optionB">B. Amazon ELB service does not allow subnet selection; instead it will automatically select all the available subnets of the VPC.</label>
  <label for="optionC" class="correct-answer">C. Internal ELBs can support only one subnet in each availability zone. ✓✓</label>
  <label for="optionD">D. An internal ELB can support all the subnets irrespective of their zones.</label>
</form>
<form class="question-form">
  <h1 class="question">
    Your customer is willing to consolidate their log streams (access logs, application logs, security logs, etc.) in one single system. Once consolidated, the customer wants to analyze these logs in real time based on heuristics. From time to time, the customer needs to validate heuristics, which requires going back to data samples extracted from the last 12 hours.
  </h1>
  <p>What is the best approach to meet your customer's requirements?</p>

  <label for="optionA">A. Configure Amazon CloudTrail to receive custom logs, use EMR to apply heuristics the logs</label>
  <label for="optionB">B. Send all the log events to Amazon SQS, setup an Auto Scaling group of EC2 servers to consume the logs and apply the heuristics</label>
  <label for="optionC">C. Setup an Auto Scaling group of EC2 syslogd servers, store the logs on S3, use EMR to apply heuristics on the logs</label>
  <label for="optionD" class="correct-answer">D. Send all the log events to Amazon Kinesis, develop a client process to apply heuristics on the logs. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is running a batch analysis every hour on their main transactional DB, running on an RDS MySQL instance, to populate their central Data Warehouse running on Redshift. During the execution of the batch, their transactional applications are very slow. When the batch completes, they need to update the top management dashboard with the new data. The dashboard is produced by another system running on-premises that is currently started when a manually-sent email notifies that an update is required. The on-premises system cannot be modified because it is managed by another team.
  </h1>
  <p>How would you optimize this scenario to solve performance issues and automate the process as much as possible?</p>

  <label for="optionA">A. Replace RDS with Redshift for the batch analysis and SNS to notify the on-premises system to update the dashboard</label>
  <label for="optionB">B. Replace RDS with Redshift for the batch analysis and SQS to send a message to the on-premises system to update the dashboard</label>
  <label for="optionC" class="correct-answer">C. Create an RDS Read Replica for the batch analysis and SNS to notify the on-premises system to update the dashboard. ✓✓</label>
  <label for="optionD">D. Create an RDS Read Replica for the batch analysis and SQS to send a message to the on-premises system to update the dashboard.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A bucket owner has allowed another account's IAM users to upload or access objects in his bucket. The IAM user of Account A is trying to access an object created by the IAM user of Account B.
  </h1>
  <p>What will happen in this scenario?</p>

  <label for="optionA">A. It is not possible to give permission to multiple IAM users</label>
  <label for="optionB" class="correct-answer">B. AWS S3 will verify proper rights given by the owner of Account A, the bucket owner as well as by the IAM user B to the object ✓✓</label>
  <label for="optionC">C. The bucket policy may not be created as S3 will give error due to conflict of Access Rights</label>
  <label for="optionD">D. It is not possible that the IAM user of one account accesses objects of the other IAM user</label>
</form>
<form class="question-form">
  <h1 class="question">
    Which of the following IAM policy elements lets you specify an exception to a list of actions?
  </h1>

  <label for="optionA">A. NotException</label>
  <label for="optionB">B. ExceptionAction</label>
  <label for="optionC">C. Exception</label>
  <label for="optionD" class="correct-answer">D. NotAction ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company wants to replace its call system with a solution built using AWS managed services.
    The company call center would like the solution to receive calls, create contact flows, and scale to handle growth projections. The call center would also like the solution to use deep learning capabilities to recognize the intent of the callers and handle basic tasks, reducing the need to speak an agent. The solution should also be able to query business applications and provide relevant information back to calls as requested.
    Which services should the Solution Architect use to build this solution? (Choose three.)
  </h1>

  <label for="optionA">A. Amazon Rekognition to identify who is calling.</label>
  <label for="optionB" class="correct-answer">B. Amazon Connect to create a cloud-based contact center. ✓✓</label>
  <label for="optionC">C. Amazon Alexa for Business to build conversational interface.</label>
  <label for="optionD" class="correct-answer">D. AWS Lambda to integrate with internal systems. ✓✓</label>
  <label for="optionE" class="correct-answer">E. Amazon Lex to recognize the intent of the caller. ✓✓</label>
  <label for="optionF">F. Amazon SQS to add incoming callers to a queue.</label>
</form>
<form class="question-form">
  <h1 class="question">
    You are setting up some EBS volumes for a customer who has requested a setup which includes a RAID (redundant array of inexpensive disks). AWS has some recommendations for RAID setups. Which RAID setup is not recommended for Amazon EBS?
  </h1>

  <label for="optionA">A. RAID 1 only</label>
  <label for="optionB">B. RAID 5 only</label>
  <label for="optionC" class="correct-answer">C. RAID 5 and RAID 6 ✓✓</label>
  <label for="optionD">D. RAID 0 only</label>
</form>
<form class="question-form">
  <h1 class="question">
    You have been given the task to define multiple AWS Data Pipeline schedules for different activities in the same pipeline. Which of the following would successfully accomplish this task?
  </h1>

  <label for="optionA">A. Creating multiple pipeline definition files</label>
  <label for="optionB">B. Defining multiple pipeline definitions in your schedule objects file and associating the desired schedule to the correct activity via its schedule field</label>
  <label for="optionC" class="correct-answer">C. Defining multiple schedule objects in your pipeline definition file and associating the desired schedule to the correct activity via its schedule field ✓✓</label>
  <label for="optionD">D. Defining multiple schedule objects in the schedule field</label>
</form>
<form class="question-form">
  <h1 class="question">
    A Solutions Architect is designing the storage layer for a recently purchased application. The application will be running on Amazon EC2 instances and has the following layers and requirements:
  </h1>
  <p>- Data layer: A POSIX file system shared across many systems.<br>
     - Service layer: Static file content that requires block storage with more than 100k IOPS.</p>

  <label for="optionA">A. Data layer - Amazon S3</label>
  <label for="optionB">B. Data layer - Amazon EC2 Ephemeral Storage</label>
  <label for="optionC" class="correct-answer">C. Data layer - Amazon EFS ✓✓</label>
  <label for="optionD" class="correct-answer">D. Service layer - Amazon EBS volumes with Provisioned IOPS ✓✓</label>
  <label for="optionE">E. Service layer - Amazon EC2 Ephemeral Storage</label>
</form>


<form class="question-form">
  <h1 class="question">
    If a single condition within an IAM policy includes multiple values for one key, it will be evaluated using a logical ______.
  </h1>

  <label for="optionA" class="correct-answer">A. OR ✓✓</label>
  <label for="optionB">B. NAND</label>
  <label for="optionC">C. NOR</label>
  <label for="optionD">D. AND</label>
</form>
<form class="question-form">
  <h1 class="question">
    A large global financial services company has multiple business units. The company wants to allow Developers to try new services, but there are multiple compliance requirements for different workloads. The Security team is concerned about the access strategy for on-premises and AWS implementations. They would like to enforce governance for AWS services used by business team for regulatory workloads, including Payment Card Industry (PCI) requirements.<br><br>
    Which solution will address the Security team's concerns and allow the Developers to try new services?
  </h1>

  <label for="optionA">A. Implement a strong identity and access management model that includes users, groups, and roles in various AWS accounts. Ensure that centralized AWS CloudTrail logging is enabled to detect anomalies. Build automation with AWS Lambda to tear down unapproved AWS resources for governance.</label>

  <label for="optionB" class="correct-answer">B. Build a multi-account strategy based on business units, environments, and specific regulatory requirements. Implement SAML-based federation across all AWS accounts with an on-premises identity store. Use AWS Organizations and build organizational units (OUs) structure based on regulations and service governance. Implement service control policies across OUs. ✓✓</label>

  <label for="optionC">C. Implement a multi-account strategy based on business units, environments, and specific regulatory requirements. Ensure that only PCI-compliant services are approved for use in the accounts. Build IAM policies to give access to only PCI-compliant services for governance.</label>

  <label for="optionD">D. Build one AWS account for the company for the strong security controls. Ensure that all the service limits are raised to meet company scalability requirements. Implement SAML federation with an on- premises identity store, and ensure that only approved services are used in the account.</label>
</form>
<form class="question-form">
  <h1 class="question">
    The Solutions Architect manages a serverless application that consists of multiple API gateways, AWS Lambda functions, Amazon S3 buckets, and Amazon DynamoDB tables. Customers say that a few application components slow while loading dynamic images, and some are timing out with the "504 Gateway Timeout" error. While troubleshooting the scenario, the Solutions Architect confirms that DynamoDB monitoring metrics are at acceptable levels.<br><br>
    Which of the following steps would be optimal for debugging these application issues? (Choose two.)
  </h1>

  <label for="optionA">A. Parse HTTP logs in Amazon API Gateway for HTTP errors to determine the root cause of the errors.</label>

  <label for="optionB" class="correct-answer">B. Parse Amazon CloudWatch Logs to determine processing times for requested images at specified intervals. ✓</label>

  <label for="optionC">C. Parse VPC Flow Logs to determine if there is packet loss between the Lambda function and S3.</label>

  <label for="optionD" class="correct-answer">D. Parse AWS X-Ray traces and analyze HTTP methods to determine the root cause of the HTTP errors. ✓</label>

  <label for="optionE">E. Parse S3 access logs to determine if objects being accessed are from specific IP addresses to narrow the scope to geographic latency issues.</label>
</form>
<form class="question-form">
  <h1 class="question">
    Can a user configure a custom health check with Auto Scaling?
  </h1>

  <label for="optionA">A. Yes, but the configured data will not be saved to Auto Scaling.</label>

  <label for="optionB">B. No, only an ELB health check can be configured with Auto Scaling.</label>

  <label for="optionC" class="correct-answer">C. Yes ✓</label>

  <label for="optionD">D. No</label>
</form>
<form class="question-form">
  <h1 class="question">
    A Solutions Architect has created an AWS CloudFormation template for a three-tier application that contains an Auto Scaling group of Amazon EC2 instances running a custom AMI.<br><br>
    The Solutions Architect wants to ensure that future updates to the custom AMI can be deployed to a running stack by first updating the template to refer to the new AMI, and then invoking UpdateStack to replace the EC2 instances with instances launched from the new AMI.<br><br>
    How can updates to the AMI be deployed to meet these requirements?
  </h1>

  <label for="optionA" class="correct-answer">A. Create a change set for a new version of the template, view the changes to the running EC2 instances to ensure that the AMI is correctly updated, and then execute the change set. ✓</label>

  <label for="optionB">B. Edit the AWS::AutoScaling::LaunchConfiguration resource in the template, changing its DeletionPolicy to Replace.</label>

  <label for="optionC">C. Edit the AWS::AutoScaling::LaunchConfiguration resource in the template, inserting an UpdatePolicy attribute.</label>

  <label for="optionD">D. Create a new stack from the updated template. Once it is successfully deployed, modify the DNS records to point to the new stack and delete the old stack.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A web company is looking to implement an intrusion detection and prevention system into their deployed VPC. This platform should have the ability to scale to thousands of instances running inside of the VPC.<br><br>
    How should they architect their solution to achieve these goals?
  </h1>

  <label for="optionA">A. Configure servers running in the VPC using the host-based "route" commands to send all traffic through the platform to a scalable virtualized IDS/IPS.</label>

  <label for="optionB">B. Create a second VPC and route all traffic from the primary application VPC through the second VPC where the scalable virtualized IDS/IPS platform resides.</label>

  <label for="optionC">C. Configure an instance with monitoring software and the elastic network interface (ENI) set to promiscuous mode packet sniffing to see all traffic across the VPC.</label>

  <label for="optionD" class="correct-answer">✔ D. Configure each host with an agent that collects all network traffic and sends that traffic to the IDS/IPS platform for inspection.</label>
</form>
<form class="question-form">
  <h1 class="question">
    Out of the striping options available for the EBS volumes, which one has the following disadvantage:<br><br>
    "Doubles the amount of I/O required from the instance to EBS compared to RAID 0, because you're mirroring all writes to a pair of volumes, limiting how much you can stripe."
  </h1>

  <label for="optionA" class="correct-answer">✔ A. RAID 1</label>

  <label for="optionB">B. RAID 0</label>

  <label for="optionC">C. RAID 1+0 (RAID 10)</label>

  <label for="optionD">D. RAID 2</label>
</form>
<form class="question-form">
  <h1 class="question">
    You want to mount an Amazon EFS file system on an Amazon EC2 instance using DNS names.<br><br>
    Which of the following generic form of a mount target's DNS name must you use to mount the file system?
  </h1>

  <label for="optionA" class="correct-answer">✔ A. availability-zone.file-system-id.efs.aws-region.amazonaws.com</label>

  <label for="optionB">B. efs-system-id.availability-zone.file-aws-region.amazonaws.com</label>

  <label for="optionC">C. $file-system-id.$availability-zone.$efs.aws-region.$amazonaws.com</label>

  <label for="optionD">D. #aws-region.#availability-zone.#file-system-id.#efs.#amazonaws.com</label>
</form>
<form class="question-form">
  <h1 class="question">
    The AWS IT infrastructure that AWS provides, complies with the following IT security standards, including:
  </h1>

  <label for="optionA" class="correct-answer">✔ A. All of the above</label>

  <label for="optionB">B. FISMA, DIACAP, and FedRAMP</label>

  <label for="optionC">C. HIPAA, Cloud Security Alliance (CSA) and Motion Picture Association of America (MPAA)</label>

  <label for="optionD">D. PCI DSS Level 1, ISO 27001, ITAR and FIPS 140-2</label>

  <label for="optionE">E. SOC 1/SSAE 16/ISAE 3402 (formerly SAS 70 Type II), SOC 2 and SOC 3</label>
</form>
<form class="question-form">
  <h1 class="question">
    You are designing a social media site and are considering how to mitigate distributed denial-of-service (DDoS) attacks.<br>
    Which of the below are viable mitigation techniques? <strong>Choose 3 answers</strong>
  </h1>

  <label for="optionA">A. Add multiple elastic network Interfaces (ENIs) to each EC2 instance to increase the network bandwidth.</label>

  <label for="optionB" class="correct-answer">✔ B. Create processes and capabilities to quickly add and remove rules to the instance OS firewall.</label>

  <label for="optionC">C. Use Dedicated Instances to ensure that each instance has the maximum performance possible.</label>

  <label for="optionD" class="correct-answer">✔ D. Use an Amazon CloudFront distribution for both static and dynamic content.</label>

  <label for="optionE">E. Add alerts to Amazon CloudWatch to look for high Network In and CPU utilization.</label>

  <label for="optionF" class="correct-answer">✔ F. Use an Elastic Load Balancer with auto scaling groups at the web, app, and Amazon Relational Database Service (RDS) tiers.</label>
</form>
<form class="question-form">
  <h1 class="question">
    What is the maximum length for an instance profile name in AWS IAM?
  </h1>

  <label for="optionA">A. 512 characters</label>
  <label for="optionB" class="correct-answer">B. 128 characters ✓✓</label>
  <label for="optionC">C. 1024 characters</label>
  <label for="optionD">D. 64 characters</label>
</form>
<form class="question-form">
  <h1 class="question">
    In Amazon RDS for PostgreSQL, you can provision up to 3TB storage and 30,000 IOPS per database instance. For a workload with 50% writes and 50% reads running on a cr1.8xlarge instance, you can realize over 25,000 IOPS for PostgreSQL. However, by provisioning more than this limit, you may be able to achieve:
  </h1>

  <label for="optionA">A. higher latency and lower throughput.</label>
  <label for="optionB" class="correct-answer">B. lower latency and higher throughput ✓✓</label>
  <label for="optionC">C. higher throughput only.</label>
  <label for="optionD">D. higher latency only.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A user is accessing an EC2 instance on the SSH port for IP 10.20.30.40/32. Which one is a secure way to configure that the instance can be accessed only from this IP?
  </h1>

  <label for="optionA">A. In the security group, open port 22 for IP 10.20.30.40</label>
  <label for="optionB">B. In the security group, open port 22 for IP 10.20.30.0</label>
  <label for="optionC" class="correct-answer">C. In the security group, open port 22 for IP 10.20.30.40/32 ✓✓</label>
  <label for="optionD">D. In the security group, open port 22 for IP 10.20.30.40/0</label>
</form>
<form class="question-form">
  <h1 class="question">
    How is AWS readily distinguished from other vendors in the traditional IT computing landscape?
  </h1>

  <label for="optionA" class="correct-answer">A. Secure. Flexible. Cost-effective. Scalable and elastic. Experienced ✓✓</label>
  <label for="optionB">B. Secure.elastic. Experienced</label>
  <label for="optionC">C. Experienced. Scalable and elastic. Secure. Cost-effective. Reliable</label>
  <label for="optionD">D. Flexible. Cost-effective. Dynamic. Secure. Experienced.</label>
</form>
<form class="question-form">
  <h1 class="question">
    Regarding Amazon SNS, you can send notification messages to mobile devices through any of the following supported push notification services, EXCEPT:
  </h1>

  <label for="optionA" class="correct-answer">A. Microsoft Windows Mobile Messaging (MWMM) ✓✓</label>
  <label for="optionB">B. Google Cloud Messaging for Android (GCM)</label>
  <label for="optionC">C. Amazon Device Messaging (ADM)</label>
  <label for="optionD">D. Apple Push Notification Service (APNS)</label>
</form>
<form class="question-form">
  <h1 class="question">
    Identify a true statement about using an IAM role to grant permissions to applications running on Amazon EC2 instances.
  </h1>

  <label for="optionA">A. When AWS credentials are rotated, developers have to update only the root Amazon EC2 instance that uses their credentials.</label>
  <label for="optionB">B. When AWS credentials are rotated, developers have to update only the Amazon EC2 instance on which the password policy was applied and which uses their credentials.</label>
  <label for="optionC" class="correct-answer">C. When AWS credentials are rotated, you don't have to manage credentials and you don't have to worry about long-term security risks. ✓✓</label>
  <label for="optionD">D. When AWS credentials are rotated, you must manage credentials and you should consider precautions for long-term security risks.</label>
</form>
<form class="question-form">
  <h1 class="question">
    In the context of AWS Cloud Hardware Security Module (HSM), does your application need to reside in the same VPC as the CloudHSM instance?
  </h1>

  <label for="optionA" class="correct-answer">
    A. No, but the server or instance on which your application and the HSM client is running must have network (IP) reachability to the HSM. ✓✓
  </label>
  <label for="optionB">B. Yes, always</label>
  <label for="optionC">C. No, but they must reside in the same Availability Zone.</label>
  <label for="optionD">D. No, but it should reside in same Availability Zone as the DB instance.</label>
</form>
<form class="question-form">
  <h1 class="question">
    You are the new IT architect in a company that operates a mobile sleep tracking application. <br><br>
    When activated at night, the mobile app is sending collected data points of 1 kilobyte every 5 minutes to your backend. <br><br>
    The backend takes care of authenticating the user and writing the data points into an Amazon DynamoDB table. <br><br>
    Every morning, you scan the table to extract and aggregate last night's data on a per user basis, and store the results in Amazon S3. Users are notified via Amazon SNS mobile push notifications that new data is available, which is parsed and visualized by the mobile app. Currently you have around 100k users who are mostly based out of North America. <br><br>
    You have been tasked to optimize the architecture of the backend system to lower cost. <br><br>
    What would you recommend? <strong>Choose 2 answers</strong>
  </h1>

  <label for="optionA">A. Have the mobile app access Amazon DynamoDB directly Instead of JSON files stored on Amazon S3.</label>
  <label for="optionB">B. Write data directly into an Amazon Redshift cluster replacing both Amazon DynamoDB and Amazon S3.</label>
  <label for="optionC" class="correct-answer">
    C. Introduce an Amazon SQS queue to buffer writes to the Amazon DynamoDB table and reduce provisioned write throughput. ✓✓
  </label>
  <label for="optionD" class="correct-answer">
    D. Introduce Amazon ElastiCache to cache reads from the Amazon DynamoDB table and reduce provisioned read throughput. ✓✓
  </label>
  <label for="optionE">E. Create a new Amazon DynamoDB table each day and drop the one for the previous day after its data is on Amazon S3.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company needs to store large, important documents within the application with the following requirements: <br><br>
    - The data must be highly durable and available.<br>
    - The data must always be encrypted at rest and in transit.<br>
    - The encryption key must be managed by the company and rotated periodically.<br><br>
    Which of the following solutions should the Solutions Architect recommend?
  </h1>

  <label for="optionA">A. Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes.</label>

  <label for="optionB" class="correct-answer">
    B. Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption. ✓✓
  </label>

  <label for="optionC">C. Use Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest.</label>

  <label for="optionD">D. Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data.</label>
</form>
<form class="question-form">
  <h1 class="question">
    When using Numeric Conditions within IAM, short versions of the available comparators can be used instead of the more verbose versions. Which of the following is the short version of the Numeric Condition "<strong>NumericLessThanEquals</strong>"?
  </h1>

  <label for="optionA">A. nvvumlteq</label>

  <label for="optionB">B. xxnumlteql</label>

  <label for="optionC" class="correct-answer">
    C. numlteq ✓✓
  </label>
  <form class="question-form">
  <h1 class="question">
    Your company is storing millions of sensitive transactions across thousands of 100-GB files that must be encrypted in transit and at rest. Analysts concurrently depend on subsets of files, which can consume up to 5 TB of space, to generate simulations that can be used to steer business decisions. You are required to design an AWS solution that can cost effectively accommodate the long-term storage and in-flight subsets of data.
  </h1>

  <label for="optionA">A. Use HDFS on Amazon EMR, and run simulations on subsets in ephemeral drives on Amazon EC2.</label>

  <label for="optionB">B. Use HDFS on Amazon Elastic MapReduce (EMR), and run simulations on subsets in-memory on Amazon Elastic Compute Cloud (EC2).</label>

  <label for="optionC" class="correct-answer">
    C.Use Amazon Simple Storage Service (S3) with server-side encryption, and run simulations on subsets in ephemeral drives on Amazon EC2. ✓✓
  </label>

  <label for="optionD">D. Use Amazon DynamoDB on Amazon EC2.</label>

  <label for="optionE">E. Store the full data set in encrypted Amazon Elastic Block Store (EBS) volumes, and regularly capture snapshots that can be cloned to EC2 workstations.</label>
</form>
<form class="question-form">
  <h1 class="question">
    You are designing the network infrastructure for an application server in Amazon VPC. Users will access all the application instances from the Internet, as well as from an on-premises network.
    The on-premises network is connected to your VPC over an AWS Direct Connect link.
    How would you design routing to meet the above requirements?
  </h1>

  <label for="optionA">A. Configure a single routing table with two default routes: one to the Internet via an Internet gateway, the other to the on-premises network via the VPN gateway. Use this routing table across all subnets in your VPC.</label>

  <label for="optionB" class="correct-answer">
    B. Configure a single routing table with a default route via the Internet gateway.
    Propagate specific routes for the on-premises networks via BGP on the AWS Direct Connect customer router.
    Associate the routing table with all VPC subnets. ✓✓
  </label>

  <label for="optionC">C. Configure two routing tables: one that has a default route via the Internet gateway, and another that has a default route via the VPN gateway. Associate both routing tables with each VPC subnet.</label>

  <label for="optionD">D. Configure a single routing table with a default route via the Internet gateway. Propagate a default route via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets.</label>
</form>
<form class="question-form">
  <h1 class="question">
    True or False: In Amazon ElastiCache replication groups of Redis, for performance tuning reasons, you can change the roles of the cache nodes within the replication group, with the primary and one of the replicas exchanging roles.
  </h1>

  <label for="optionA">A. True, however, you get lower performance.</label>

  <label for="optionB">B. FALSE</label>

  <label for="optionC" class="correct-answer">
    C. TRUE ✓✓
  </label>

  <label for="optionD">D. False, you must recreate the replication group to improve performance tuning.</label>
</form>
<form class="question-form">
  <h1 class="question">
    Which of the following is not included in the metrics sent from Billing to Amazon CloudWatch?
  </h1>

  <label for="optionA">A. Recurring fees for AWS products and services</label>

  <label for="optionB">B. Total AWS charges</label>

  <label for="optionC" class="correct-answer">
    C. One-time charges and refunds ✓✓
  </label>

  <label for="optionD">D. Usage charges for AWS products and services</label>
</form>
<form class="question-form">
  <h1 class="question">
    Once the user has set ElastiCache for an application and it is up and running, which services does Amazon <u>not</u> provide for the user:
  </h1>

  <label for="optionA">A. The ability for client programs to automatically identify all of the nodes in a cache cluster, and to initiate and maintain connections to all of these nodes</label>

  <label for="optionB">B. Automating common administrative tasks such as failure detection and recovery, and software patching</label>

  <label for="optionC" class="correct-answer">
    C. Providing default Time To Live (TTL) in the AWS ElastiCache Redis implementation for different types of data ✓✓
  </label>

  <label for="optionD">D. Providing detailed monitoring metrics associated with your Cache Nodes, enabling you to diagnose and react to issues very quickly</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has released a new version of a website to target an audience in Asia and South America. The website's media assets are hosted on Amazon S3 and have an Amazon CloudFront distribution to improve end-user performance. However, users are having a poor login experience because the authentication service is only available in the us-east-1 AWS Region.<br><br>
    <strong>How can the Solutions Architect improve the login experience and maintain high security and performance with minimal management overhead?</strong>
  </h1>

  <label for="optionA">
    A. Replicate the setup in each new geography and use Amazon Route 53 geo-based routing to route traffic to the AWS Region closest to the users.
  </label>

  <label for="optionB">
    B. Use an Amazon Route 53 weighted routing policy to route traffic to the CloudFront distribution. Use CloudFront cached HTTP methods to improve the user login experience.
  </label>

  <label for="optionC" class="correct-answer">
    C. Use Amazon Lambda@Edge attached to the CloudFront viewer request trigger to authenticate and authorize users by maintaining a secure cookie token with a session expiry to improve the user experience in multiple geographies. ✓✓
  </label>

  <label for="optionD">
    D. Replicate the setup in each geography and use Network Load Balancers to route traffic to the authentication service running in the closest region to users.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A Solutions Architect is responsible for redesigning a legacy Java application to improve its availability, data durability, and scalability. Currently, the application runs on a single high-memory Amazon EC2 instance. It accepts HTTP requests from upstream clients, adds them to an in-memory queue, and responds with a 200 status. A separate application thread reads items from the queue, processes them, and persists the results to an Amazon RDS MySQL instance.<br><br>

    The processing time for each item takes 90 seconds on average, most of which is spent waiting on external service calls, but the application is written to process multiple items in parallel.<br><br>

    Traffic to this service is unpredictable. During periods of high load, items may sit in the internal queue for over an hour while the application processes the backlog. In addition, the current system has issues with availability and data loss if the single application node fails.<br><br>

    Clients that access this service cannot be modified. They expect to receive a response to each HTTP request they send within 10 seconds before they will time out and retry the request.<br><br>

    <strong>Which approach would improve the availability and durability of the system while decreasing the processing latency and minimizing costs?</strong>
  </h1>

  <label for="optionA">
    A. Update the application to use a Redis task queue instead of the in-memory queue. Build a Docker container image for the application. Create an Amazon ECS task definition that includes the application container and a separate container to host Redis. Deploy the new task definition as an ECS service using AWS Fargate and enable Auto Scaling.
  </label>

  <label for="optionB">
    B. Modify the application to use Amazon DynamoDB instead of Amazon RDS. Configure Auto Scaling for the DynamoDB table. Deploy the application within an Auto Scaling group with a scaling policy based on CPU utilization. Back the in-memory queue with a memory-mapped file to an instance store volume and periodically write that file to Amazon S3.
  </label>

  <label for="optionC" class="correct-answer">
    C. Create an Amazon API Gateway REST API that uses a service proxy to put items in an Amazon SQS queue. Extract the core processing code from the existing application and update it to pull items from Amazon SQS instead of an in-memory queue. Deploy the new processing application to smaller EC2 instances within an Auto Scaling group that scales dynamically based on the approximate number of messages in the Amazon SQS queue. ✓✓
  </label>

  <label for="optionD">
    D. Create an Amazon API Gateway REST API that uses Lambda proxy integration to pass requests to an AWS Lambda function. Migrate the core processing code to a Lambda function and write a wrapper class that provides a handler method that converts the proxy events to the internal application data model and invokes the processing module.
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    When using string conditions within IAM, short versions of the available comparators can be used instead of the more verbose ones.<br><br>

    <strong>streqi is the short version of the _____ string condition.</strong>
  </h1>

  <label for="optionA" class="correct-answer">
    A. StringEqualsIgnoreCase ✓✓
  </label>

  <label for="optionB">
    B. StringNotEqualsIgnoreCase
  </label>

  <label for="optionC">
    C. StringLikeStringEquals
  </label>

  <label for="optionD">
    D. StringNotEquals
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    A large company experienced a drastic increase in its monthly AWS spend. This is after Developers accidentally launched Amazon EC2 instances in unexpected regions. The company has established practices around least privileges for Developers and controls access to on-premises resources using Active Directory groups. The company now wants to control costs by restricting the level of access that Developers have to the AWS Management Console without impacting their productivity. The company would also like to allow Developers to launch Amazon EC2 in only one region, without limiting access to other services in any region.<br><br>
    
    <strong>How can this company achieve these new security requirements while minimizing the administrative burden on the Operations team?</strong>
  </h1>

  <label for="optionA">
    A. Set up SAML-based authentication tied to an IAM role that has an AdministrativeAccess managed policy attached to it. Attach a customer managed policy that denies access to Amazon EC2 in each region except for the one required.
  </label>

  <label for="optionB">
    B. Create an IAM user for each Developer and add them to the developer IAM group that has the PowerUserAccess managed policy attached to it. Attach a customer managed policy that allows the Developers access to Amazon EC2 only in the required region.
  </label>

  <label for="optionC">
    C. Set up SAML-based authentication tied to an IAM role that has a PowerUserAccess managed policy and a customer managed policy that deny all the Developers access to any AWS services except AWS Service Catalog. Within AWS Service Catalog, create a product containing only the EC2 resources in the approved region.
  </label>

  <label for="optionD" class="correct-answer">
    D. Set up SAML-based authentication tied to an IAM role that has the PowerUserAccess managed policy attached to it. Attach a customer managed policy that denies access to Amazon EC2 in each region except for the one required. ✓✓
  </label>
</form>
<form class="question-form">
  <h1 class="question">
    Dave is the main administrator in Example Corp., and he decides to use paths to help delineate the users in the company and set up a separate administrator group for each path-based division. Following is a subset of the full list of paths he plans to use:<br><br>
    • /marketing<br>
    • /sales<br>
    • /legal<br><br>
    
    Dave creates an administrator group for the marketing part of the company and calls it <strong>Marketing_Admin</strong>. He assigns it the <code>/marketing</code> path. The group's ARN is <code>arn:aws:iam::123456789012:group/marketing/Marketing_Admin</code>.<br><br>
    
    Dave assigns the following policy to the <strong>Marketing_Admin</strong> group that gives the group permission to use all IAM actions with all groups and users in the <code>/marketing</code> path. The policy also gives the <strong>Marketing_Admin</strong> group permission to perform any AWS S3 actions on the objects in the portion of the corporate bucket.<br><br>
    
    <pre>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Action": "iam:*",
      "Resource": [
        "arn:aws:iam::123456789012:group/marketing/*",
        "arn:aws:iam::123456789012:user/marketing/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": "arn:aws:s3:::example_bucket/marketing/*"
    },
    {
      "Effect": "Allow",
      "Action": "s3:ListBucket*",
      "Resource": "arn:aws:s3:::example_bucket",
      "Condition": {
        "StringLike": {
          "s3:prefix": "marketing/*"
        }
      }
    }
  ]
}</pre>
    
    <strong>Does this policy allow the Marketing_Admin group to manage IAM users and groups in the /marketing path?</strong>
  </h1>

  <label for="optionA" class="correct-answer">
    A. False ✓✓
  </label>

  <label for="optionB">
    B. True
  </label>
</form>


 





 <form class="question-form">
  <h1 class="question">
    A company wants to migrate its content sharing web application hosted on Amazon EC2 to a serverless architecture.
  </h1>
  <p>Which solution will meet these requirements?</p>

  <label for="optionA">A. Use AWS CDK to define infrastructure. Use EC2 Image Builder to create AMIs for deployment.</label>
  <label for="optionB" class="correct-answer">B. Use AWS CloudFormation to deploy Amazon S3, Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. ✓✓</label>
  <label for="optionC">C. Use AWS Elastic Beanstalk to deploy a Docker-based application.</label>
  <label for="optionD">D. Use AWS OpsWorks with custom Chef recipes to deploy the application and supporting resources.</label>
</form>

<form class="question-form">
  <h1 class="question">
    A company's application is currently deployed to a single AWS Region. The company wants to improve the application's availability by using multiple Regions.
  </h1>
  <p>Which combination of steps should a solutions architect take to migrate to a multi-Region architecture? (Choose three.)</p>

  <label for="optionA">A. Create a new DynamoDB table and configure synchronous replication between the tables.</label>
  <label for="optionB">B. Create new ALB and Auto Scaling group global resources in a backup Region.</label>
  <label for="optionC" class="correct-answer">C. Create new ALB and Auto Scaling group resources in the second Region. ✓✓</label>
  <label for="optionD" class="correct-answer">D. Create Amazon Route 53 records with a failover routing policy. ✓✓</label>
  <label for="optionE">E. Create Amazon Route 53 aliases for both Regions using a latency-based routing policy.</label>
  <label for="optionF" class="correct-answer">F. Convert the DynamoDB table to a global table. ✓✓</label>
</form>




   <form class="question-form">
  <h1 class="question">
    A DevOps engineer used an AWS CloudFormation custom resource...
  </h1>
  <p>Which action should the engineer take to ensure CloudFormation receives a successful response?</p>

  <label for="optionA">A. Ensure the Lambda function code has exited successfully.</label>
  <label for="optionB" class="correct-answer">B. Ensure the Lambda function code returns a response to the pre-signed URL. ✓✓</label>
  <label for="optionC">C. Ensure the Lambda function IAM role has cloudformation:UpdateStack...</label>
  <label for="optionD">D. Ensure the Lambda function IAM role has ds:ConnectDirectory...</label>
</form>

<form class="question-form">
  <h1 class="question">
    A company plans to stop using Amazon EC2 key pairs for SSH access...
  </h1>
  <p>Which combination of steps will allow secure and auditable access to the instances? (Choose two.)</p>

  <label for="optionA">A. Allow inbound access to TCP port 22...</label>
  <label for="optionB" class="correct-answer">B. Attach an IAM policy... ✓✓</label>
  <label for="optionC" class="correct-answer">C. Create a VPC endpoint for Systems Manager... ✓✓</label>
  <label for="optionD">D. Deploy a new EC2 instance...</label>
  <label for="optionE">E. Remove any default routes...</label>
</form>

<form class="question-form">
  <h1 class="question">
    A company runs an application with an Amazon EC2 and on-premises configuration...
  </h1>
  <p>Which combination of actions should the company take to enable patching of all servers? (Choose three.)</p>

  <label for="optionA" class="correct-answer">A. Add the physical machines... ✓✓</label>
  <label for="optionB" class="correct-answer">B. Attach an IAM role to the EC2 instances... ✓✓</label>
  <label for="optionC">C. Create IAM access keys...</label>
  <label for="optionD">D. Execute an AWS Systems Manager Automation document...</label>
  <label for="optionE">E. Use Amazon CloudWatch Events scheduled events...</label>
  <label for="optionF" class="correct-answer">F. Use AWS Systems Manager Maintenance Windows... ✓✓</label>
</form>

<form class="question-form">
  <h1 class="question">
    Which one of the following components should not influence an organization’s security policy?
  </h1>
  <p>Select the option that is least likely to shape a security policy.</p>

  <label for="optionA">A. Business objectives</label>
  <label for="optionB">B. Regulatory requirements</label>
  <label for="optionC">C. Risk</label>
  <label for="optionD">D. Cost–benefit analysis</label>
  <label for="optionE" class="correct-answer">E. Current firewall limitations ✓✓</label>
</form>


    <form class="question-form">
  <h1 class="question">
    A Solutions Architect is working with a company that is extremely sensitive to its IT costs and wishes to implement controls that will result in a predictable AWS spend each month.
  </h1>
  <p>Which combination of steps can help the company control and monitor its monthly AWS usage to achieve a cost that is as close as possible to the target amount? (Choose three.)</p>
  
  <label for="optionA">A. Implement an IAM policy that requires users to specify a 'workload' tag for cost allocation when launching Amazon EC2 instances.</label>
  <label for="optionB">B. Contact AWS Support and ask that they apply limits to the account so that users are not able to launch more than a certain number of instance types.</label>
  <label for="optionC" class="correct-answer">C. Purchase all upfront Reserved Instances that cover 100% of the account's expected Amazon EC2 usage. ✓✓</label>
  <label for="optionD">D. Place conditions in the users' IAM policies that limit the number of instances they are able to launch.</label>
  <label for="optionE" class="correct-answer">E. Define 'workload' as a cost allocation tag in the AWS Billing and Cost Management console. ✓✓</label>
  <label for="optionF" class="correct-answer">F. Set up AWS Budgets to alert and notify when a given workload is expected to exceed a defined cost. ✓✓</label>
</form>

    <!-- THIS IS A QUESTION WITHOUT THE WRAPPER, WHICH WILL NOW BE HANDLED -->
    <form class="question-form">
      <h1 class="question">Consider the following statements about the AAA architecture:</h1>
      <p>I. Authentication deals with the question “Who is the user?”</p>
      <p>II. Authorization addresses the question “What is the user allowed to do?”</p>
      <p>III. Accountability answers the question “What did the user do?”</p>
      <h1 class="question">Which of the following is correct?</h1>
      <label for="optionA">A. Only I is correct.</label>
      <label for="optionB">B. Only II is correct.</label>
      <label for="optionC">C. I, II, and III are correct.</label>
      <label for="optionD" class="correct-answer">D. I and II are correct. ✓✓</label>
      <label for="optionE">E. II and III are correct.</label>
    </form>


    <form class="question-form">
      <h1 class="question">Consider the following statements about the AAA architecture:</h1>
      <p>I. Authentication deals with the question “Who is the user?”</p>
      <p>II. Authorization addresses the question “What is the user allowed to do?”</p>
      <p>III. Accountability answers the question “What did the user do?”</p>
      <h1 class="question">Which of the following is correct?</h1>
      <label for="optionA">A. Only I is correct.</label>
      <label for="optionB">B. Only II is correct.</label>
      <label for="optionC">C. I, II, and III are correct.</label>
      <label for="optionD" class="correct-answer">D. I and II are correct. &#x2713;&#x2713;</label>
      <label for="optionE">E. II and III are correct.</label>
    </form>
    <form class="question-form">
      <h1 class="question">What is the difference between denial-of-service (DoS) and distributed denial-of-service
        (DDoS) attacks?</h1>
      <label for="optionA">A. DDoS attacks have many targets, whereas DoS attacks have only one each.</label>
      <label for="optionB">B. DDoS attacks target multiple networks, whereas DoS attacks target a single
        network.</label>
      <label for="optionC" class="correct-answer">C. DDoS attacks have many sources, whereas DoS attacks have only one
        each. &#x2713;&#x2713;</label>
      <label for="optionD">D. DDoS attacks target multiple layers of the OSI model and DoS attacks only one.</label>
      <label for="optionE">E. DDoS attacks are synonymous with DoS attacks</label>
    </form>
    <form class="question-form">
      <h1 class="question">Which of the following options is incorrect?</h1>
      <label for="optionA">A. A firewall is a security system aimed at isolating specific areas of the network and delimiting domains of trust.</label>
      <label for="optionB">B. Generally speaking, the web application firewall (WAF) is a specialized security element that acts as a full-reverse proxy, protecting applications that are accessed through HTTP.</label>
      <label for="optionC" class="correct-answer">C. Whereas intrusion prevention system (IPS) devices handle only copies of the packets and are mainly concerned with monitoring and alerting tasks, intrusion detection system (IDS) solutions are deployed inline in the traffic flow and have the inherent design goal of avoiding actual damage to systems. &#x2713;&#x2713;</label>
      <label for="optionD">D. Security information and event management (SIEM) solutions are designed to collect security-related logs as well as flow information generated by systems (at the host or the application level), networking devices, and dedicated defense elements such as firewalls, IPSs, IDSs, and antivirus software.</label>
    </form>



    <form class="question-form">
      <h1 class="question">In the standard shared responsibility model, AWS is responsible for which of the following options?</h1>
      <label for="optionA">A. Regions, availability zones, and data encryption</label>
      <label for="optionB">B. Hardware, firewall configuration, and hypervisor software</label>
      <label for="optionC" class="correct-answer">C. Hypervisor software, regions, and availability zones &#x2713;&#x2713;</label>
      <label for="optionD">D. Network traffic protection and identity and access management</label>
    </form>
    <form class="question-form">
      <h1 class="question">Which AWS service allows you to generate compliance reports that enable you to evaluate the AWS security controls and posture?</h1>
      <label for="optionA">A. AWS Trusted Advisor</label>
      <label for="optionB">B. AWS Well-Architected Tool</label>
      <label for="optionC" class="correct-answer">C. AWS Artifact &#x2713;&#x2713;</label>
      <label for="optionD">D. Amazon Inspector</label>
    </form>
    <form class="question-form">
      <h1 class="question">Which of the following contains a definition that is not a pillar from the AWS Well-Architected Framework?</h1>
      <label for="optionA">A. Security and operational excellence</label>
      <label for="optionB">B. Reliability and performance efficiency</label>
      <label for="optionC" class="correct-answer">C. Cost optimization and availability &#x2713;&#x2713;</label>
      <label for="optionD">D. Security and performance efficiency</label>
    </form>
    <form class="question-form">
      <h1 class="question">Which of the following services provides a set of APIs that control access to your resources on the AWS Cloud?</h1>
      <label for="optionA">A. AWS AAA</label>
      <label for="optionB" class="correct-answer">B. AWS IAM &#x2713;&#x2713;</label>
      <label for="optionC">C. AWS Authenticator</label>
      <label for="optionD">D. AWS AD</label>
    </form>
    <form class="question-form">
      <h1 class="question">Regarding AWS IAM principals, which option is not correct?</h1>
      <label for="optionA">A. A principal is an IAM entity that has permission to interact with resources in the AWS Cloud.</label>
      <label for="optionB" class="correct-answer">B. They can only be permanent. &#x2713;&#x2713;</label>
      <label for="optionC">C. They can represent a human user, a resource, or an application.</label>
      <label for="optionD">D. They have three types: root users, IAM users, and roles.</label>
    </form>
    <form class="question-form">
      <h1 class="question">Which of the following is not a recommendation for protecting your root user credentials?</h1>
      <label for="optionA">A. Use a strong password to help protect account-level access to the management console.</label>
      <label for="optionB">B. Enable MFA on your AWS root user account.</label>
      <label for="optionC">C. Do not create an access key for programmatic access to your root user account unless such a procedure is mandatory.</label>
      <label for="optionD" class="correct-answer">D. If you must maintain an access key to your root user account, you should never rotate it using the AWS Console. &#x2713;&#x2713;</label>
    </form>
    <form class="question-form">
      <h1 class="question">In AWS Config, which option is not correct?</h1>
      <label for="optionA">A. The main goal of AWS Config is to record configuration and the changes of the resources.</label>
      <label for="optionB">B. AWS Config Rules can decide if a change is good or bad and if it needs to execute an action.</label>
      <label for="optionC" class="correct-answer">C. AWS Config cannot integrate with external resources like on-premises servers and applications. &#x2713;&#x2713;</label>
      <label for="optionD">D. AWS Config can provide configuration history files, configuration snapshots, and configuration streams.</label>
    </form>
    <form class="question-form">
      <h1 class="question">AWS CloudTrail is the service in charge of keeping records of API calls to the AWS Cloud. Which option is not a type of AWS CloudTrail event?</h1>
      <label for="optionA">A. Management</label>
      <label for="optionB">B. Insights</label>
      <label for="optionC">C. Data</label>
      <label for="optionD" class="correct-answer">D. Control &#x2713;&#x2713;</label>
    </form>
    <form class="question-form">
      <h1 class="question">In Amazon VPCs, which of the following is not correct?</h1>
      <label for="optionA">A. VPC is the acronym of Virtual Private Cloud.</label>
      <label for="optionB">B. VPCs do not extend beyond an AWS region.</label>
      <label for="optionC" class="correct-answer">C. You can deploy only private IP addresses from RFC 1918 within VPCs. &#x2713;&#x2713;</label>
      <label for="optionD">D. You can configure your VPC to not share hardware with other AWS accounts.</label>
    </form>
    <form class="question-form">
      <h1 class="question">In NAT gateways, which option is not correct?</h1>
      <label for="optionA">A. NAT gateways are always positioned in public subnets.</label>
      <label for="optionB">B. Route table configuration is usually required to direct traffic to these devices.</label>
      <label for="optionC" class="correct-answer">C. NAT gateways are highly available by default. &#x2713;&#x2713;</label>
      <label for="optionD">D. Amazon CloudWatch automatically monitors traffic flowing through NAT gateways</label>
    </form>
    <form class="question-form">
      <h1 class="question">In security groups, which option is not correct?</h1>
      <label for="optionA">A. Security groups only have allow (permit) rules.</label>
      <label for="optionB">B. The default security group allows all inbound communications from resources that are associated to the same security group.</label>
      <label for="optionC" class="correct-answer">C. You cannot have more than one security group associated to an instance’s ENI. &#x2713;&#x2713;</label>
      <label for="optionD">D. The default security group allows all outbound communications to any destination.</label>
    </form>
    <form class="question-form">
      <h1 class="question">In network ACLs, which option is not correct?</h1>
      <label for="optionA">A. They can be considered an additional layer of traffic filtering to security groups.</label>
      <label for="optionB">B. Network ACLs have allow and deny rules.</label>
      <label for="optionC" class="correct-answer">C. The default network ACL has only one inbound rule, denying all traffic from all protocols, all port ranges, from any source. &#x2713;&#x2713;</label>
      <label for="optionD">D. A subnet can be associated with only one network ACL at a time.</label>
    </form>
    <form class="question-form">
      <h1 class="question">In AWS KMS, which option is not correct?</h1>
      <label for="optionA">A. KMS can integrate with Amazon S3 and Amazon EBS.</label>
      <label for="optionB" class="correct-answer">B. KMS can be used to generate SSH access keys for Amazon EC2 instances. &#x2713;&#x2713;</label>
      <label for="optionC">C. KMS is considered multitenant, not a dedicated hardware security module.</label>
      <label for="optionD">D. KMS can be used to provide data-at-rest encryption for RDS, Aurora, DynamoDB, and Redshift databases</label>
    </form>
    <form class="question-form">
      <h1 class="question">Which option is not correct in regard to AWS KMS customer master keys?</h1>
      <label for="optionA">A. A CMK is a 256-bit AES for symmetric keys.</label>
      <label for="optionB">B. A CMK has a key ID, an alias, and an ARN (Amazon Resource Name).</label>
      <label for="optionC" class="correct-answer">C. A CMK can also use IAM users, IAM groups, and IAM roles. . &#x2713;&#x2713;</label>
      <label for="optionD">D. A CMK has two policies roles: key administrators and key users</label>
    </form>
    <form class="question-form">
      <h1 class="question">Which of the following actions is not recommended when an Amazon EC2 instance is compromised by malware?</h1>
      <label for="optionA">A. Take a snapshot of the EBS volume at the time of the incident.</label>
      <label for="optionB" class="correct-answer">B. Change its security group accordingly and reattach any IAM role attached to the instance. &#x2713;&#x2713;</label>
      <label for="optionC">C. Tag the instance as compromised together with an AWS IAM policy that explicitly restricts all operations related to the instance, the incident response, and forensics teams.</label>
      <label for="optionD">D. When the incident forensics team wants to analyze the instance, they should deploy it into a totally isolated environment—ideally a private subnet.</label>
    </form>
    <form class="question-form">
      <h1 class="question">Which of the following actions is recommended when temporary credentials from an Amazon EC2 instance are inadvertently made public?</h1>
      <label for="optionA" class="correct-answer">A. You should assume that the access key was compromised and revoke it immediately. &#x2713;&#x2713;</label>
      <label for="optionB">B. You should try to locate where the key was exposed and inform AWS.</label>
      <label for="optionC">C. You should not reevaluate the IAM roles attached to the instance.</label>
      <label for="optionD">D. You should avoid rotating your key</label>
    </form>
    <form class="question-form">
      <h1 class="question">Which of the following options may not be considered a security automation trigger?</h1>
      <label for="optionA">A. Unsafe configurations from AWS Config or Amazon Inspector</label>
      <label for="optionB">B. AWS Security Hub findings</label>
      <label for="optionC" class="correct-answer">C. Systems Manager Automation documents &#x2713;&#x2713;</label>
      <label for="optionD">D. Event from Amazon CloudWatch Events</label>
    </form>
    <form class="question-form">
      <h1 class="question">Which of the following options may not be considered a security automation response task?</h1>
      <label for="optionA">A. An AWS Lambda function can use AWS APIs to change security groups or network ACLs.</label>
      <label for="optionB">B. A Systems Manager Automation document execution run.</label>
      <label for="optionC">C. Systems Manager Run Command can be used to execute commands to multiple hosts.</label>
      <label for="optionD" class="correct-answer">D. Apply a thorough forensic analysis in an isolated instance. &#x2713;&#x2713;</label>
    </form>
    <form class="question-form">
      <h1 class="question">Which of the following options may not be considered a security troubleshooting tool for security in AWS Cloud environments?</h1>
      <label for="optionA">A. AWS CloudTrail</label>
      <label for="optionB">B. Amazon CloudWatch Logs</label>
      <label for="optionC" class="correct-answer">C. AWS Key Management Service &#x2713;&#x2713;</label>
      <label for="optionD">D. Amazon EventBridge</label>
    </form>
    <form class="question-form">
      <h1 class="question">Right after you correctly deploy VPC peering between two VPCs (A and B), inter-VPC traffic is still not happening. What is the most probable cause?</h1>
      <label for="optionA">A. The peering must be configured as transitive.</label>
      <label for="optionB" class="correct-answer">B. The route tables are not configured. &#x2713;&#x2713;</label>
      <label for="optionC">C. You need a shared VPC.</label>
      <label for="optionD">D. You need to configure a routing protocol</label>
    </form>
    <form class="question-form">
      <h1 class="question">A good mental exercise for your future cloud security design can start with the analysis of how AWS native security services and features (as well as third-party security solutions) can replace your traditional security controls. Which of the options is not a valid mapping between traditional security controls and potential AWS security controls?</h1>
      <label for="optionA">A. Network segregation (such as firewall rules and router access control lists) and security groups and network ACLs, Web Application Firewall (WAF)</label>
      <label for="optionB">B. Data encryption at rest and Amazon S3 server-side encryption, Amazon EBS encryption, Amazon RDS encryption, and other AWS KMS-enabled encryption features</label>
      <label for="optionC" class="correct-answer">C. Monitor intrusion and implementing security controls at the operating system level versus Amazon GuardDuty &#x2713;&#x2713;</label>
      <label for="optionD">D. Role-based access control (RBAC) versus AWS IAM, Active Directory integration through IAM groups, temporary security credentials, AWS Organizations</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company needs to allow network traffic between these VPCs. Approximately 500 GB of data transfer will occur between the VPCs each month.</h1>
      <h1 class="question">What is the MOST cost-effective solution to connect these VPCs?</h1>
      <label for="optionA">A. Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to use the transit gateway for inter-VPC communication.</label>
      <label for="optionB">B. Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each VPC to use the VPN tunnel for inter-VPC communication.</label>
      <label for="optionC" class="correct-answer">C. Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication. &#x2713;&#x2713;</label>
      <label for="optionD">D. Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of each VPC to use the Direct Connect connection for inter-VPC communication.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company has a VPC in the us-west-1 Region and another VPC in the ap-southeast-2 Region. Network engineers set up an AWS Direct Connect connection from their data center to the us-east-1 Region. They create a private virtual interface (VIF) that references a Direct Connect gateway, which is then connected to virtual private gateways in both VPCs. When the setup is complete, the engineers cannot access resources in us-west-1 from ap-southeast-2.</h1>
      <h1 class="question">What should the network engineers do to resolve this issue?</h1>
      <label for="optionA">A. Add the subnet range for the VPCs in us-west-1 and ap-southeast-2 to the route tables for both VPCs. Add the Direct Connect gateway as a target.</label>
      <label for="optionB">B. Configure the Direct Connect gateway to route traffic between the VPCs in ap-southeast-2 and us-west-2.</label>
      <label for="optionC" class="correct-answer">C. Establish a VPC peering connection between the VPCs in ap-southeast-2 and us-west-2. Add the subnet ranges to the routing tables. &#x2713;&#x2713;</label>
      <label for="optionD">D. Create static routes in each VPC that point to the destination VPC with the virtual private gateway as the route target.</label>
    </form>

    <form class="question-form">
      <h1 class="question">A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain cloud.example.com for the resources stored within VPCs.</h1>
      <h1 class="question">The company has the following DNS resolution requirements:</h1>
      <ul>
        <li>On-premises systems should be able to resolve and connect to cloud.example.com.</li>
        <li>All VPCs should be able to resolve cloud.example.com.</li>
      </ul>
      <h1 class="question">There is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.</h1>
      <h1 class="question">Which architecture should the company use to meet these requirements with the HIGHEST performance?</h1>
      <label for="optionA" class="correct-answer">A. Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver. &#x2713;&#x2713;</label>
      <label for="optionB">B. Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the conditional forwarder.</label>
      <label for="optionC">C. Associate the private hosted zone to the shared services VPC. Create a Route 53 outbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the outbound resolver.</label>
      <label for="optionD">D. Associate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the shared services VPC to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated with different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of weather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the ability to fail over to a different AWS Region.</h1>
      <h1 class="question">Which solution will meet these requirements?</h1>
      <label for="optionA">A. Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDB tables to global tables.</label>
      <label for="optionB">B. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.</label>
      <label for="optionC" class="correct-answer">C. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables. &#x2713;&#x2713;</label>
      <label for="optionD">D. Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.</h1>
      <h1 class="question">The company recently acquired a new business unit and invited the new unit’s existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company’s policies.</h1>
      <h1 class="question">Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?</h1>
      <label for="optionA">A. Remove the organization’s root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company’s standard AWS Config rules and deploy them throughout the organization, including the new account.</label>
      <label for="optionB">B. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete.</label>
      <label for="optionC">C. Convert the organization’s root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization’s root that allows AWS Config actions for principals only in the new account.</label>
      <label for="optionD" class="correct-answer">D. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization’s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete. &#x2713;&#x2713;</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a stateful application. The application connects to a PostgreSQL database running on a separate server. The application’s user base is expected to grow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon EC2 Auto Scaling, and Elastic Load Balancing.</h1>
      <h1 class="question">Which solution will provide a consistent user experience that will allow the application and database tiers to scale?</h1>
      <label for="optionA">A. Enable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.</label>
      <label for="optionB">B. Enable Aurora Auto Scaling for Aurora writers. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled.</label>
      <label for="optionC" class="correct-answer">C. Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled. &#x2713;&#x2713;</label>
      <label for="optionD">D. Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and internet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are present in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices, which the company identified by the User-Agent headers.</h1>
      <h1 class="question">The company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company has already migrated the applications into a set of AWS Lambda functions.</h1>
      <h1 class="question">Which solution will meet these requirements?</h1>
      
      <label for="optionA">A. Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header.</label>
      
      <label for="optionB">B. Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Modify the default gateway responses to remove the problematic headers based on the value of the User-Agent header.</label>
      
      <label for="optionC">C. Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Create a response mapping template to remove the problematic headers based on the value of the User-Agent. Associate the response data mapping with the HTTP API.</label>
      
      <label for="optionD" class="correct-answer">D. Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a Lambda@Edge function that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header. &#x2713;&#x2713;</label>
    </form>
    <form class="question-form">
      <h1 class="question">A retail company needs to provide a series of data files to another company, which is its business partner. These files are saved in an Amazon S3 bucket under Account A, which belongs to the retail company. The business partner company wants one of its IAM users, User_DataProcessor, to access the files from its own AWS account (Account B).</h1>
      <h1 class="question">Which combination of steps must the companies take so that User_DataProcessor can access the S3 bucket successfully? (Choose two.)</h1>
    
      <label for="optionA">A. Turn on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A.</label>
    
      <label for="optionB">B. In Account A, set the S3 bucket policy to the following:
        <pre>
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Allow",
          "Principal": {
            "AWS": "arn:aws:iam::AccountB-ID:root"
          },
          "Action": "s3:GetObject",
          "Resource": "arn:aws:s3:::bucket-name/*"
        }
      ]
    }
        </pre>
      </label>
    
      <label for="optionC" class="correct-answer">C. In Account A, set the S3 bucket policy to the following:
        <pre>
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Allow",
          "Principal": {
            "AWS": "arn:aws:iam::AccountB-ID:user/User_DataProcessor"
          },
          "Action": "s3:GetObject",
          "Resource": "arn:aws:s3:::bucket-name/*"
        }
      ]
    }
        </pre>
        &#x2713;&#x2713;
      </label>
    
      <label for="optionD">D. In Account B, set the permissions of User_DataProcessor to the following:
        <pre>
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Allow",
          "Action": "s3:GetObject",
          "Resource": "arn:aws:s3:::bucket-name/*"
        }
      ]
    }
        </pre>
      </label>
    
      <label for="optionE" class="correct-answer">E. In Account B, set the permissions of User_DataProcessor to the following:
        <pre>
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Allow",
          "Action": "s3:GetObject",
          "Resource": "arn:aws:s3:::AccountA-ID:bucket-name/*"
        }
      ]
    }
        </pre>
        &#x2713;&#x2713;
      </label>
    </form>
    <form class="question-form">
      <h1 class="question">A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices that run on containers. Separate versions of the application exist in two distinct environments: production and testing. Load for the application is variable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a serverless architecture that minimizes operational complexity.</h1>
      <h1 class="question">Which solution will meet these requirements MOST cost-effectively?</h1>
    
      <label for="optionA">A. Upload the container images to AWS Lambda as functions. Configure a concurrency limit for the associated Lambda functions to handle the expected peak load. Configure two separate Lambda integrations within Amazon API Gateway: one for production and one for testing.</label>
    
      <label for="optionB" class="correct-answer">B. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters. &#x2713;&#x2713;</label>
    
      <label for="optionC">C. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the EKS clusters.</label>
    
      <label for="optionD">D. Upload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production and testing. Configure two separate Application Load Balancers to direct traffic to the Elastic Beanstalk deployments.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the maximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application’s data. The DB instance has a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.</h1>
      <h1 class="question">The company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region. The company does not have a large enough budget for an active-active strategy.</h1>
      <h1 class="question">What should a solutions architect recommend to meet these requirements?</h1>
    
      <label for="optionA">A. Reconfigure the application’s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.</label>
    
      <label for="optionB" class="correct-answer">B. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application’s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs. &#x2713;&#x2713;</label>
    
      <label for="optionC">C. Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region. Reconfigure the application’s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Remove the read replica. Replace the read replica with a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3.</label>
    
      <label for="optionD">D. Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node cluster for an in-memory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application to function, each piece of the infrastructure must be healthy and must be in an active state.</h1>
      <h1 class="question">A solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least possible downtime.</h1>
      <h1 class="question">Which combination of steps will meet these requirements? (Choose three.)</h1>
      <label for="optionA" class="correct-answer">A. Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances. &#x2713;&#x2713;</label>
      <label for="optionB">B. Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are configured in unlimited mode.</label>
      <label for="optionC">C. Modify the DB instance to create a read replica in the same Availability Zone. Promote the read replica to be the primary DB instance in failure scenarios.</label>
      <label for="optionD" class="correct-answer">D. Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones. &#x2713;&#x2713;</label>
      <label for="optionE">E. Create a replication group for the ElastiCache for Redis cluster. Configure the cluster to use an Auto Scaling group that has a minimum capacity of two instances.</label>
      <label for="optionF" class="correct-answer">F. Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster. &#x2713;&#x2713;</label>
    </form>
    <form class="question-form">
      <h1 class="question">A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.</h1>
      <h1 class="question">After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.</h1>
      <h1 class="question">While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.</h1>
      <h1 class="question">Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)</h1>
    
      <label for="optionA" class="correct-answer">A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3. &#x2713;&#x2713;</label>
    
      <label for="optionB">B. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.</label>
    
      <label for="optionC">C. Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.</label>
    
      <label for="optionD">D. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.</label>
    
      <label for="optionE" class="correct-answer">E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page. &#x2713;&#x2713;</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company has a mobile application that makes HTTP API calls to an Application Load Balancer (ALB). The ALB routes requests to an AWS Lambda function. Many different versions of the application are in use at any given time, including versions that are in testing by a subset of users. The version of the application is defined in the user-agent header that is sent with all requests to the API.</h1>
      <h1 class="question">After a series of recent changes to the API, the company has observed issues with the application. The company needs to gather a metric for each API operation by response code for each version of the application that is in use. A DevOps engineer has modified the Lambda function to extract the API operation name, version information from the user-agent header and response code.</h1>
      <h1 class="question">Which additional set of actions should the DevOps engineer take to gather the required metrics?</h1>
      <label for="optionA" class="correct-answer">A. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric. &#x2713;&#x2713;</label>
      <label for="optionB">B. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs Insights query to populate CloudWatch metrics from the log lines. Specify response code and application version as dimensions for the metric.</label>
      <label for="optionC">C. Configure the ALB access logs to write to an Amazon CloudWatch Logs log group. Modify the Lambda function to respond to the ALB with the API operation name, response code, and version number as response metadata. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.</label>
      <label for="optionD">D. Configure AWS X-Ray integration on the Lambda function. Modify the Lambda function to create an X-Ray subsegment with the API operation name, response code, and version number. Configure X-Ray insights to extract an aggregated metric for each API operation name and to publish the metric to Amazon CloudWatch. Specify response code and application version as dimensions for the metric.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company provides an application to customers. The application has an Amazon API Gateway REST API that invokes an AWS Lambda function. On initialization, the Lambda function loads a large amount of data from an Amazon DynamoDB table. The data load process results in long cold-start times of 8-10 seconds. The DynamoDB table has DynamoDB Accelerator (DAX) configured.</h1>
      <h1 class="question">Customers report that the application intermittently takes a long time to respond to requests. The application receives thousands of requests throughout the day. In the middle of the day, the application experiences 10 times more requests than at any other time of the day. Near the end of the day, the application's request volume decreases to 10% of its normal total.</h1>
      <h1 class="question">A DevOps engineer needs to reduce the latency of the Lambda function at all times of the day.</h1>
      <h1 class="question">Which solution will meet these requirements?</h1>
      <label for="optionA">A. Configure provisioned concurrency on the Lambda function with a concurrency value of 1. Delete the DAX cluster for the DynamoDB table.</label>
      <label for="optionB">B. Configure reserved concurrency on the Lambda function with a concurrency value of 0.</label>
      <label for="optionC" class="correct-answer">C. Configure provisioned concurrency on the Lambda function. Configure AWS Application Auto Scaling on the Lambda function with provisioned concurrency values set to a minimum of 1 and a maximum of 100. &#x2713;&#x2713;</label>
      <label for="optionD">D. Configure reserved concurrency on the Lambda function. Configure AWS Application Auto Scaling on the API Gateway API with a reserved concurrency maximum value of 100.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company is adopting AWS CodeDeploy to automate its application deployments for a Java-Apache Tomcat application with an Apache Webserver. The development team started with a proof of concept, created a deployment group for a developer environment, and performed functional tests within the application. After completion, the team will create additional deployment groups for staging and production.</h1>
      <h1 class="question">The current log level is configured within the Apache settings, but the team wants to change this configuration dynamically when the deployment occurs, so that they can set different log level configurations depending on the deployment group without having a different application revision for each group.</h1>
      <h1 class="question">How can these requirements be met with the LEAST management overhead and without requiring different script versions for each deployment group?</h1>
    
      <label for="optionA">A. Tag the Amazon EC2 instances depending on the deployment group. Then place a script into the application revision that calls the metadata service and the EC2 API to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference the script as part of the AfterInstall lifecycle hook in the appspec.yml file.</label>
      <label for="optionB" class="correct-answer">B. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_NAME to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the BeforeInstall lifecycle hook in the appspec.yml file. &#x2713;&#x2713;</label>
      <label for="optionC">C. Create a CodeDeploy custom environment variable for each environment. Then place a script into the application revision that checks this environment variable to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the ValidateService lifecycle hook in the appspec.yml file.</label>
      <label for="optionD">D. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ID to identify which deployment group the instance is part of to configure the log level settings. Reference this script as part of the Install lifecycle hook in the appspec.yml file.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company requires its developers to tag all Amazon Elastic Block Store (Amazon EBS) volumes in an account to indicate a desired backup frequency. This requirement includes EBS volumes that do not require backups. The company uses custom tags named Backup_Frequency that have values of none, daily, or weekly that correspond to the desired backup frequency. An audit finds that developers are occasionally not tagging the EBS volumes.</h1>
      <h1 class="question">A DevOps engineer needs to ensure that all EBS volumes always have the Backup_Frequency tag so that the company can perform backups at least weekly unless a different value is specified.</h1>
      <h1 class="question">Which solution will meet these requirements?</h1>
    
      <label for="optionA">A. Set up AWS Config in the account. Create a custom rule that returns a compliance failure for all Amazon EC2 resources that do not have a Backup_Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.</label>
    
      <label for="optionB" class="correct-answer">B. Set up AWS Config in the account. Use a managed rule that returns a compliance failure for EC2::Volume resources that do not have a Backup_Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. &#x2713;&#x2713;</label>
    
      <label for="optionC">C. Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule.</label>
    
      <label for="optionD">D. Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events or EBS ModifyVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company is using an Amazon Aurora cluster as the data store for its application. The Aurora cluster is configured with a single DB instance. The application performs read and write operations on the database by using the cluster's instance endpoint.</h1>
      <h1 class="question">The company has scheduled an update to be applied to the cluster during an upcoming maintenance window. The cluster must remain available with the least possible interruption during the maintenance window.</h1>
      <h1 class="question">What should a DevOps engineer do to meet these requirements?</h1>
      <label for="optionA" class="correct-answer">A. Add a reader instance to the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster's reader endpoint for reads. &#x2713;&#x2713;</label>
      <label for="optionB">B. Add a reader instance to the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations.</label>
      <label for="optionC">C. Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster’s reader endpoint for reads.</label>
      <label for="optionD">D. Turn on the Multi-AZ option on the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company must encrypt all AMIs that the company shares across accounts. A DevOps engineer has access to a source account where an unencrypted custom AMI has been built. The DevOps engineer also has access to a target account where an Amazon EC2 Auto Scaling group will launch EC2 instances from the AMI. The DevOps engineer must share the AMI with the target account.</h1>
      <h1 class="question">The company has created an AWS Key Management Service (AWS KMS) key in the source account.</h1>
      <h1 class="question">Which additional steps should the DevOps engineer perform to meet the requirements? (Choose three.)</h1>
    
      <label for="optionA" class="correct-answer">A. In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the KMS key in the copy action. &#x2713;&#x2713;</label>
    
      <label for="optionB">B. In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the default Amazon Elastic Block Store (Amazon EBS) encryption key in the copy action.</label>
    
      <label for="optionC">C. In the source account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role in the target account.</label>
    
      <label for="optionD" class="correct-answer">D. In the source account, modify the key policy to give the target account permissions to create a grant. In the target account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role. &#x2713;&#x2713;</label>
    
      <label for="optionE">E. In the source account, share the unencrypted AMI with the target account.</label>
    
      <label for="optionF" class="correct-answer">F. In the source account, share the encrypted AMI with the target account. &#x2713;&#x2713;</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company uses AWS CodePipeline pipelines to automate releases of its application. A typical pipeline consists of three stages: build, test, and deployment. The company has been using a separate AWS CodeBuild project to run scripts for each stage. However, the company now wants to use AWS CodeDeploy to handle the deployment stage of the pipelines.</h1>
      <h1 class="question">The company has packaged the application as an RPM package and must deploy the application to a fleet of Amazon EC2 instances. The EC2 instances are in an EC2 Auto Scaling group and are launched from a common AMI.</h1>
      <h1 class="question">Which combination of steps should a DevOps engineer perform to meet these requirements? (Choose two.)</h1>
    
      <label for="optionA" class="correct-answer">A. Create a new version of the common AMI with the CodeDeploy agent installed. Update the IAM role of the EC2 instances to allow access to CodeDeploy. &#x2713;</label>
      <label for="optionB">B. Create a new version of the common AMI with the CodeDeploy agent installed. Create an AppSpec file that contains application deployment scripts and grants access to CodeDeploy.</label>
      <label for="optionC">C. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI. Configure CodeDeploy to deploy the newly created AMI.</label>
      <label for="optionD" class="correct-answer">D. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application. &#x2713;</label>
      <label for="optionE">E. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the EC2 instances that are launched from the common AMI as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company’s security team requires that all external Application Load Balancers (ALBs) and Amazon API Gateway APIs are associated with AWS WAF web ACLs. The company has hundreds of AWS accounts, all of which are included in a single organization in AWS Organizations. The company has configured AWS Config for the organization. During an audit, the company finds some externally facing ALBs that are not associated with AWS WAF web ACLs.</h1>
      <h1 class="question">Which combination of steps should a DevOps engineer take to prevent future violations? (Choose two.)</h1>
    
      <label for="optionA" class="correct-answer">A. Delegate AWS Firewall Manager to a security account. &#x2713;</label>
      <label for="optionB">B. Delegate Amazon GuardDuty to a security account.</label>
      <label for="optionC" class="correct-answer">C. Create an AWS Firewall Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs. &#x2713;</label>
      <label for="optionD">D. Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.</label>
      <label for="optionE">E. Configure an AWS Config managed rule to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company uses AWS Key Management Service (AWS KMS) keys and manual key rotation to meet regulatory compliance requirements. The security team wants to be notified when any keys have not been rotated after 90 days.</h1>
      <h1 class="question">Which solution will accomplish this?</h1>
    
      <label for="optionA">A. Configure AWS KMS to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.</label>
      <label for="optionB">B. Configure an Amazon EventBridge event to launch an AWS Lambda function to call the AWS Trusted Advisor API and publish to an Amazon Simple Notification Service (Amazon SNS) topic.</label>
      <label for="optionC" class="correct-answer">C. Develop an AWS Config custom rule that publishes to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old. &#x2713;</label>
      <label for="optionD">D. Configure AWS Security Hub to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team does not allow unauthenticated requests to S3 buckets for this project.</h1>
      <h1 class="question">How can this issue be corrected in the MOST secure manner?</h1>
    
      <label for="optionA">A. Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.</label>
      <label for="optionB">B. Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.</label>
      <label for="optionC" class="correct-answer">C. Remove unauthenticated access from the S3 bucket with a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access. Use the AWS CLI to download the database population script. &#x2713;</label>
      <label for="optionD">D. Remove unauthenticated access from the S3 bucket with a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company wants to ensure that their EC2 instances are secure. They want to be notified if any new vulnerabilities are discovered on their instances, and they also want an audit trail of all login activities on the instances.</h1>
      <h1 class="question">Which solution will meet these requirements?</h1>
    
      <label for="optionA">A. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Amazon Kinesis Agent to capture system logs and deliver them to Amazon S3.</label>
      <label for="optionB">B. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture system logs and view login activity in the CloudTrail console.</label>
      <label for="optionC">C. Configure Amazon CloudWatch to detect vulnerabilities on the EC2 instances. Install the AWS Config daemon to capture system logs and view them in the AWS Config console.</label>
      <label for="optionD" class="correct-answer">D. Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs. &#x2713;</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company has many applications. Different teams in the company developed the applications by using multiple languages and frameworks. The applications run on premises and on different servers with different operating systems. Each team has its own release protocol and process. The company wants to reduce the complexity of the release and maintenance of these applications. The company is migrating its technology stacks, including these applications, to AWS. The company wants centralized control of source code, a consistent and automatic delivery pipeline, and as few maintenance tasks as possible on the underlying infrastructure.</h1>
      <h1 class="question">What should a DevOps engineer do to meet these requirements?</h1>
    
      <label for="optionA">A. Create one AWS CodeCommit repository for all applications. Put each application's code in different branch. Merge the branches, and use AWS CodeBuild to build the applications. Use AWS CodeDeploy to deploy the applications to one centralized application server.</label>
      <label for="optionB">B. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time. Use AWS CodeDeploy to deploy the applications to one centralized application server.</label>
      <label for="optionC">C. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time to create one AMI for each server. Use AWS CloudFormation StackSets to automatically provision and decommission Amazon EC2 fleets by using these AMIs.</label>
      <label for="optionD" class="correct-answer">D. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build one Docker image for each application in Amazon Elastic Container Registry (Amazon ECR). Use AWS CodeDeploy to deploy the applications to Amazon Elastic Container Service (Amazon ECS) on infrastructure that AWS Fargate manages. &#x2713;</label>
    </form>
    <form class="question-form">
      <h1 class="question">A DevOps engineer is developing an application for a company. The application needs to persist files to Amazon S3. The application needs to upload files with different security classifications that the company defines. These classifications include confidential, private, and public. Files that have a confidential classification must not be viewable by anyone other than the user who uploaded them. The application uses the IAM role of the user to call the S3 API operations.</h1>
      <h1 class="question">The DevOps engineer has modified the application to add a DataClassification tag with the value of confidential and an Owner tag with the uploading user's ID to each confidential object that is uploaded to Amazon S3. Which set of additional steps must the DevOps engineer take to meet the company's requirements?</h1>
    
      <label for="optionA">A. Modify the S3 bucket's ACL to grant bucket-owner-read access to the uploading user's IAM role. Create an IAM policy that grants s3:GetObject operations on the S3 bucket when aws:ResourceTag/DataClassification equals confidential, and s3:ExistingObjectTag/Owner equals ${aws:userid}. Attach the policy to the IAM roles for users who require access to the S3 bucket.</label>
      
      <label for="optionB" class="correct-answer">B. Modify the S3 bucket policy to allow the s3:GetObject action when aws:ResourceTag/DataClassification equals confidential, and s3:ExistingObjectTag/Owner equals ${aws:userid}. Create an IAM policy that grants s3:GetObject operations on the S3 bucket. Attach the policy to the IAM roles for users who require access to the S3 bucket. &#x2713;</label>
      
      <label for="optionC">C. Modify the S3 bucket policy to allow the s3:GetObject action when aws:ResourceTag/DataClassification equals confidential, and aws:RequestTag/Owner equals ${aws:userid}. Create an IAM policy that grants s3:GetObject operations on the S3 bucket. Attach the policy to the IAM roles for users who require access to the S3 bucket.</label>
      
      <label for="optionD">D. Modify the S3 bucket's ACL to grant authenticated-read access when aws:ResourceTag/DataClassification equals confidential, and s3:ExistingObjectTag/Owner equals ${aws:userid}. Create an IAM policy that grants s3:GetObject operations on the S3 bucket. Attach the policy to the IAM roles for users who require access to the S3 bucket.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A company has developed an AWS Lambda function that handles orders received through an API. The company is using AWS CodeDeploy to deploy the Lambda function as the final stage of a CI/CD pipeline.</h1>
      <h1 class="question">A DevOps Engineer has noticed there are intermittent failures of the ordering API for a few seconds after deployment. After some investigation, the DevOps Engineer believes the failures are due to database changes not having fully propagated before the Lambda function begins executing.</h1>
      <h1 class="question">How should the DevOps Engineer overcome this?</h1>
    
      <label for="optionA" class="correct-answer">A. Add a BeforeAllowTraffic hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function. &#x2713;</label>
    
      <label for="optionB">B. Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond.</label>
    
      <label for="optionC">C. Add a BeforeInstall hook to the AppSpec file that tests and waits for any necessary database changes before deploying the new version of the Lambda function.</label>
    
      <label for="optionD">D. Add a ValidateService hook to the AppSpec file that inspects incoming traffic and rejects the payload if dependent services, such as the database, are not yet ready.</label>
    </form>
    <form class="question-form">
      <h1 class="question">A software company wants to automate the build process for a project where the code is stored in GitHub. When the repository is updated, source code should be compiled, tested, and pushed to Amazon S3.</h1>
      <h1 class="question">Which combination of steps would address these requirements? (Choose three.)</h1>
    
      <label for="optionA" class="correct-answer">A. Add a buildspec.yml file to the source code with build instructions. &#x2713;</label>
      <label for="optionB" class="correct-answer">B. Configure a GitHub webhook to trigger a build every time a code change is pushed to the repository. &#x2713;</label>
      <label for="optionC" class="correct-answer">C. Create an AWS CodeBuild project with GitHub as the source repository. &#x2713;</label>
      <label for="optionD">D. Create an AWS CodeDeploy application with the Amazon EC2/On-Premises compute platform.</label>
      <label for="optionE">E. Create an AWS OpsWorks deployment with the install dependencies command.</label>
      <label for="optionF">F. Provision an Amazon EC2 instance to perform the build.</label>
    </form>
    <form class="question-form">
      <h1 class="question">An online retail company based in the United States plans to expand its operations to Europe and Asia in the next six months. Its product currently runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. All data is stored in an Amazon Aurora database instance.</h1>
      <h1 class="question">When the product is deployed in multiple regions, the company wants a single product catalog across all regions, but for compliance purposes, its customer information and purchases must be kept in each region.</h1>
      <h1 class="question">How should the company meet these requirements with the LEAST amount of application changes?</h1>
    
      <label for="optionA">A. Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer information and purchases.</label>
      <label for="optionB">B. Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer information and purchases.</label>
      <label for="optionC" class="correct-answer">C. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases. &#x2713;</label>
      <label for="optionD">D. Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer information and purchases.</label>
    </form>
    <form class="question-form">
      <h1 class="question">
        A company wants to ensure that their EC2 instances are secure. They want to be notified if any new vulnerabilities are discovered on their instances, and they also want an audit trail of all login activities on the instances.
      </h1>
      <h1 class="question">Which solution will meet these requirements?</h1>
    
      <label for="optionA">
        A. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Amazon Kinesis Agent to capture system logs and deliver them to Amazon S3.
      </label>
    
      <label for="optionB">
        B. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture system logs and view login activity in the CloudTrail console.
      </label>
    
      <label for="optionC">
        C. Configure Amazon CloudWatch to detect vulnerabilities on the EC2 instances. Install the AWS Config daemon to capture system logs and view them in the AWS Config console.
      </label>
    
      <label for="optionD" class="correct-answer">
        D. Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs. &#x2713;
      </label>
    </form>
    <form class="question-form">
      <h1 class="question">A DevOps Engineer needs to back up sensitive Amazon S3 objects that are stored within an S3 bucket with a private bucket policy using the S3 cross-region replication functionality. The objects need to be copied to a target bucket in a different AWS Region and account. Which actions should be performed to enable this replication? (Choose three.)</h1>
    
      <label for="optionA" class="correct-answer">A. Create a replication IAM role in the source account. &#x2713;</label><br>
      <label for="optionB">B. Create a replication IAM role in the target account.</label><br>
      <label for="optionC">C. Add statements to the source bucket policy allowing the replication IAM role to replicate objects.</label><br>
      <label for="optionD" class="correct-answer">D. Add statements to the target bucket policy allowing the replication IAM role to replicate objects. &#x2713;</label><br>
      <label for="optionE" class="correct-answer">E. Create a replication rule in the source bucket to enable the replication. &#x2713;</label><br>
      <label for="optionF">F. Create a replication rule in the target bucket to enable the replication.</label><br>
    </form>
    <form class="question-form">
      <h1 class="question">A company is using Amazon EC2 for various workloads. Company policy requires that instances be managed centrally to standardize configurations. These configurations include standard logging, metrics, security assessments, and weekly patching.</h1>
      <h1 class="question">How can the company meet these requirements? (Choose three.)</h1>
    
      <label for="optionA" class="correct-answer">A. Use AWS Config to ensure all EC2 instances are managed by AWS Systems Manager. &#x2713;</label><br>
      <label for="optionB">B. Use AWS Config to ensure all EC2 instances are managed by Amazon Inspector.</label><br>
      <label for="optionC" class="correct-answer">C. Use AWS Systems Manager to install and manage Amazon Inspector, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances. &#x2713;</label><br>
      <label for="optionD">D. Use Amazon Inspector to install and manage AWS Systems Manager, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances.</label><br>
      <label for="optionE">E. Use AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use the Amazon CloudWatch agent to schedule Amazon Inspector assessment runs.</label><br>
      <label for="optionF" class="correct-answer">F. Use AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use Amazon CloudWatch Events to schedule Amazon Inspector assessment runs. &#x2713;</label>
    </form>
    <form class="question-form">
      <h1 class="question">
        A business has an application that consists of five independent AWS Lambda functions. The DevOps Engineer has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon CloudWatch Events rule to ensure the pipeline execution starts as quickly as possible after a change is made to the application source code.
      </h1>
      <h1 class="question">
        After working with the pipeline for a few months, the DevOps Engineer has noticed the pipeline takes too long to complete. What should the DevOps Engineer implement to BEST improve the speed of the pipeline?
      </h1>
    
      <label for="optionA">A. Modify the CodeBuild projects within the pipeline to use a compute type with more available network throughput.</label><br>
      <label for="optionB">B. Create a custom CodeBuild execution environment that includes a symmetric multiprocessing configuration to run the builds in parallel.</label><br>
      <label for="optionC" class="correct-answer">C. Modify the CodePipeline configuration to execute actions for each Lambda function in parallel by specifying the same runOrder. &#x2713;</label><br>
      <label for="optionD">D. Modify each CodeBuild project to run within a VPC and use dedicated instances to increase throughput.</label><br>
    </form>

    <form class="question-form">
      <h1 class="question">
        A company is creating a software solution that executes a specific parallel-processing mechanism. The software can scale to tens of servers in some special scenarios. This solution uses a proprietary library that is license-based, requiring that each individual server have a single, dedicated license installed. The company has 200 licenses and is planning to run 200 server nodes concurrently at most.
      </h1>
      <h1 class="question">
        The company has requested the following features:
        <ul>
          <li>A mechanism to automate the use of the licenses at scale.</li>
          <li>Creation of a dashboard to use in the future to verify which licenses are available at any moment.</li>
        </ul>
      </h1>
      <h1 class="question">
        What is the MOST effective way to accomplish these requirements?
      </h1>
    
      <label for="optionA">A. Upload the licenses to a private Amazon S3 bucket. Create an AWS CloudFormation template with a Mappings section for the licenses. In the template, create an Auto Scaling group to launch the servers. In the user data script, acquire an available license from the Mappings section. Create an Auto Scaling lifecycle hook, then use it to update the mapping after the instance is terminated.</label><br>
    
      <label for="optionB">B. Upload the licenses to a private Amazon S3 bucket. Populate an Amazon SQS queue with the list of licenses stored in S3. Create an AWS CloudFormation template that uses an Auto Scaling group to launch the servers. In the user data script acquire an available license from SQS. Create an Auto Scaling lifecycle hook, then use it to put the license back in SQS after the instance is terminated.</label><br>
    
      <label for="optionC" class="correct-answer">C. Upload the licenses to an Amazon DynamoDB table. Create an AWS CloudFormation template that uses an Auto Scaling group to launch the servers. In the user data script, acquire an available license from the DynamoDB table. Create an Auto Scaling lifecycle hook, then use it to update the mapping after the instance is terminated . &#x2713;</label><br>
    
      <label for="optionD">D. Upload the licenses to an Amazon DynamoDB table. Create an AWS CLI script to launch the servers by using the parameter --count, with min:max instances to launch. In the user data script, acquire an available license from the DynamoDB table. Monitor each instance and, in case of failure, replace the instance, then manually update the DynamoDB table.</label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        A DevOps Engineer administers an application that manages video files for a video production company. The application runs on Amazon EC2 instances behind an ELB Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. Data is stored in an Amazon RDS PostgreSQL Multi-AZ DB instance, and the video files are stored in an Amazon S3 bucket. On a typical day, 50 GB of new video are added to the S3 bucket. The Engineer must implement a multi-region disaster recovery plan with the least data loss and the lowest recovery times. The current application infrastructure is already described using AWS CloudFormation.
      </h1>
      <h1 class="question">
        Which deployment option should the Engineer choose to meet the uptime and recovery objectives for the system?
      </h1>
    
      <label for="optionA">A. Launch the application from the CloudFormation template in the second region, which sets the capacity of the Auto Scaling group to 1.</label><br>
    
      <label for="optionB" class="correct-answer">
        B. Create an Amazon RDS read replica in the second region. In the second region, enable cross-region replication between the original S3 bucket and a new S3 bucket. To fail over, promote the read replica as master. Update the CloudFormation stack and increase the capacity of the Auto Scaling group. &#x2713;
      </label><br>
    
      <label for="optionC">C. Launch the application from the CloudFormation template in the second region, which sets the capacity of the Auto Scaling group to 1. Create a scheduled task to take daily Amazon RDS cross-region snapshots to the second region. In the second region, enable cross-region replication between the original S3 bucket and Amazon Glacier. In a disaster, launch a new application stack in the second region and restore the database from the most recent snapshot.</label><br>
    
      <label for="optionD">D. Launch the application from the CloudFormation template in the second region, which sets the capacity of the Auto Scaling group to 1. Use Amazon CloudWatch Events to schedule a nightly task to take a snapshot of the database, copy the snapshot to the second region, and replace the DB instance in the second region from the snapshot. In the second region, enable cross-region replication between the original S3 bucket and a new S3 bucket. To fail over, increase the capacity of the Auto Scaling group.</label><br>
    
      <label for="optionE">E. Use Amazon CloudWatch Events to schedule a nightly task to take a snapshot of the database and copy the snapshot to the second region. Create an AWS Lambda function that copies each object to a new S3 bucket in the second region in response to S3 event notifications. In the second region, launch the application from the CloudFormation template and restore the database from the most recent snapshot.</label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        A company has multiple AWS accounts in an organization in AWS Organizations that has all features enabled. The company’s DevOps administrator needs to improve security across all the company's AWS accounts. The administrator needs to identify the top users and roles in use across all accounts.
      </h1>
      <h1 class="question">
        Which solution will meet these requirements with the MOST operational efficiency?
      </h1>
    
      <label for="optionA" class="correct-answer">
        A. Create a new organization trail in AWS CloudTrail. Configure the trail to send log events to Amazon CloudWatch Logs. Create a CloudWatch Contributor Insights rule for the userIdentity.arn log field. View the results in CloudWatch Contributor Insights. &#x2713;
      </label><br>
    
      <label for="optionB">
        B. Create an unused access analysis for the organization by using AWS Identity and Access Management Access Analyzer. Review the analyzer results and determine if each finding has the intended level of permissions required for the workload.
      </label><br>
    
      <label for="optionC">
        C. Create a new organization trail in AWS CloudTrail. Create a table in Amazon Athena that uses partition projection. Load the Athena table with CloudTrail data. Query the Athena table to find the top users and roles.
      </label><br>
    
      <label for="optionD">
        D. Generate a Service access report for each account by using Organizations. From the results, pull the last accessed date and last accessed by account fields to find the top users and roles.
      </label><br>
    </form>

    <form class="question-form">
      <h1 class="question">
        A company manages 500 AWS accounts that are in an organization in AWS Organizations. The company discovers many unattached Amazon Elastic Block Store (Amazon EBS) volumes in all the accounts. The company wants to automatically tag the unattached EBS volumes for investigation.
      </h1>
      <h1 class="question">
        A DevOps engineer needs to deploy an AWS Lambda function to all the AWS accounts. The Lambda function must run every 30 minutes to tag all the EBS volumes that have been unattached for a period of 7 days or more.
      </h1>
      <h1 class="question">
        Which solution will meet these requirements in the MOST operationally efficient manner?
      </h1>
    
      <label for="optionA" class="correct-answer">
        A. Configure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization.. &#x2713;
      </label><br>
    
      <label for="optionB">
        B. Create a cross-account IAM role in the organization's member accounts. Attach the AWSLambda_FullAccess policy and the AWSCloudFormationFullAccess policy to the role. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Create a custom script in the organization’s management account that assumes the role and deploys the CloudFormation template to the member accounts.
      </label><br>
    
      <label for="optionC">
        C. AM role in the organization's member accounts. Attach the AWSLambda_FullAccess policy and the AWSCloudFormationFullAccess policy to the role. Create an AWS CloudFormation template that contains the Lambda function a 
      </label><br>
    
      <label for="optionD">
        D. Create a cross-account IAM role in the organization's member accounts. Attach the AmazonS3FullAccess policy and the AWSCodeDeployDeployerAccess policy to the role. Use AWS CodeDeploy to assume the role to deploy the Lambda function from the organization's management account. Configure an Amazon EventBridge scheduled rule in the member accounts to invoke the Lambda function every 30 minutes.
      </label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        A DevOps engineer is planning to deploy a Ruby-based application to production. The application needs to interact with an Amazon RDS for MySQL database and should have automatic scaling and high availability. The stored data in the database is critical and should persist regardless of the state of the application stack.
      </h1>
      <h1 class="question">
        The DevOps engineer needs to set up an automated deployment strategy for the application with automatic rollbacks. The solution also must alert the application team when a deployment fails.
      </h1>
      <h1 class="question">
        Which combination of steps will meet these requirements? (Choose three.)
      </h1>
    
      <label for="optionA">
        A. Deploy the application on AWS Elastic Beanstalk. Deploy an Amazon RDS for MySQL DB instance as part of the Elastic Beanstalk configuration.
      </label><br>
    
      <label for="optionB" class="correct-answer">
        B. Deploy the application on AWS Elastic Beanstalk. Deploy a separate Amazon RDS for MySQL DB instance outside of Elastic Beanstalk. &#x2713;
      </label><br>
    
      <label for="optionC" class="correct-answer">
        C. Configure a notification email address that alerts the application team in the AWS Elastic Beanstalk configuration. &#x2713;
      </label><br>
    
      <label for="optionD">
        D. Configure an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor AWS Health events. Use an Amazon Simple Notification Service (Amazon SNS) topic as a target to alert the application team.
      </label><br>
    
      <label for="optionE" class="correct-answer">
        E. Use the immutable deployment method to deploy new application versions. &#x2713;
      </label><br>
    
      <label for="optionF">
        F. Use the rolling deployment method to deploy new application versions.
      </label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        An ecommerce company is looking for ways to deploy an application on AWS that satisfies the following requirements:
      </h1>
      <ul>
        <li>Has a simple and automated application deployment process.</li>
        <li>Has minimal deployment costs while ensuring that at least half of the instances are available to receive end-user requests.</li>
        <li>If the application fails, an automated healing mechanism will replace the affected instances.</li>
      </ul>
      <h1 class="question">
        Which deployment strategy will meet these requirements?
      </h1>
    
      <label for="optionA">
        A. Create an AWS Elastic Beanstalk environment and configure it to use Auto Scaling and an Elastic Load Balancer. Use rolling deployments with a batch size of 50%.
      </label><br>
    
      <label for="optionB">
        B. Create an AWS OpsWorks stack. Configure the application layer to use rolling deployments as a deployment strategy. Add an Elastic Load Balancing layer. Enable auto healing on the application layer.
      </label><br>
    
      <label for="optionC" class="correct-answer">
        C. Use AWS CodeDeploy with Auto Scaling and an Elastic Load Balancer. Use the CodeDeployDefault.HalfAtATime deployment strategy. Enable an Elastic Load Balancing health check to report the status of the application, and set the Auto Scaling health check to ELB. &#x2713;
      </label><br>
    
      <label for="optionD">
        D. Use AWS CodeDeploy with Auto Scaling and an Elastic Load Balancer. Use a blue/green deployment strategy. Enable an Elastic Load Balancing health check to report the status of the application, and set the Auto Scaling health check to ELB.
      </label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        A company wants to ensure that their EC2 instances are secure. They want to be notified if any new vulnerabilities are discovered on their instances, and they also want an audit trail of all login activities on the instances.
      </h1>
      <h1 class="question">
        Which solution will meet these requirements?
      </h1>
    
      <label for="optionA">
        A. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Amazon Kinesis Agent to capture system logs and deliver them to Amazon S3.
      </label><br>
    
      <label for="optionB">
        B. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture system logs and view login activity in the CloudTrail console.
      </label><br>
    
      <label for="optionC">
        C. Configure Amazon CloudWatch to detect vulnerabilities on the EC2 instances. Install the AWS Config daemon to capture system logs and view them in the AWS Config console.
      </label><br>
    
      <label for="optionD" class="correct-answer">
        D. Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs. &#x2713;
      </label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        An online retail company based in the United States plans to expand its operations to Europe and Asia in the next six months. Its product currently runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. All data is stored in an Amazon Aurora database instance.
      </h1>
      <h1 class="question">
        When the product is deployed in multiple regions, the company wants a single product catalog across all regions, but for compliance purposes, its customer information and purchases must be kept in each region.
      </h1>
      <h1 class="question">
        How should the company meet these requirements with the LEAST amount of application changes?
      </h1>
    
      <label for="optionA">
        A. Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer information and purchases.
      </label><br>
    
      <label for="optionB" class="correct-answer">
        B. Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer information and purchases. &#x2713;
      </label><br>
    
      <label for="optionC">
        C. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases.
      </label><br>
    
      <label for="optionD">
        D. Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer information and purchases.
      </label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        A company has a mobile application that makes HTTP API calls to an Application Load Balancer (ALB). The ALB routes requests to an AWS Lambda function. Many different versions of the application are in use at any given time, including versions that are in testing by a subset of users. The version of the application is defined in the user-agent header that is sent with all requests to the API.
      </h1>
      <h1 class="question">
        After a series of recent changes to the API, the company has observed issues with the application. The company needs to gather a metric for each API operation by response code for each version of the application that is in use. A DevOps engineer has modified the Lambda function to extract the API operation name, version information from the user-agent header and response code.
      </h1>
      <h1 class="question">
        Which additional set of actions should the DevOps engineer take to gather the required metrics?
      </h1>
    
      <label for="optionA" class="correct-answer">
        A. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric. &#x2713;
      </label><br>
    
      <label for="optionB">
        B. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs Insights query to populate CloudWatch metrics from the log lines. Specify response code and application version as dimensions for the metric.
      </label><br>
    
      <label for="optionC">
        C. Configure the ALB access logs to write to an Amazon CloudWatch Logs log group. Modify the Lambda function to respond to the ALB with the API operation name, response code, and version number as response metadata. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.
      </label><br>
    
      <label for="optionD">
        D. Configure AWS X-Ray integration on the Lambda function. Modify the Lambda function to create an X-Ray subsegment with the API operation name, response code, and version number. Configure X-Ray insights to extract an aggregated metric for each API operation name and to publish the metric to Amazon CloudWatch. Specify response code and application version as dimensions for the metric.
      </label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        A company has a mobile application that makes HTTP API calls to an Application Load Balancer (ALB). The ALB routes requests to an AWS Lambda function. Many different versions of the application are in use at any given time, including versions that are in testing by a subset of users. The version of the application is defined in the user-agent header that is sent with all requests to the API.
      </h1>
      <h1 class="question">
        After a series of recent changes to the API, the company has observed issues with the application. The company needs to gather a metric for each API operation by response code for each version of the application that is in use. A DevOps engineer has modified the Lambda function to extract the API operation name, version information from the user-agent header and response code.
      </h1>
      <h1 class="question">
        Which additional set of actions should the DevOps engineer take to gather the required metrics?
      </h1>
    
      <label for="optionA" class="correct-answer">
        A. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric. &#x2713;
      </label><br>
    
      <label for="optionB">
        B. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs Insights query to populate CloudWatch metrics from the log lines. Specify response code and application version as dimensions for the metric.
      </label><br>
    
      <label for="optionC">
        C. Configure the ALB access logs to write to an Amazon CloudWatch Logs log group. Modify the Lambda function to respond to the ALB with the API operation name, response code, and version number as response metadata. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.
      </label><br>
    
      <label for="optionD">
        D. Configure AWS X-Ray integration on the Lambda function. Modify the Lambda function to create an X-Ray subsegment with the API operation name, response code, and version number. Configure X-Ray insights to extract an aggregated metric for each API operation name and to publish the metric to Amazon CloudWatch. Specify response code and application version as dimensions for the metric.
      </label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        A company uses AWS Organizations to manage its AWS accounts. The organization root has a child OU named Department. The Department OU has a child OU named Engineering. The default FullAWSAccess policy is attached to the root, the Department OU, and the Engineering OU.
      </h1>
      <h1 class="question">
        The company has many AWS accounts in the Engineering OU. Each account has an administrative IAM role with the AdministratorAccess IAM policy attached. The default FullAWSAccess policy is also attached to each account.
      </h1>
      <h1 class="question">
        A DevOps engineer plans to remove the FullAWSAccess policy from the Department OU. The DevOps engineer will replace the policy with a policy that contains an Allow statement for all Amazon EC2 API operations.
      </h1>
      <h1 class="question">
        What will happen to the permissions of the administrative IAM roles as a result of this change?
      </h1>
    
      <label for="optionA">
        A. All API actions on all resources will be allowed.
      </label><br>
    
      <label for="optionB" class="correct-answer">
        B. All API actions on EC2 resources will be allowed. All other API actions will be denied. &#x2713;
      </label><br>
    
      <label for="optionC">
        C. All API actions on all resources will be denied.
      </label><br>
    
      <label for="optionD">
        D. All API actions on EC2 resources will be denied. All other API actions will be allowed.
      </label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        A company wants to deploy a workload on several hundred Amazon EC2 instances. The company will provision the EC2 instances in an Auto Scaling group by using a launch template.
      </h1>
      <h1 class="question">
        The workload will pull files from an Amazon S3 bucket, process the data, and put the results into a different S3 bucket. The EC2 instances must have least-privilege permissions and must use temporary security credentials.
      </h1>
      <h1 class="question">
        Which combination of steps will meet these requirements? (Select TWO.)
      </h1>
    
      <label for="optionA" class="correct-answer">
        A. Create an IAM role that has the appropriate permissions for S3 buckets. Add the IAM role to an instance profile. &#x2713;
      </label><br>
    
      <label for="optionB" class="correct-answer">
        B. Update the launch template to include the IAM instance profile. &#x2713;
      </label><br>
    
      <label for="optionC">
        C. Create an IAM user that has the appropriate permissions for Amazon S3. Generate a secret key and token.
      </label><br>
    
      <label for="optionD">
        D. Create a trust anchor and profile. Attach the IAM role to the profile.
      </label><br>
    
      <label for="optionE">
        E. Update the launch template. Modify the user data to use the new secret key and token.
      </label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        A company is using an Amazon Aurora cluster as the data store for its application. The Aurora cluster is configured with a single DB instance. The application performs read and write operations on the database by using the cluster's instance endpoint.
      </h1>
      <h1 class="question">
        The company has scheduled an update to be applied to the cluster during an upcoming maintenance window. The cluster must remain available with the least possible interruption during the maintenance window.
      </h1>
      <h1 class="question">
        What should a DevOps engineer do to meet these requirements?
      </h1>
    
      <label for="optionA" class="correct-answer">
        A. Add a reader instance to the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster's reader endpoint for reads. &#x2713;
      </label><br>
    
      <label for="optionB">
        B. Add a reader instance to the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations.
      </label><br>
    
      <label for="optionC">
        C. Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster's reader endpoint for reads.
      </label><br>
    
      <label for="optionD">
        D. Turn on the Multi-AZ option on the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations.
      </label><br>
    </form>

    
    <form class="question-form">
      <h1 class="question">
        A DevOps engineer has created an AWS CloudFormation template that deploys an application on Amazon EC2 instances. The EC2 instances run Amazon Linux. The application is deployed to the EC2 instances by using shell scripts that contain user data. The EC2 instances have an IAM instance profile that has an IAM role with the AmazonSSMManagedInstanceCore managed policy attached.
      </h1>
      <h1 class="question">
        The DevOps engineer has modified the user data in the CloudFormation template to install a new version of the application. The engineer has also applied the stack update. However, the application was not updated on the running EC2 instances. The engineer needs to ensure that the changes to the application are installed on the running EC2 instances.
      </h1>
      <h1 class="question">
        Which combination of steps will meet these requirements? (Choose two.)
      </h1>
    
      <label for="optionA" class="correct-answer">
        A. Refactor the user data command to use an AWS Systems Manager document (SSM document). Use Systems Manager State Manager to create an association between the SSM document and the EC2 instances. &#x2713;
      </label><br>
    
      <label for="optionB" class="correct-answer">
        B. Refactor the user data commands to use the cfn-init helper script. Update the user data to install and configure the cfn-hup and cfn-init helper scripts to monitor and apply the metadata changes. &#x2713;
      </label><br>
    
      <label for="optionC">
        C. Configure an EC2 launch template for the EC2 instances. Create a new EC2 Auto Scaling group. Associate the Auto Scaling group with the EC2 launch template. Use the AutoScalingScheduledAction update policy for the Auto Scaling group.
      </label><br>
    
      <label for="optionD">
        D. Refactor the user data commands to use an AWS Systems Manager document (SSM document). Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 instances.
      </label><br>
    
      <label for="optionE">
        E. commands to use an AWS Systems Manager document (SSM document). Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 i.
      </label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        A company uses several AWS CloudFormation stacks to handle the deployment of a suite of applications. The leader of the company's application development team notices that the stack deployments fail with permission errors when some team members try to deploy the stacks. However, other team members can deploy the stacks successfully.
      </h1>
      <h1 class="question">
        The team members access the account by assuming a role that has a specific set of permissions that are necessary for the job responsibilities of the team members. All team members have permissions to perform operations on the stacks.
      </h1>
      <h1 class="question">
        Which combination of steps will ensure consistent deployment of the stacks MOST securely? (Choose three.)
      </h1>
    
      <label for="optionA" class="correct-answer">
        A. Create a service role that has a composite principal that contains each service that needs the necessary permissions. Configure the role to allow the sts:AssumeRole action. &#x2713;
      </label><br>
    
      <label for="optionB" class="correct-answer">
        B. Create a service role that has cloudformation.amazonaws.com as the service principal. Configure the role to allow the sts:AssumeRole action. &#x2713;
      </label><br>
    
      <label for="optionC">
        C. For each required set of permissions, add a separate policy to the role to allow those permissions. Add the ARN of each CloudFormation stack in the resource field of each policy.
      </label><br>
    
      <label for="optionD">
        D. For each required set of permissions, add a separate policy to the role to allow those permissions. Add the ARN of each service that needs the permissions in the resource field of the corresponding policy.
      </label><br>
    
      <label for="optionE" class="correct-answer">
        E. Update each stack to use the service role. &#x2713;
      </label><br>
    
      <label for="optionF" class="correct-answer">
        F. Add a policy to each member role to allow the iam:PassRole action. Set the policy's resource field to the ARN of the service role. &#x2713;
      </label><br>
    </form>
    <form class="question-form">
      <h1 class="question">
        A company wants to migrate its content sharing web application hosted on Amazon EC2 to a serverless architecture. The company currently deploys changes to its application by creating a new Auto Scaling group of EC2 instances and a new Elastic Load Balancer, and then shifting the traffic away using an Amazon Route 53 weighted routing policy.
      </h1>
      <h1 class="question">
        For its new serverless application, the company is planning to use Amazon API Gateway and AWS Lambda. The company will need to update its deployment processes to work with the new application. It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base.
      </h1>
      <h1 class="question">
        Which deployment strategy will meet these requirements?
      </h1>
    
      <label for="optionA">
        A. Use AWS CDK to deploy API Gateway and Lambda functions. When code needs to be changed, update the AWS CloudFormation stack and deploy the new version of the APIs and Lambda functions. Use a Route 53 failover routing policy for the canary release strategy.
      </label><br>
    
      <label for="optionB" class="correct-answer">
        B. Use AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete. &#x2713;
      </label><br>
    
      <label for="optionC">
        C. Use AWS Elastic Beanstalk to deploy API Gateway and Lambda functions. When code needs to be changed, deploy a new version of the API and Lambda functions. Shift traffic gradually using an Elastic Beanstalk blue/green deployment.
      </label><br>
    
      <label for="optionD">
        D. Use AWS OpsWorks to deploy API Gateway in the service layer and Lambda functions in a custom layer. When code needs to be changed, use OpsWorks to perform a blue/green deployment and shift traffic gradually.
      </label><br>
    </form>
    <form class="question-form">
  <h1 class="question">
    A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain <code>cloud.example.com</code> for the resources stored within VPCs.
  </h1>
  <h1 class="question">
    The company has the following DNS resolution requirements:
    <ul>
      <li>On-premises systems should be able to resolve and connect to <code>cloud.example.com</code>.</li>
      <li>All VPCs should be able to resolve <code>cloud.example.com</code>.</li>
      <li>There is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.</li>
    </ul>
  </h1>
  <h1 class="question">
    Which architecture should the company use to meet these requirements with the <strong>HIGHEST performance</strong>?
  </h1>

  <label for="optionA" class="correct-answer">
    A. Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for <code>cloud.example.com</code> that point to the inbound resolver. &#x2713;
  </label><br>

  <label for="optionB">
    B. Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for <code>cloud.example.com</code> that point to the conditional forwarder.
  </label><br>

  <label for="optionC">
    C. Associate the private hosted zone to the shared services VPC. Create a Route 53 outbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for <code>cloud.example.com</code> that point to the outbound resolver.
  </label><br>

  <label for="optionD">
    D. Associate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the shared services VPC to the transit gateway and create forwarding rules in the on-premises DNS server for <code>cloud.example.com</code> that point to the inbound resolver.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated with different AWS Lambda functions for each API operation.
  </h1>
  <h1 class="question">
    The company uses Amazon Route 53 for DNS and has created a resource record of <code>weather.example.com</code>. The company stores data for the API in Amazon DynamoDB tables.
  </h1>
  <h1 class="question">
    The company needs a solution that will give the API the ability to fail over to a different AWS Region.
  </h1>
  <h1 class="question">
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">
    A. Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDB tables to global tables.
  </label><br>

  <label for="optionB">
    B. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables. &#x2713;
  </label><br>

  <label for="optionD">
    D. Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A company uses AWS Organizations with a single OU named <strong>Production</strong> to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.
  </h1>
  <h1 class="question">
    The company recently acquired a new business unit and invited the new unit’s existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company’s policies.
  </h1>
  <h1 class="question">
    Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?
  </h1>

  <label for="optionA">
    A. Remove the organization’s root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company’s standard AWS Config rules and deploy them throughout the organization, including the new account.
  </label><br>

  <label for="optionB" class="correct-answer">
    B. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization’s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.;
  </label><br>

  <label for="optionC">
    C. Convert the organization’s root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization’s root that allows AWS Config actions for principals only in the new account.
  </label><br>

  <label for="optionD">
    D. Create oot SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization’s root that allows AWS Config actions for principals only in the new ac complete.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a stateful application. The application connects to a PostgreSQL database running on a separate server.
  </h1>
  <h1 class="question">
    The application’s user base is expected to grow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon EC2 Auto Scaling, and Elastic Load Balancing.
  </h1>
  <h1 class="question">
    Which solution will provide a consistent user experience that will allow the application and database tiers to scale?
  </h1>

  <label for="optionA">
    A. Enable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.
  </label><br>

  <label for="optionB">
    B. Enable Aurora Auto Scaling for Aurora writers. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled. &#x2713;
  </label><br>

  <label for="optionD">
    D. Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.
  </label><br>
</form>


<form class="question-form">
  <h1 class="question">
    A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and internet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are present in responses.
  </h1>
  <h1 class="question">
    The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices, which the company identified by the <code>User-Agent</code> headers.
  </h1>
  <h1 class="question">
    The company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company has already migrated the applications into a set of AWS Lambda functions.
  </h1>
  <h1 class="question">
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">
    A. ateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Create a response mapping template to remove the problr.
  </label><br>

  <label for="optionB">
    B. Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Modify the default gateway responses to remove the problematic headers based on the value of the <code>User-Agent</code> header.
  </label><br>

  <label for="optionC">
    C. Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Create a response mapping template to remove the problematic headers based on the value of the <code>User-Agent</code>. Associate the response data mapping with the HTTP API.
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a CloudFront function to remove the problematic headers based on the value of the <code>User-Agent</code> header.;
  </label><br>
</form>

<form class="question-form">
  <h1 class="question">
    A retail company needs to provide a series of data files to another company, which is its business partner. These files are saved in an Amazon S3 bucket under Account A, which belongs to the retail company.
  </h1>
  <h1 class="question">
    The business partner company wants one of its IAM users, <code>User_DataProcessor</code>, to access the files from its own AWS account (Account B).
  </h1>
  <h1 class="question">
    Which combination of steps must the companies take so that <code>User_DataProcessor</code> can access the S3 bucket successfully? <strong>(Choose two.)</strong>
  </h1>

  <label for="optionA">
    A. Turn on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A.
  </label><br>

  <label for="optionB" class="correct-answer">
    B. In Account A, set the S3 bucket policy to the following:<br>
    <pre>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::AccountB-ID:user/User_DataProcessor"
      },
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::bucket-name/*"
    }
  ]
}</pre> &#x2713;
  </label><br>

  <label for="optionC">
    C. In Account A, set the S3 bucket policy to the following:<br>
    <pre>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:*",
      "Resource": "*"
    }
  ]
}</pre>
  </label><br>

  <label for="optionD" class="correct-answer">
    D. In Account B, set the permissions of <code>User_DataProcessor</code> to the following:<br>
    <pre>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::bucket-name/*"
    }
  ]
}</pre> &#x2713;
  </label><br>

  <label for="optionE">
    E. In Account B, set the permissions of <code>User_DataProcessor</code> to the following:<br>
    <pre>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Action": "s3:*",
      "Resource": "*"
    }
  ]
}</pre>
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices that run on containers. Separate versions of the application exist in two distinct environments: <strong>production</strong> and <strong>testing</strong>.
  </h1>
  <h1 class="question">
    Load for the application is variable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a <strong>serverless architecture</strong> that <strong>minimizes operational complexity</strong>.
  </h1>
  <h1 class="question">
    Which solution will meet these requirements <strong>MOST cost-effectively</strong>?
  </h1>

  <label for="optionA" class="correct-answer">
    A.  Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters.
  </label><br>

  <label for="optionB">
    B. Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handle
  </label><br>

  <label for="optionC">
    C. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the EKS clusters.
  </label><br>

  <label for="optionD">
    D. Upload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production and testing. Configure two separate Application Load Balancers to direct traffic to the Elastic Beanstalk deployments.
  </label><br>
</form>
  <form class="question-form">
  <h1 class="question">
    A technology company has a suite of container-based web applications and serverless solutions that are hosted in AWS. The Solutions Architect must define a standard infrastructure that will be used across development teams and applications. There are application-specific resources too that change frequently, especially during the early stages of application development. Developers must be able to add supplemental resources to their applications, which are beyond what the architects predefined in the system environments and service templates. Which of the following should be implemented to satisfy this requirement?
  </h1>

  <label for="optionA">A. Use the Amazon Elastic Container Service (ECS) Anywhere service for deploying container applications and serverless solutions. Configure Prometheus metrics collection on the ECS cluster and use Amazon Managed Service for Prometheus for monitoring frequently-changing resources</label>
  <label for="optionB" class="correct-answer">B. Set up AWS Proton for deploying container applications and serverless solutions. Create components from the AWS Proton console and attach them to their respective service instance. ✓✓</label>
  <label for="optionC">C. Set up AWS Control Tower to automate container-based application deployments. Use AWS Config for application-specific resources that change frequently.</label>
  <label for="optionD">D. Use the Amazon EKS Anywhere service for deploying container applications and serverless solutions. Create a service instance for each application-specific resource.</label>
</form>
<form class="question-form">
      <h1 class="question">
        A company collects atmospheric data such as temperature, air pressure, and humidity from different countries. Each site location is equipped with various weather instruments and a high-speed Internet connection. The average collected data in each location is around 500 GB and will be analyzed by a weather forecasting application hosted in Northern Virginia. As the Solutions Architect, you need to aggregate all the data in the fastest way. Which of the following options can satisfy the given requirement?
      </h1>

      <label for="optionA">A. Use AWS Snowball Edge to transfer large amounts of data.</label>
      <label for="optionB">B. Upload the data to the closest S3 bucket. Set up a cross-region replication and copy the objects to the destination bucket.</label>
      <label for="optionC" class="correct-answer">C. Enable Transfer Acceleration in the destination bucket and upload the collected data using Multipart Upload. ✓✓</label>
      <label for="optionD">D. Set up a Site-to-Site VPN connection.</label>
    </form>
    <form class="question-form">
  <h1 class="question">
    A company's containerized application runs on an Amazon EC2 instance. The application needs to download security certificates before it can communicate with other business applications. The company wants a highly secure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in highly available storage after the data is encrypted.
    Which solution will meet these requirements with the LEAST operational overhead?
  </h1>

  <label for="optionA">A. Create an AWS Lambda function that uses the Python cryptography library to receive and perform encryption operations. Store the function in an Amazon S3 bucket.</label>
  <label for="optionB">B. Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block Store (Amazon EBS) volumes.</label>
  <label for="optionC">C. Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as needed. Control access to the data by using fine-grained IAM access.</label>
  <label for="optionD" class="correct-answer">D. Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon S3. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis.
    The company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days.
    What is the MOST operationally efficient solution that meets these requirements?
  </h1>

  <label for="optionA" class="correct-answer">A. Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days. ✓✓</label>
  <label for="optionB">B. Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.</label>
  <label for="optionC">C. Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon Elasticsearch Service (Amazon ES) cluster. Set up the Amazon ES cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.</label>
  <label for="optionD">D. Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue.</label>
</form>
<form class="question-form">
  <h1 class="question">
    Both historical records and frequently accessed data are stored on an on-premises storage system.
    The amount of current data is growing at an exponential rate. As the storage's capacity is nearing its limit, the company's Solutions Architect has decided to move the historical records to AWS to free up space for the active data.
    Which of the following architectures deliver the best solution in terms of cost and operational management?
  </h1>

  <label for="optionA">A. Use AWS Storage Gateway to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier Deep Archive to be the destination for the data.</label>
  <label for="optionB" class="correct-answer">B. Use AWS DataSync to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier Deep Archive to be the destination for the data. ✓✓</label>
  <label for="optionC">C. Use AWS Storage Gateway to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier to be the destination for the data. Modify the S3 lifecycle configuration to move the data from the Standard tier to Amazon S3 Glacier Deep Archive after 30 days.</label>
  <label for="optionD">D. Use AWS DataSync to move the historical records from on-premises to AWS. Choose Amazon S3 Standard to be the destination for the data. Modify the S3 lifecycle configuration to move the data from the Standard tier to Amazon S3 Glacier Deep Archive after 30 days.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company uses AWS Organizations to create dedicated AWS accounts for each business unit to manage each business unit's account independently upon request. The root email recipient missed a notification that was sent to the root user email address of one account. The company wants to ensure that all future notifications are not missed. Future notifications must be limited to account administrators.
    Which solution will meet these requirements?
  </h1>

  <label for="optionA" class="correct-answer">A. Configure all existing AWS accounts and all newly created accounts to use the same root user email address. Configure AWS account alternate contacts in the AWS Organizations console or programmatically. ✓✓</label>
  <label for="optionB">B. Configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.</label>
  <label for="optionC">C. Configure the company's email server to forward notification email messages that are sent to the AWS account root user email address to all users in the organization.</label>
  <label for="optionD">D. Configure all AWS account root user email messages to be sent to one administrator who is responsible for monitoring alerts and forwarding those alerts to the appropriate groups.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is building a containerized application on premises and decides to move the application to AWS. The application will have thousands of users soon after it is deployed. The company is unsure how to manage the deployment of containers at scale. The company needs to deploy the containerized application in a highly available architecture that minimizes operational overhead.
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">A. Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the containers. Use target tracking to scale automatically based on demand.</label>
  <label for="optionB">B. Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image. Launch EC2 Instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon CloudWatch alarm to scale out EC2 instances when the average CPU utilization threshold is breached.</label>
  <label for="optionC">C. Store container images in a repository that runs on an Amazon EC2 instance. Run the containers on EC2 instances that are spread across multiple Availability Zones. Monitor the average CPU utilization in Amazon CloudWatch. Launch new EC2 instances as needed.</label>
  <label for="optionD" class="correct-answer">D. Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A Solutions Architect for a global news company is configuring a fleet of EC2 instances in a subnet that currently is in a VPC with an Internet gateway attached. All of these EC2 instances can be accessed from the Internet. The architect launches another subnet and deploys an EC2 instance in it, however, the architect is not able to access the EC2 instance from the Internet.
    What could be the possible reasons for this issue? (Select TWO.)
  </h1>

  <label for="optionA">A. The Amazon EC2 instance does not have an attached Elastic Fabric Adapter (EFA).</label>
  <label for="optionB" class="correct-answer">B. The Amazon EC2 instance does not have a public IP address associated with it. ✓✓</label>
  <label for="optionC">C. The route table is not configured properly to send traffic from the EC2 instance to the Internet through the customer gateway (CGW).</label>
  <label for="optionD">D. The Amazon EC2 instance is not a member of the same Auto Scaling group.</label>
  <label for="optionE" class="correct-answer">E. The route table is not configured properly to send traffic from the EC2 instance to the Internet through the Internet gateway. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a large EC2 instance.
    The database's performance degrades quickly as application load increases. The application handles more read requests than write transactions. The company wants a solution that will automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability.
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">A. Use Amazon Redshift with a single node for leader and compute functionality.</label>
  <label for="optionB">B. Use Amazon ElastiCache for Memcached with EC2 Spot Instances.</label>
  <label for="optionC">C. Use Amazon RDS with a Single-AZ deployment. Configure Amazon RDS to add reader instances in a different Availability Zone.</label>
  <label for="optionD" class="correct-answer">D. Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it. The job is stateless in nature, can be started and stopped at any given time with no negative impact, and typically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design a scalable and cost-effective solution that meets the requirements of the job.
    What should the solutions architect recommend?
  </h1>

  <label for="optionA">A. Implement the processing on AWS Lambda</label>
  <label for="optionB" class="correct-answer">B. Implement EC2 Spot Instances ✓✓</label>
  <label for="optionC">C. Implement EC2 On-Demand Instances</label>
  <label for="optionD">D. Purchase EC2 Reserved Instances</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">A. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.</label>
  <label for="optionB">B. Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.</label>
  <label for="optionC" class="correct-answer">C. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups. ✓✓</label>
  <label for="optionD">D. Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has a High Performance Computing (HPC) cluster that is composed of EC2 Instances with Provisioned IOPS volume to process transaction-intensive, low-latency workloads. The Solutions Architect must maintain high IOPS while keeping the latency down by setting the optimal queue length for the volume. The size of each volume is 10 GiB.
    Which of the following is the MOST suitable configuration that the Architect should set up?
  </h1>

  <label for="optionA">A.  Set the IOPS to 800 then maintain a low queue length.</label>
  <label for="optionB">B. Set the IOPS to 400 then maintain a low queue length.</label>
  <label for="optionC">C. Set the IOPS to 600 then maintain a high queue length.</label>
  <label for="optionD" class="correct-answer">D. Set the IOPS to 500 then maintain a low queue length. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A large media company hosts a web application on AWS. The company wants to start caching confidential media files so that users around the world will have reliable access to the files. The content is stored in Amazon S3 buckets. The company must deliver the content quickly, regardless of where the requests originate geographically.
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">A. Use AWS DataSync to connect the S3 buckets to the web application.</label>
  <label for="optionB">B. Deploy AWS Global Accelerator to connect the S3 buckets to the web application.</label>
  <label for="optionC" class="correct-answer">C. Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers. ✓✓</label>
  <label for="optionD">D. Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A pharmaceutical company has resources hosted on both their on-premises network and in AWS cloud. They want all of their Software Architects to access resources on both environments using their on-premises credentials, which is stored in Active Directory.
    In this scenario, which of the following can be used to fulfill this requirement?
  </h1>

  <label for="optionA">A. Use Amazon VPC</label>
  <label for="optionB">B. Use IAM users</label>
  <label for="optionC" class="correct-answer">C. Set up SAML 2.0-Based Federation by using a Microsoft Active Directory Federation Service (AD FS). ✓✓</label>
  <label for="optionD">D. Set up SAML 2.0-Based Federation by using a Web Identity Federation.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is streamed, and a storage solution for the data.
    Which solution will meet these requirements with the LEAST operational overhead?
  </h1>

  <label for="optionA">A. Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination checking on the EC2 instance. Use AWS Glue to transform the data and to send the data to Amazon S3.</label>
  <label for="optionB">B. Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform the data. Use AWS Glue to send the data to Amazon S3.</label>
  <label for="optionC">C. Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.</label>
  <label for="optionD" class="correct-answer">D. Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A financial analytics application that collects, processes and analyzes stock data in real-time is using Kinesis Data Streams. The producers continually push data to Kinesis Data Streams while the consumers process the data in real time. In Amazon Kinesis, where can the consumers store their results? (Select TWO.)
  </h1>

  <label for="optionA">A. Amazon Athena</label>
  <label for="optionB">B. Glacier Select</label>
  <label for="optionC" class="correct-answer">C. Amazon Redshift ✓✓</label>
  <label for="optionD">D. AWS Glue</label>
  <label for="optionE" class="correct-answer">E. Amazon S3 ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    There is a technical requirement by a financial firm that does online credit card processing to have a secure application environment on AWS. They are trying to decide on whether to use KMS or CloudHSM. <br>
    Which of the following statements is right when it comes to CloudHSM and KMS?
  </h1>

  <label for="optionA">A. AWS CloudHSM should always be used for any payment transactions.</label>
  <label for="optionB" class="correct-answer">B. You should consider using AWS CloudHSM over AWS KMS if you require your keys stored in dedicated, third-party validated hardware security modules under your exclusive control. ✓✓</label>
  <label for="optionC">C. If you want a managed service for creating and controlling your encryption keys but don't want or need to operate your own HSM, consider using AWS CloudHSM.</label>
  <label for="optionD">D. No major difference. They both do the same thing.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has an event-driven application that invokes AWS Lambda functions up to 800 times each minute with varying runtimes. The Lambda functions access data that is stored in an Amazon Aurora MySQL DB cluster. The company is noticing connection timeouts as user activity increases. The database shows no signs of being overloaded. CPU, memory, and disk access metrics are all low. <br>
    Which solution will resolve this issue with the LEAST operational overhead?
  </h1>

  <label for="optionA">A. Adjust the size of the Aurora MySQL nodes to handle more connections. Configure retry logic in the Lambda functions for attempts to connect to the database.</label>
  <label for="optionB">B. Set up Amazon ElastiCache for Redis to cache commonly read items from the database. Configure the Lambda functions to connect to ElastiCache for reads.</label>
  <label for="optionC">C. Add an Aurora Replica as a reader node. Configure the Lambda functions to connect to the reader endpoint of the DB cluster rather than to the writer endpoint.</label>
  <label for="optionD" class="correct-answer">D. Use Amazon RDS Proxy to create a proxy. Set the DB cluster as the target database. Configure the Lambda functions to connect to the proxy rather than to the DB cluster. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has a set of Linux servers running on multiple On-Demand EC2 Instances. The Audit team wants to collect and process the application log files generated from these servers for their report. <br>
    Which of the following services is best to use in this case?
  </h1>

  <label for="optionA">A. A single On-Demand Amazon EC2 instance for both storing and processing the log files.</label>
  <label for="optionB">B. Amazon S3 Glacier Deep Archive for storing the application log files and AWS ParallelCluster for processing the log files.</label>
  <label for="optionC">C. Amazon S3 Glacier for storing the application log files and Spot EC2 Instances for processing them.</label>
  <label for="optionD" class="correct-answer">D. Amazon S3 for storing the application log files and Amazon Elastic MapReduce for processing the log files. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A popular social network is hosted in AWS and is using a DynamoDB table as its database. There is a requirement to implement a 'follow' feature where users can subscribe to certain updates made by a particular user and be notified via email. <br>
    Which of the following is the most suitable solution that you should implement to meet the requirement?
  </h1>

  <label for="optionA">A. Create a Lambda function that uses DynamoDB Streams Kinesis Adapter which will fetch data from the DynamoDB Streams endpoint. Set up an SNS Topic that will notify the subscribers via email when there is an update made by a particular user.</label>
  <label for="optionB">B. Using the Kinesis Client Library (KCL), write an application that leverages on DynamoDB Streams Kinesis Adapter that will fetch data from the DynamoDB Streams endpoint. When there are updates made by a particular user, notify the subscribers via email using SNS.</label>
  <label for="optionC">C. Set up a DAX cluster to access the source DynamoDB table. Create a new DynamoDB trigger and a Lambda function. For every update made in the user data, the trigger will send data to the Lambda function which will then notify the subscribers via email using SNS.</label>
  <label for="optionD" class="correct-answer">D. Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to SNS Topic that will notify the subscribers via email. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company launched a website that accepts high-quality photos and turns them into a downloadable video montage. The website offers a free and a premium account that guarantees faster processing. All requests by both free and premium members go through a single SQS queue and then processed by a group of EC2 instances that generate the videos. The company needs to ensure that the premium users who paid for the service have higher priority than the free members. <br>
    How should the company re-design its architecture to address this requirement?
  </h1>

  <label for="optionA">A. For the requests made by premium members, set a higher priority in the SQS queue so it will be processed first compared to the requests made by free members.</label>
  <label for="optionB">B. Use Amazon Kinesis to process the photos and generate the video montage in real-time.</label>
  <label for="optionC" class="correct-answer">C. Create an SQS queue for free members and another one for premium members. Configure your EC2 instances to consume messages from the premium queue first and if it is empty, poll from the free members' SQS queue. ✓✓</label>
  <label for="optionD">D. Use Amazon S3 to store and process the photos and then generate the video montage afterward.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has a UAT and production EC2 instances running on AWS. They want to ensure that employees who are responsible for the UAT instances don't have the access to work on the production instances to minimize security risks. <br>
    Which of the following would be the best way to achieve this?
  </h1>

  <label for="optionA" class="correct-answer">A. Define the tags on the UAT and production servers and add a condition to the IAM policy which allows access to specific tags. ✓✓</label>
  <label for="optionB">B. Provide permissions to the users via the AWS Resource Access Manager (RAM) service to only access EC2 instances that are used for production or development.</label>
  <label for="optionC">C. Launch the UAT and production EC2 instances in separate VPC's connected by VPC peering.</label>
  <label for="optionD">D. Launch the UAT and production instances in different Availability Zones and use Multi Factor Authentication.</label>
</form>
<form class="question-form">
  <h1 class="question">
    There was an incident in your production environment where the user data stored in the S3 bucket has been accidentally deleted by one of the Junior DevOps Engineers. The issue was escalated to your manager and after a few days, you were instructed to improve the security and protection of your AWS resources. <br>
    What combination of the following options will protect the S3 objects in your bucket from both accidental deletion and overwriting? (Select TWO.)
  </h1>

  <label for="optionA">A. Provide access to S3 data strictly through pre-signed URL only</label>
  <label for="optionB">B. Enable Amazon S3 Intelligent-Tiering</label>
  <label for="optionC" class="correct-answer">C. Enable Multi-Factor Authentication Delete ✓✓</label>
  <label for="optionD" class="correct-answer">D. Enable Versioning ✓✓</label>
  <label for="optionE">E. Disallow S3 Delete using an IAM bucket policy</label>
</form>
<form class="question-form">
  <h1 class="question">
    A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should be protected throughout the entire application stack, and access to the information should be restricted to certain applications. <br>
    Which action should the solutions architect take?
  </h1>

  <label for="optionA">A. Configure a CloudFront signed URL.</label>
  <label for="optionB">B. Configure a CloudFront signed cookie.</label>
  <label for="optionC" class="correct-answer">C. Configure a CloudFront field-level encryption profile ✓✓</label>
  <label for="optionD">D. Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company runs a stateless web application in production on a group of Amazon EC2 On-Demand Instances behind an Application Load Balancer. 
    The application experiences heavy usage during an 8-hour period each business day. Application usage is moderate and steady overnight. 
    Application usage is low during weekends. <br>
    The company wants to minimize its EC2 costs without affecting the availability of the application. <br>
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">A. Use Dedicated Instances for the baseline level of usage. Use On-Demand Instances for any additional capacity that the application needs.</label>
  <label for="optionB">B. Use On-Demand Instances for the baseline level of usage. Use Spot Instances for any additional capacity that the application needs.</label>
  <label for="optionC">C. Use Spot Instances for the entire workload.</label>
  <label for="optionD" class="correct-answer">D. Use Reserved Instances for the baseline level of usage. Use Spot Instances for any additional capacity that the application needs. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. 
    The website is experiencing increased demand from around the world. 
    The company must decrease latency for users who access the website. <br>
    Which solution meets these requirements MOST cost-effectively?
  </h1>

  <label for="optionA">A. Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation routing entries.</label>
  <label for="optionB">B. Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3 bucket. Edit the Route 53 entries to point to the IP addresses of the accelerators.</label>
  <label for="optionC">C. Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint.</label>
  <label for="optionD" class="correct-answer">D. Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to the CloudFront distribution. ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A Docker application, which is running on an Amazon ECS cluster behind a load balancer, 
    is heavily using DynamoDB. You are instructed to improve the database performance by 
    distributing the workload evenly and using the provisioned throughput efficiently. <br>
    Which of the following would you consider to implement for your DynamoDB table?
  </h1>

  <label for="optionA">A. Use partition keys with low-cardinality attributes, which have a few number of distinct values for each item.</label>
  <label for="optionB">B. Reduce the number of partition keys in the DynamoDB table.</label>
  <label for="optionC" class="correct-answer">C. Use partition keys with high-cardinality attributes, which have a large number of distinct values for each item. ✓✓</label>
  <label for="optionD">D. Avoid using a composite primary key, which is composed of a partition key and a sort key.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A loan processing application is hosted in a single On-Demand EC2 instance in your VPC. 
    To improve the scalability of your application, you have to use Auto Scaling to automatically 
    add new EC2 instances to handle a surge of incoming requests. <br>
    Which of the following items should be done in order to add an existing EC2 instance 
    to an Auto Scaling group? (Select TWO.)
  </h1>

  <label for="optionA">A. You must stop the instance first.</label>
  <label for="optionB" class="correct-answer">B. You have to ensure that the instance is launched in one of the Availability Zones defined in your Auto Scaling group. ✓✓</label>
  <label for="optionC">C. You have to ensure that the AMI used to launch the instance no longer exists.</label>
  <label for="optionD" class="correct-answer">D. You have to ensure that the AMI used to launch the instance still exists. ✓✓</label>
  <label for="optionE">E. You have to ensure that the instance is in a different Availability Zone as the Auto Scaling group.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A data analytics company keeps a massive volume of data that they store in their on-premises data center. 
    To scale their storage systems, they are looking for cloud-backed storage volumes that they can mount using 
    Internet Small Computer System Interface (iSCSI) devices from their on-premises application servers. <br><br>
    They have an on-site data analytics application that frequently accesses the latest data subsets locally while 
    the older data are rarely accessed. You are required to minimize the need to scale the on-premises storage 
    infrastructure while still providing their web application with low-latency access to the data. <br><br>
    Which type of AWS Storage Gateway service will you use to meet the above requirements?
  </h1>

  <label for="optionA" class="correct-answer">A. Volume Gateway in cached mode ✓✓</label>
  <label for="optionB">B. Volume Gateway in stored mode</label>
  <label for="optionC">C. Tape Gateway</label>
  <label for="optionD">D. File Gateway</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company developed a meal planning application that provides meal recommendations for the week as well as the food consumption of the users. 
    The application resides on an EC2 instance which requires access to various AWS services for its day-to-day operations. <br><br>
    Which of the following is the best way to allow the EC2 instance to access the S3 bucket and other AWS services?
  </h1>

  <label for="optionA">A. Store the API credentials in a bastion host.</label>
  <label for="optionB">B. Store the API credentials in the EC2 instance.</label>
  <label for="optionC" class="correct-answer">C. Create a role in IAM and assign it to the EC2 instance. ✓</label>
  <label for="optionD">D. Add the API Credentials in the Security Group and assign it to the EC2 instance.</label>
</form>
<form class="question-form">
  <h1 class="question">
    An advertising company is currently working on a proof of concept project that automatically provides SEO analytics for its clients. 
    Your company has a VPC in AWS that operates in a dual-stack mode in which IPv4 and IPv6 communication is allowed. 
    You deployed the application to an Auto Scaling group of EC2 instances with an Application Load Balancer in front that evenly distributes the incoming traffic. 
    You are ready to go live but you need to point your domain name (tutorialsdojo.com) to the Application Load Balancer. <br><br>
    In Route 53, which record types will you use to point the DNS name of the Application Load Balancer? (Select TWO.)
  </h1>

  <label for="optionA" class="correct-answer">A. Alias with a type "A" record set ✓✓</label>
  <label for="optionB">B. Non-Alias with a type "A" record set</label>
  <label for="optionC">C. Alias with a type of "MX" record set</label>
  <label for="optionD" class="correct-answer">D. Alias with a type "AAAA" record set ✓✓</label>
  <label for="optionE">E. Alias with a type "CNAME" record set</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company runs a shopping application that uses Amazon DynamoDB to store customer information. 
    In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour. <br><br>
    What should the solutions architect recommend to meet these requirements?
  </h1>

  <label for="optionA">A. Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table by using the EBS snapshot.</label>
  <label for="optionB" class="correct-answer">B. Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time. ✓</label>
  <label for="optionC">C. Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data from S3 Glacier to DynamoDB.</label>
  <label for="optionD">D. Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS Region.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company plans to use a durable storage service to store on-premises database backups to the AWS cloud. 
    To move their backup data, they need to use a service that can store and retrieve objects through standard file storage protocols for quick recovery. <br><br>
    Which of the following options will meet this requirement?
  </h1>

  <label for="optionA">A. Use the AWS Storage Gateway volume gateway to store the backup data and directly access it using Amazon S3 API actions.</label>
  <label for="optionB">B. Use Amazon EBS volumes to store all the backup data and attach it to an Amazon EC2 instance.</label>
  <label for="optionC">C. Use AWS Snowball Edge to directly backup the data in Amazon S3 Glacier.</label>
  <label for="optionD" class="correct-answer">D. Use the AWS Storage Gateway file gateway to store all the backup data in Amazon S3. ✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company needs to use Amazon S3 to store irreproducible financial documents. 
    For their quarterly reporting, the files are required to be retrieved after a period of 3 months. 
    There will be some occasions when a surprise audit will be held, which requires access to the archived data that they need to present immediately. <br><br>
    What will you do to satisfy this requirement in a cost-effective way?
  </h1>

  <label for="optionA">A. Use Amazon Glacier Deep Archive</label>
  <label for="optionB">B.  Use Amazon S3 - Intelligent Tiering</label>
  <label for="optionC" class="correct-answer">C. Use Amazon S3 Standard - Infrequent Access  ✓✓</label>
  <label for="optionD">D. Use Amazon S3 Standard</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company needs to use Amazon S3 to store irreproducible financial documents. 
    For their quarterly reporting, the files are required to be retrieved after a period of 3 months. 
    There will be some occasions when a surprise audit will be held, which requires access to the archived data that they need to present immediately. <br><br>
    What will you do to satisfy this requirement in a cost-effective way?
  </h1>

  <label for="optionA">A. Use Amazon Glacier Deep Archive</label>
  <label for="optionB" class="correct-answer">B. Use Amazon S3 - Intelligent Tiering ✓✓</label>
  <label for="optionC">C. Use Amazon S3 Standard - Infrequent Access</label>
  <label for="optionD">D. Use Amazon S3 Standard</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company deployed a high-performance computing (HPC) cluster that spans multiple EC2 instances across multiple Availability Zones and processes various wind simulation models. 
    Currently, the Solutions Architect is experiencing a slowdown in their applications and upon further investigation, it was discovered that it was due to latency issues. <br><br>
    Which is the MOST suitable solution that the Solutions Architect should implement to provide low-latency network performance necessary for tightly-coupled node-to-node communication of the HPC cluster?
  </h1>

  <label for="optionA" class="correct-answer">A. Set up a cluster placement group within a single Availability Zone in the same AWS Region ✓✓</label>
  <label for="optionB">B. Set up a spread placement group across multiple Availability Zones in multiple AWS Regions</label>
  <label for="optionC">C. Set up AWS Direct Connect connections across multiple Availability Zones for increased bandwidth throughput and more consistent network experience</label>
  <label for="optionD">D. Use EC2 Dedicated Instances</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is developing a two-tier web application on AWS. The company's developers have deployed the application on an Amazon EC2 instance that connects directly to a backend Amazon RDS database. 
    The company must not hardcode database credentials in the application. The company must also implement a solution to automatically rotate the database credentials on a regular basis. <br><br>
    Which solution will meet these requirements with the LEAST operational overhead?
  </h1>

  <label for="optionA">A. Store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store. Turn on automatic rotation for the encrypted parameters. Attach the required permission to the EC2 role to grant access to the encrypted parameters.</label>
  <label for="optionB" class="correct-answer">B. Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the secret. Attach the required permission to the EC2 role to grant access to the secret ✓✓</label>
  <label for="optionC">C. Store the database credentials in the instance metadata. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS credentials and instance metadata at the same time.</label>
  <label for="optionD">D. Store the database credentials in a configuration file in an encrypted Amazon S3 bucket. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS credentials and the credentials in the configuration file at the same time. Use S3 Versioning to ensure the ability to fall back to previous values.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A manufacturing company launched a new type of IoT sensor. The sensor will be used to collect large streams of data records. 
    You need to create a solution that can ingest and analyze the data in real-time with millisecond response times. <br><br>
    Which of the following is the best option that you should implement in this scenario?
  </h1>

  <label for="optionA">A. Ingest the data using Amazon Simple Queue Service and create an AWS Lambda function to store the data in Amazon Redshift</label>
  <label for="optionB">B. Ingest the data using Amazon Kinesis Data Streams and create an AWS Lambda function to store the data in Amazon Redshift</label>
  <label for="optionC">C. Ingest the data using Amazon Kinesis Data Firehose and create an AWS Lambda function to store the data in Amazon DynamoDB</label>
  <label for="optionD" class="correct-answer">D. Ingest the data using Amazon Kinesis Data Streams and create an AWS Lambda function to store the data in Amazon DynamoDB ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    In Amazon EC2, you can manage your instances from the moment you launch them up to their termination. 
    You can flexibly control your computing costs by changing the EC2 instance state. <br><br>
    Which of the following statements is true regarding EC2 billing? (Select TWO.)
  </h1>

  <label for="optionA" class="correct-answer">A. You will be billed when your On-Demand instance is preparing to hibernate with a stopping state ✓✓</label>
  <label for="optionB" class="correct-answer">B. You will be billed when your On-Demand instance is in pending state ✓✓</label>
  <label for="optionC">C. You will not be billed for any instance usage while an instance is not in the running state</label>
  <label for="optionD">D. You will be billed when your Spot instance is preparing to stop with a stopping state</label>
  <label for="optionE">E. You will be billed when your Reserved instance is in terminated state</label>
</form>
<form class="question-form">
  <h1 class="question">
    A solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 origin to store a static website. 
    The company's security policy requires that all website traffic be inspected by AWS WAF. <br><br>
    How should the solutions architect comply with these requirements?
  </h1>

  <label for="optionA">A. Configure an S3 bucket policy to accept requests coming from the AWS WAF Amazon Resource Name (ARN) only</label>
  <label for="optionB">B. Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin</label>
  <label for="optionC">C. Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only. Associate AWS WAF to CloudFront</label>
  <label for="optionD" class="correct-answer">D. Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the distribution ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company stores its application logs in an Amazon CloudWatch Logs log group. 
    A new policy requires the company to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in near-real time. <br><br>
    Which solution will meet this requirement with the LEAST operational overhead?
  </h1>

  <label for="optionA" class="correct-answer">A. Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery stream's source. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery stream's destination ✓✓</label>
  <label for="optionB">B. Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service)</label>
  <label for="optionC">C. Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service)</label>
  <label for="optionD">D. Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service)</label>
</form>
<form class="question-form">
  <h1 class="question">
    The media company that you are working for has a video transcoding application running on Amazon EC2. 
    Each EC2 instance polls a queue to find out which video should be transcoded, and then runs a transcoding process. 
    If this process is interrupted, the video will be transcoded by another instance based on the queuing system. 
    This application has a large backlog of videos which need to be transcoded. Your manager would like to reduce this backlog by adding more EC2 instances, however, these instances are only needed until the backlog is reduced. <br><br>
    In this scenario, which type of Amazon EC2 instance is the most cost-effective type to use?
  </h1>

  <label for="optionA" class="correct-answer">A. Spot instances ✓✓</label>
  <label for="optionB">B. Reserved instances</label>
  <label for="optionC">C. Dedicated instances</label>
  <label for="optionD">D. On-demand instances</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company runs an application on a large fleet of Amazon EC2 instances. The application reads and writes entries into an Amazon DynamoDB table. 
    The size of the DynamoDB table continuously grows but the application needs only data from the last 30 days. 
    The company needs a solution that minimizes cost and development effort. <br><br>
    Which solution meets these requirements?
  </h1>

  <label for="optionA">A. Use an AWS CloudFormation template to deploy the complete solution. Redeploy the CloudFormation stack every 30 days and delete the original stack</label>
  <label for="optionB">B. Use an EC2 instance that runs a monitoring application from AWS Marketplace. Configure the monitoring application to use Amazon DynamoDB Streams to store the timestamp when a new item is created in the table. Use a script that runs on the EC2 instance to delete items that have a timestamp that is older than 30 days</label>
  <label for="optionC">C. Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is created in the table. Configure the Lambda function to delete items in the table that are older than 30 days</label>
  <label for="optionD" class="correct-answer">D. Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each new item that is created in the table. Configure DynamoDB to use the attribute as the TTL attribute ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is building an internal application that serves as a repository for images uploaded by a couple of users. 
    Whenever a user uploads an image, it would be sent to Kinesis Data Streams for processing before it is stored in an S3 bucket. 
    If the upload was successful, the application will return a prompt informing the user that the operation was successful. 
    The entire processing typically takes about 5 minutes to finish. <br><br>
    Which of the following options will allow you to asynchronously process the request to the application from upload request to Kinesis, S3, and return a reply in the most cost-effective manner?
  </h1>

  <label for="optionA">A. Use a combination of Lambda and Step Functions to orchestrate service components and asynchronously process the requests</label>
  <label for="optionB">B. Replace the Kinesis Data Streams with an Amazon SQS queue. Create a Lambda function that will asynchronously process the requests</label>
  <label for="optionC">C. Use a combination of SNS to buffer the requests and then asynchronously process them using On-Demand EC2 Instances</label>
  <label for="optionD" class="correct-answer">D.  Use a combination of SQS to queue the requests and then asynchronously process them using On-Demand EC2 Instances  ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A government agency plans to store confidential tax documents on AWS. 
    Due to the sensitive information in the files, the Solutions Architect must restrict the data access requests made to the storage solution to a specific Amazon VPC only. 
    The solution should also prevent the files from being deleted or overwritten to meet the regulatory requirement of having a write-once-read-many (WORM) storage model. <br><br>
    Which combination of the following options should the Architect implement? (Select TWO.)
  </h1>

  <label for="optionA" class="correct-answer">A. Create a new Amazon S3 bucket with the S3 Object Lock feature enabled. Store the documents in the bucket and set the Legal Hold option for object retention ✓✓</label>
  <label for="optionB" class="correct-answer">B. Configure an Amazon S3 Access Point for the S3 bucket to restrict data access to a particular Amazon VPC only ✓✓</label>
  <label for="optionC">C. Set up a new Amazon S3 bucket to store the tax documents and integrate it with AWS Network Firewall. Configure the Network Firewall to only accept data access requests from a specific Amazon VPC</label>
  <label for="optionD">D. Store the tax documents in the Amazon S3 Glacier Instant Retrieval storage class to restrict fast data retrieval to a particular Amazon VPC of your choice</label>
  <label for="optionE">E. Enable Object Lock but disable Object Versioning on the new Amazon S3 bucket to comply with the write-once-read-many (WORM) storage model requirement</label>
</form>
<form class="question-form">
  <h1 class="question">
    A computer animation film studio has a web application running on an Amazon EC2 instance. 
    It uploads 5 GB video objects to an Amazon S3 bucket. Video uploads are taking longer than expected, which impacts the performance of your application. <br><br>
    Which method will help improve the performance of the application?
  </h1>

  <label for="optionA">A. Enable Enhanced Networking with the Elastic Network Adapter (ENA) on your EC2 instances</label>
  <label for="optionB" class="correct-answer">B. Use Amazon S3 Multipart Upload API ✓✓</label>
  <label for="optionC">C. Leverage on Amazon CloudFront and use HTTP POST method to reduce latency</label>
  <label for="optionD">D. Use Amazon Elastic Block Store Provisioned IOPS and an Amazon EBS-optimized instance</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company plans to migrate all of their applications to AWS. The Solutions Architect suggested storing all the data on EBS volumes. 
    The Chief Technical Officer is worried that EBS volumes are not appropriate for the existing workloads due to compliance requirements, downtime scenarios, and IOPS performance. <br><br>
    Which of the following are valid points in proving that EBS is the best service to use for migration? (Select TWO.)
  </h1>

  <label for="optionA" class="correct-answer">A. EBS volumes support live configuration changes while in production which means that you can modify the volume type, volume size, and IOPS capacity without service interruptions ✓✓</label>
  <label for="optionB">B. Amazon EBS provides the ability to create snapshots (backups) of any EBS volume and write a copy of the data in the volume to Amazon RDS, where it is stored redundantly in multiple Availability Zones</label>
  <label for="optionC">C. EBS volumes can be attached to any EC2 instance in any Availability Zone</label>
  <label for="optionD">D. When you create an EBS volume in an Availability Zone, it is automatically replicated on a separate AWS region to prevent data loss due to a failure of any single hardware component</label>
  <label for="optionE" class="correct-answer">E. An EBS volume is off-instance storage that can persist independently from the life of an instance ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles during peak operating hours. 
    The company wants to use these data points in its existing analytics platform. 
    A Solutions Architect must determine the most viable multi-tier option to support this architecture. 
    The data points must be accessible from the REST API. <br><br>
    Which action meets these requirements for storing and retrieving location data?
  </h1>

  <label for="optionA">A. Use Amazon Athena with Amazon S3</label>
  <label for="optionB" class="correct-answer">B. Use Amazon API Gateway with Amazon Kinesis Data Analytics  ✓✓</label>
  <label for="optionC">C. Use Amazon QuickSight with Amazon Redshift</label>
  <label for="optionD">D. Use Amazon API Gateway with AWS Lambda</label>
</form>

<form class="question-form">
  <h1 class="question">
    A production MySQL database hosted on Amazon RDS is running out of disk storage. 
    The management has consulted its Solutions Architect to increase the disk space without impacting the database performance. <br><br>
    How can the Solutions Architect satisfy the requirement with the LEAST operational overhead?
  </h1>

  <label for="optionA">A. Change the default_storage_engine of the DB instance's parameter group to MyISAM</label>
  <label for="optionB">B. Increase the allocated storage for the DB instance</label>
  <label for="optionC">C. Modify the DB instance storage type to Provisioned IOPS</label>
  <label for="optionD" class="correct-answer">D. Modify the DB instance settings and enable storage autoscaling ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company plans to migrate a MySQL database from an on-premises data center to the AWS Cloud. 
    This database will be used by a legacy batch application that has steady-state workloads in the morning but has its peak load at night for the end-of-day processing. 
    You need to choose an EBS volume that can handle a maximum of 450 GB of data and can also be used as the system boot volume for your EC2 instance. <br><br>
    Which of the following is the most cost-effective storage type to use in this scenario?
  </h1>

  <label for="optionA">A. Amazon EBS Cold HDD (sc1)</label>
  <label for="optionB">B. Amazon EBS Provisioned IOPS SSD (io1)</label>
  <label for="optionC" class="correct-answer">C. Amazon EBS General Purpose SSD (gp2) ✓✓</label>
  <label for="optionD">D. Amazon EBS Throughput Optimized HDD (st1)</label>
</form>
<form class="question-form">
  <h1 class="question">
    There are a lot of outages in the Availability Zone of your RDS database instance to the point that you have lost access to the database. <br><br>
    What could you do to prevent losing access to your database in case that this event happens again?
  </h1>

  <label for="optionA">A. Create a read replica</label>
  <label for="optionB">B. Increase the database instance size</label>
  <label for="optionC" class="correct-answer">C. Enable Multi-AZ failover ✓✓</label>
  <label for="optionD">D. Make a snapshot of the database</label>
</form>
<form class="question-form">
  <h1 class="question">
    A content management system (CMS) is hosted on a fleet of auto-scaled, On-Demand EC2 instances that use Amazon Aurora as its database. 
    Currently, the system stores the file documents that the users upload in one of the attached EBS Volumes. 
    Your manager noticed that the system performance is quite slow and he has instructed you to improve the architecture of the system. <br><br>
    In this scenario, what will you do to implement a scalable, high-available POSIX-compliant shared file system?
  </h1>

  <label for="optionA">A. Create an S3 bucket and use this as the storage for the CMS</label>
  <label for="optionB" class="correct-answer">B. Use EFS ✓✓</label>
  <label for="optionC">C. Use ElastiCache</label>
  <label for="optionD">D. Upgrade your existing EBS volumes to Provisioned IOPS SSD Volumes</label>
</form>
<form class="question-form">
  <h1 class="question">
    There was an incident in your production environment where the user data stored in the S3 bucket has been accidentally deleted by one of the Junior DevOps Engineers. 
    The issue was escalated to your manager and after a few days, you were instructed to improve the security and protection of your AWS resources. <br><br>
    What combination of the following options will protect the S3 objects in your bucket from both accidental deletion and overwriting? (Select TWO.)
  </h1>

  <label for="optionA">A. Enable Amazon S3 Intelligent-Tiering</label>
  <label for="optionB" class="correct-answer">B. Enable Versioning ✓✓</label>
  <label for="optionC" class="correct-answer">C. Enable Multi-Factor Authentication Delete ✓✓</label>
  <label for="optionD">D. Provide access to S3 data strictly through pre-signed URL only</label>
  <label for="optionE">E. Disallow S3 Delete using an IAM bucket policy</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company runs an online marketplace web application on AWS. The application serves hundreds of thousands of users during peak hours. 
    The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. 
    Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval. <br><br>
    What should a solutions architect recommend to meet these requirements?
  </h1>

  <label for="optionA">A. Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon write. Use DynamoDB Streams to share the transactions data with other applications</label>
  <label for="optionB">B. Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3</label>
  <label for="optionC" class="correct-answer">C. Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream ✓✓</label>
  <label for="optionD">D. Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3</label>
</form>
<form class="question-form">
  <h1 class="question">
    A leading IT consulting company has an application which processes a large stream of financial data by an Amazon ECS Cluster then stores the result to a DynamoDB table. 
    You have to design a solution to detect new entries in the DynamoDB table then automatically trigger a Lambda function to run some tests to verify the processed data. <br><br>
    What solution can be easily implemented to alert the Lambda function of new entries while requiring minimal configuration change to your architecture?
  </h1>

  <label for="optionA">A. Invoke the Lambda functions using SNS each time that the ECS Cluster successfully processed financial data</label>
  <label for="optionB" class="correct-answer">B. Enable DynamoDB Streams to capture table activity and automatically trigger the Lambda function ✓✓</label>
  <label for="optionC">C. Use Systems Manager Automation to detect new entries in the DynamoDB table then automatically invoke the Lambda function for processing</label>
  <label for="optionD">D. Use CloudWatch Alarms to trigger the Lambda function whenever a new entry is created in the DynamoDB table</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company wants to streamline the process of creating multiple AWS accounts within an AWS Organization. 
    Each organizational unit (OU) must be able to launch new accounts with preapproved configurations from the security team, which will standardize the baselines and network configurations for all accounts in the organization. <br><br>
    Which solution entails the least amount of effort to implement?
  </h1>

  <label for="optionA">A. Set up an AWS Config aggregator on the root account of the organization to enable multi-account, multi-region data aggregation. Deploy conformance packs to standardize the baselines and network configurations for all accounts</label>
  <label for="optionB" class="correct-answer">B. Set up an AWS Control Tower Landing Zone. Enable pre-packaged guardrails to enforce policies or detect violations ✓✓</label>
  <label for="optionC">C. Configure AWS Resource Access Manager (AWS RAM) to launch new AWS accounts as well as standardize the baselines and network configurations for each organizational unit</label>
  <label for="optionD">D. Centralize the creation of AWS accounts using AWS Systems Manager OpsCenter. Enforce policies and detect violations to all AWS accounts using AWS Security Hub</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has an application that ingests incoming messages. These messages are then quickly consumed by dozens of other applications and microservices. 
    The number of messages varies drastically and sometimes spikes as high as 100,000 each second. The company wants to decouple the solution and increase scalability. <br><br>
    Which solution meets these requirements?
  </h1>

  <label for="optionA">A. Persist the messages to Amazon Kinesis Data Analytics. All the applications will read and process the messages</label>
  <label for="optionB">B. Deploy the application on Amazon EC2 instances in an Auto Scaling group, which scales the number of EC2 instances based on CPU metrics</label>
  <label for="optionC">C. Write the messages to Amazon Kinesis Data Streams with a single shard. All applications will read from the stream and process the messages</label>
  <label for="optionD" class="correct-answer">D. Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with one or more Amazon Simple Queue Service (Amazon SQS) subscriptions. All applications then process the messages from the queues ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A production MySQL database hosted on Amazon RDS is running out of disk storage. 
    The management has consulted its Solutions Architect to increase the disk space without impacting the database performance. <br><br>
    How can the Solutions Architect satisfy the requirement with the LEAST operational overhead?
  </h1>

  <label for="optionA">A. Modify the DB instance storage type to Provisioned IOPS</label>
  <label for="optionB">B. Increase the allocated storage for the DB instance</label>
  <label for="optionC">C. Change the default_storage_engine of the DB instance's parameter group to MyISAM</label>
  <label for="optionD" class="correct-answer">D. Modify the DB instance settings and enable storage autoscaling ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company wants to run applications in containers in the AWS Cloud. These applications are stateless and can tolerate disruptions within the underlying infrastructure. 
    The company needs a solution that minimizes cost and operational overhead. <br><br>
    What should a Solutions Architect do to meet these requirements?
  </h1>

  <label for="optionA">A. Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers</label>
  <label for="optionB" class="correct-answer">B. Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group  ✓✓</label>
  <label for="optionC">C. Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers </label>
  <label for="optionD">D. Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group</label>
</form>
<form class="question-form">
  <h1 class="question">
    A Solutions Architect is designing the cloud architecture for a new application being deployed on AWS. 
    The process should run in parallel while adding and removing application nodes as needed based on the number of jobs to be processed. 
    The processor application is stateless. The Solutions Architect must ensure that the application is loosely coupled and the job items are durably stored. <br><br>
    Which design should the Solutions Architect use?
  </h1>

  <label for="optionA">A. Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage</label>

  <label for="optionB">B. Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network usage</label>

  <label for="optionC" class="correct-answer">C. Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue ✓✓</label>

  <label for="optionD">D. Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon EBS volume. 
    For better scalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind an Application Load Balancer. 
    After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or the other, but never all of the documents at the same time. <br><br>
    What should a Solutions Architect propose to ensure users see all of their documents at once?
  </h1>

  <label for="optionA">A. Copy the data so both EBS volumes contain all the documents</label>
  <label for="optionB">B. Configure the Application Load Balancer to direct a user to the server with the documents</label>
  <label for="optionC" class="correct-answer">C. Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS ✓✓</label>
  <label for="optionD">D. Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server</label>
</form>
<form class="question-form">
  <h1 class="question">
    A Solutions Architect designed a real-time data analytics system based on Kinesis Data Stream and Lambda. 
    A week after the system has been deployed, the users noticed that it performed slowly as the data rate increases. 
    The Architect identified that the performance of the Kinesis Data Streams is causing this problem. <br><br>
    Which of the following should the Architect do to improve performance?
  </h1>

  <label for="optionA">A. Improve the performance of the stream by decreasing the number of its shards using the MergeShard command</label>
  <label for="optionB" class="correct-answer">B. Increase the number of shards of the Kinesis stream by using the UpdateShardCount command ✓✓</label>
  <label for="optionC">C. Implement Step Scaling to the Kinesis Data Stream</label>
  <label for="optionD">D. Replace the data stream with Amazon Kinesis Data Firehose instead</label>
</form>
<form class="question-form">
  <h1 class="question">
    An organization stores and manages financial records of various companies in its on-premises data center, which is almost out of space. 
    The management decided to move all of their existing records to a cloud storage service. All future financial records will also be stored in the cloud. 
    For additional security, all records must be prevented from being deleted or overwritten. <br><br>
    Which of the following should you do to meet the above requirement?
  </h1>

  <label for="optionA" class="correct-answer">A. Use AWS DataSync to move the data. Store all of your data in Amazon S3 and enable object lock ✓✓</label>
  <label for="optionB">B. Use AWS Storage Gateway to establish hybrid cloud storage. Store all of your data in Amazon S3 and enable object lock</label>
  <label for="optionC">C. Use AWS Storage Gateway to establish hybrid cloud storage. Store all of your data in Amazon EBS and enable object lock</label>
  <label for="optionD">D. Use AWS DataSync to move the data. Store all of your data in Amazon EFS and enable object lock</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company recently launched an e-commerce application that is running in the eu-east-2 region, which strictly requires six EC2 instances running at all times. 
    In that region, there are 3 Availability Zones (AZs) that you can use — eu-east-2a, eu-east-2b, and eu-east-2c. <br><br>
    Which of the following deployments provide 100% fault tolerance if any single AZ in the region becomes unavailable? (Select TWO.)
  </h1>

  <label for="optionA" class="correct-answer">A. eu-east-2a with six EC2 instances, eu-east-2b with six EC2 instances, and eu-east-2c with no EC2 instances  ✓✓</label>
  <label for="optionB">B. XXXXXXXXXX</label>
  <label for="optionC">C. eu-east-2a with two EC2 instances, eu-east-2b with four EC2 instances, and eu-east-2c with two EC2 instances</label>
  <label for="optionD" class="correct-answer">D. eu-east-2a with three EC2 instances, eu-east-2b with three EC2 instances, and eu-east-2c with three EC2 instances ✓✓</label>
  <label for="optionE">E. eu-east-2a with two EC2 instances, eu-east-2b with two EC2 instances, and eu-east-2c with two EC2 instances</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has several microservices that send messages to an Amazon SQS queue and a backend application that polls the queue to process the messages. 
    The company also has a Service Level Agreement (SLA) which defines the acceptable amount of time that can elapse from the point when the messages are received until a response is sent. 
    The backend operations are I/O-intensive as the number of messages is constantly growing, causing the company to miss its SLA. 
    The Solutions Architect must implement a new architecture that improves the application's processing time and load management. <br><br>
    Which of the following is the MOST effective solution that can satisfy the given requirement?
  </h1>

  <label for="optionA" class="correct-answer">A. Create an AMI of the backend application's EC2 instance. Use the image to set up an Auto Scaling group and configure a target tracking scaling policy based on the ApproximateAgeOfOldestMessage metric ✓✓</label>
  <label for="optionB">B. Create an AMI of the backend application's EC2 instance. Use the image to set up an Auto Scaling group and configure a target tracking scaling policy based on the CPUUtilization metric with a target value of 80%</label>
  <label for="optionC">C. Create an AMI of the backend application's EC2 instance and replace it with a larger instance size</label>
  <label for="optionD">D. Create an AMI of the backend application's EC2 instance and launch it to a cluster placement group</label>
</form>
<form class="question-form">
  <h1 class="question">
    An online stocks trading application that stores financial data in an S3 bucket has a lifecycle policy that moves older data to Glacier every month. 
    There is a strict compliance requirement where a surprise audit can happen at any time, and you should be able to retrieve the required data in under 15 minutes under all circumstances. 
    Your manager instructed you to ensure that retrieval capacity is available when you need it and should handle up to 150 MB/s of retrieval throughput. <br><br>
    Which of the following should you do to meet the above requirement? (Select TWO.)
  </h1>

  <label for="optionA" class="correct-answer">A. Use Expedited Retrieval to access the financial data ✓✓</label>
  <label for="optionB" class="correct-answer">B. Purchase provisioned retrieval capacity ✓✓</label>
  <label for="optionC">C. Retrieve the data using Amazon Glacier Select</label>
  <label for="optionD">D. Do no nothing</label>
  <label for="optionE">E. Use Bulk Retrieval to access the financial data</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company has a web application hosted in AWS cloud where the application logs are sent to Amazon CloudWatch. 
    Lately, the web application has recently been encountering some errors which can be resolved simply by restarting the instance. <br><br>
    What will you do to automatically restart the EC2 instances whenever the same application error occurs?
  </h1>

  <label for="optionA" class="correct-answer">A. First, look at the existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance ✓✓</label>
  <label for="optionB">B. First, look at the existing Flow logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance</label>
  <label for="optionC">C. First, look at the existing Flow logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which calls a Lambda function that invokes an action to restart the EC2 instance</label>
  <label for="optionD">D. First, look at the existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create an alarm in Amazon SNS for that custom metric which invokes an action to restart the EC2 instance</label>
</form>
<form class="question-form">
  <h1 class="question">
    An online trading platform with thousands of clients across the globe is hosted in AWS. 
    To reduce latency, you have to direct user traffic to the nearest application endpoint to the client. 
    The traffic should be routed to the closest edge location via an Anycast static IP address. 
    AWS Shield should also be integrated into the solution for DDoS protection. <br><br>
    Which of the following is the MOST suitable service that the Solutions Architect should use to satisfy the above requirements?
  </h1>

  <label for="optionA">A. AWS WAF</label>
  <label for="optionB" class="correct-answer">B. AWS Global Accelerator ✓✓</label>
  <label for="optionC">C. AWS PrivateLink</label>
  <label for="optionD">D. Amazon CloudFront</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region for an upcoming event that will last 1 week. <br><br>
    What should the company do to guarantee the EC2 capacity?
  </h1>

  <label for="optionA">A. Purchase Reserved instances that specify the Region needed</label>
  <label for="optionB">B. Create an On Demand Capacity Reservation that specifies the Region needed</label>
  <label for="optionC">C. Purchase Reserved instances that specify the Region and three Availability Zones needed</label>
  <label for="optionD" class="correct-answer">D. Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company needs secure access to its Amazon RDS for MySQL database that is used by multiple applications. 
    Each IAM user must use a short-lived authentication token to connect to the database. <br><br>
    Which of the following is the most suitable solution in this scenario?
  </h1>

  <label for="optionA">A. Use AWS Secrets Manager to generate and store short-lived authentication tokens</label>
  <label for="optionB">B. Use an MFA token to access and connect to a database</label>
  <label for="optionC">C. Use AWS SSO to access the RDS database</label>
  <label for="optionD" class="correct-answer">D. Use IAM DB Authentication and create database accounts using the AWS-provided AWSAuthenticationPlugin plugin in MySQL ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    An organization needs to control the access for several S3 buckets. 
    They plan to use a gateway endpoint to allow access to trusted buckets. <br><br>
    Which of the following could help you achieve this requirement?
  </h1>

  <label for="optionA">A. Generate a bucket policy for trusted S3 buckets</label>
  <label for="optionB">B. Generate a bucket policy for trusted VPCs</label>
  <label for="optionC">C. Generate an endpoint policy for trusted VPCs</label>
  <label for="optionD" class="correct-answer">D. Generate an endpoint policy for trusted S3 buckets ✓✓</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company installed sensors to track the number of people who visit the park. 
    The data is sent every day to an Amazon Kinesis stream with default settings for processing, 
    in which a consumer is configured to process the data every other day. 
    You noticed that the S3 bucket is not receiving all of the data that is being sent to the Kinesis stream. 
    You checked the sensors if they are properly sending the data to Amazon Kinesis and verified that the data is indeed sent every day. <br><br>
    What could be the reason for this?
  </h1>

  <label for="optionA">A. There is a problem in the sensors. They probably had some intermittent connection hence, the data is not sent to the stream</label>
  <label for="optionB" class="correct-answer">B. By default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream ✓✓</label>
  <label for="optionC">C. By default, Amazon S3 stores the data for 1 day and moves it to Amazon Glacier</label>
  <label for="optionD">D. Your AWS account was hacked and someone has deleted some data in your Kinesis stream</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is using a combination of API Gateway and Lambda for the web services of the online web portal that is being accessed by hundreds of thousands of clients each day. 
    They will be announcing a new revolutionary product and it is expected that the web portal will receive a massive number of visitors all around the globe. <br><br>
    How can you protect the backend systems and applications from traffic spikes?
  </h1>

  <label for="optionA">A. Manually upgrade the EC2 instances being used by API Gateway</label>
  <label for="optionB">B. Deploy Multi-AZ in API Gateway with Read Replica</label>
  <label for="optionC" class="correct-answer">C. Use throttling limits in API Gateway ✓✓</label>
  <label for="optionD">D. API Gateway will automatically scale and handle massive traffic spikes so you do not have to do anything</label>
</form>
<form class="question-form">
  <h1 class="question">
    A company is building an ecommerce web application on AWS. 
    The application sends information about new orders to an Amazon API Gateway REST API to process. 
    The company wants to ensure that orders are processed in the order that they are received. <br><br>
    Which solution will meet these requirements?
  </h1>

  <label for="optionA">A. Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to the topic to perform processing</label>
  <label for="optionB" class="correct-answer">B. Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing ✓✓</label>
  <label for="optionC">C. Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the application receives an order. Configure the SQS standard queue to invoke an AWS Lambda function for processing</label>
  <label for="optionD">D. Use an API Gateway authorizer to block any requests while the application processes an order</label>
</form>



<form class="question-form">
  <h1 class="question">
    A company is deploying a new public web application to AWS. The application will run behind an Application Load Balancer (ALB). 
    The application needs to be encrypted at the edge with an SSL/TLS certificate that is issued by an external certificate authority (CA). 
    The certificate must be rotated each year before the certificate expires. <br>
    What should a solutions architect do to meet these requirements?
  </h1>

  <label for="optionA">A. Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Import the key material from the certificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.</label>
  
  <label for="optionB" class="correct-answer">
    B. Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the ALB. 
    Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the certificate is nearing expiration. 
    Rotate the certificate manually. ✓✓
  </label>
  
  <label for="optionC">C. Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.</label>
  
  <label for="optionD">D. Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.</label>
</form>

<form class="question-form">
  <h1 class="question">
    A company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed. <br>
    What should the solutions architect recommend?
  </h1>

  <label for="optionA">A. Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.</label>
  <label for="optionB">B. Move the website to Amazon S3. Use cross-Region replication between Regions.</label>
  <label for="optionC" class="correct-answer">C. Use Amazon CloudFront with a custom origin pointing to the on-premises servers ✓✓</label>
  <label for="optionD">D. Use an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers.</label>
</form>
<form class="question-form">
  <h1 class="question">
    A corporation has recruited a new cloud engineer who should not have access to the CompanyConfidential Amazon S3 bucket. 
    The cloud engineer must have read and write permissions on an S3 bucket named AdminTools. <br>
    Which IAM policy will satisfy these criteria?
  </h1>

  <label for="optionA" class="correct-answer">
    A.
    <pre>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Action": "s3:*",
      "Resource": "arn:aws:s3:::CompanyConfidential/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::AdminTools/*"
    }
  ]
} ✓✓</pre>
  </label>

  <label for="optionB">
    B.
    <pre>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": "*"
    }
  ]
}</pre>
  </label>

  <label for="optionC">
    C.
    <pre>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "*"
    }
  ]
}</pre>
  </label>

  <label for="optionD">
    D.
    <pre>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::CompanyConfidential/*"
    }
  ]
}</pre>
  </label>
</form>

<form class="question-form">
  <h1 class="question">
    A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum and maximum values for the Auto Scaling group in the backup Region are set to zero.
  </h1>
  <h1 class="question">
    An Amazon RDS Multi-AZ DB instance stores the application's data. The DB instance has a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.
  </h1>
  <h1 class="question">
    The company needs to reduce its <strong>RTO to less than 15 minutes</strong> by giving the application the ability to <strong>automatically fail over</strong> to the backup Region. The company does <strong>not</strong> have the budget for an active-active strategy.
  </h1>
  <h1 class="question">
    What should a solutions architect recommend to meet these requirements?
  </h1>

  <label for="optionA">
    A. Reconfigure the application’s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.
  </label><br>

  <label for="optionB" class="correct-answer">
    B. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application’s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs. &#x2713;
  </label><br>

  <label for="optionC">
    C. Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region. Reconfigure the application’s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Remove the read replica. Replace the read replica with a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3.
  </label><br>

  <label for="optionD">
    D. Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node cluster for an in-memory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application to function, each piece of the infrastructure must be healthy and in an active state.
  </h1>
  <h1 class="question">
    A solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least possible downtime.
  </h1>
  <h1 class="question">
    Which combination of steps will meet these requirements? (Choose <strong>three</strong>.)
  </h1>

  <label for="optionA" class="correct-answer">
    A. Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances. &#x2713;
  </label><br>

  <label for="optionB">
    B. Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are configured in unlimited mode.
  </label><br>

  <label for="optionC">
    C. Modify the DB instance to create a read replica in the same Availability Zone. Promote the read replica to be the primary DB instance in failure scenarios.
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones. &#x2713;
  </label><br>

  <label for="optionE">
    E. Create a replication group for the ElastiCache for Redis cluster. Configure the cluster to use an Auto Scaling group that has a minimum capacity of two instances.
  </label><br>

  <label for="optionF" class="correct-answer">
    F. Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster. &#x2713;
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.
  </h1>
  <h1 class="question">
    After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.
  </h1>
  <h1 class="question">
    While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.
  </h1>
  <h1 class="question">
    Which combination of steps will meet this requirement with the <strong>LEAST amount of operational overhead</strong>? (Choose <strong>two</strong>.)
  </h1>

  <label for="optionA" class="correct-answer">
    A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3. &#x2713;
  </label><br>

  <label for="optionB">
    B. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.
  </label><br>

  <label for="optionC">
    C. Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.
  </label><br>

  <label for="optionD">
    D. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.
  </label><br>

  <label for="optionE" class="correct-answer">
    E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page. &#x2713;
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.
  </h1>
  <h1 class="question">
    After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.
  </h1>
  <h1 class="question">
    While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.
  </h1>
  <h1 class="question">
    Which combination of steps will meet this requirement with the <strong>LEAST amount of operational overhead</strong>? (Choose <strong>two</strong>.)
  </h1>

  <label for="optionA" class="correct-answer">
    A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3. &#x2713;
  </label><br>

  <label for="optionB">
    B. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.
  </label><br>

  <label for="optionC">
    C. Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.
  </label><br>

  <label for="optionD">
    D. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.
  </label><br>

  <label for="optionE" class="correct-answer">
    E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page. &#x2713;
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A company hosts a three-tier ecommerce application on a fleet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance.
  </h1>
  <h1 class="question">
    The company wants to optimize customer session management during transactions. The application must store session data durably.
  </h1>
  <h1 class="question">
    Which solutions will meet these requirements? (Choose <strong>two</strong>.)
  </h1>

  <label for="optionA">
    A. Turn on the sticky sessions feature (session affinity) on the ALB.
  </label><br>

  <label for="optionB" class="correct-answer">
    B. Use an Amazon DynamoDB table to store customer session information. &#x2713;
  </label><br>

  <label for="optionC">
    C. Deploy an Amazon Cognito user pool to manage user session information.
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information. &#x2713;
  </label><br>

  <label for="optionE">
    E. Use AWS Systems Manager Application Manager in the application to manage user session information.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A company has a few AWS accounts for development and wants to move its production application to AWS.
  </h1>
  <h1 class="question">
    The company needs to enforce Amazon Elastic Block Store (Amazon EBS) encryption at rest in current production accounts and future production accounts only. The company needs a solution that includes built-in blueprints and guardrails.
  </h1>
  <h1 class="question">
    Which combination of steps will meet these requirements? (Choose <strong>three</strong>.)
  </h1>

  <label for="optionA">
    A. Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts.
  </label><br>

  <label for="optionB">
    B. Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Create a new AWS Control Tower landing zone in the company's management account. Add production and development accounts to production and development OUs, respectively. &#x2713;
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance. &#x2713;
  </label><br>

  <label for="optionE">
    E. Create a guardrail from the management account to detect EBS encryption.
  </label><br>

  <label for="optionF" class="correct-answer">
    F. Create a guardrail for the production OU to detect EBS encryption. &#x2713;
  </label><br>
</form>


<form class="question-form">
  <h1 class="question">
    A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application Load Balancer.
  </h1>
  <h1 class="question">
    The company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not adversely affect legitimate traffic to the application.
  </h1>
  <h1 class="question">
    How should a solutions architect configure the web ACLs to meet these requirements?
  </h1>

  <label for="optionA" class="correct-answer">
    A. Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positives. Over time, change the action of the web ACL rules from Count to Block. &#x2713;
  </label><br>

  <label for="optionB">
    B. Use only rate-based rules in the web ACLs, and set the throttle limit as high as possible. Temporarily block all requests that exceed the limit. Define nested rules to narrow the scope of the rate tracking.
  </label><br>

  <label for="optionC">
    C. Set the action of the web ACL rules to Block. Use only AWS managed rule groups in the web ACLs. Evaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWS WAF logs.
  </label><br>

  <label for="optionD">
    D. Use only custom rule groups in the web ACLs, and set the action to Allow. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positives. Over time, change the action of the web ACL rules from Allow to Block.
  </label><br>
</form>

<form class="question-form">
  <h1 class="question">
    Your company policies require encryption of sensitive data at rest.
  </h1>
  <h1 class="question">
    You are considering the possible options for protecting data while storing it at rest on an EBS data volume, attached to an EC2 instance.
  </h1>
  <h1 class="question">
    Which of these options would allow you to encrypt your data at rest? (Choose three.)
  </h1>

  <label for="optionA" class="correct-answer">
    A. Implement third party volume encryption tools &#x2713;
  </label><br>

  <label for="optionB">
    B. Implement SSL/TLS for all services running on the server
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Encrypt data inside your applications before storing it on EBS &#x2713;
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Encrypt data using native data encryption drivers at the file system level &#x2713;
  </label><br>

  <label for="optionE">
    E. Do nothing as EBS volumes are encrypted by default
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A customer is deploying an SSL-enabled web application to AWS and would like to implement a separation of roles between the EC2 service administrators that are entitled to log in to instances and make API calls, and the security officers who will maintain and have exclusive access to the application's X.509 certificate that contains the private key.
  </h1>

  <label for="optionA">
    A. Upload the certificate on an S3 bucket owned by the security officers and accessible only by EC2 Role of the web servers.
  </label><br>

  <label for="optionB">
    B. Configure the web servers to retrieve the certificate upon boot from a CloudHSM managed by the security officers.
  </label><br>

  <label for="optionC">
    C. Configure system permissions on the web servers to restrict access to the certificate only to the authority security officers.
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Configure IAM policies authorizing access to the certificate store only to the security officers and terminate SSL on an ELB. &#x2713;
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    You have recently joined a startup company building sensors to measure street noise and air quality in urban areas.
  </h1>
  <h1 class="question">
    The company has been running a pilot deployment of around 100 sensors for 3 months. Each sensor uploads 1KB of sensor data every minute to a backend hosted on AWS.
  </h1>
  <h1 class="question">
    During the pilot, you measured a peak of 10 IOPS on the database, and you stored an average of 3GB of sensor data per month in the database.
  </h1>
  <h1 class="question">
    The current deployment consists of a load-balanced auto-scaled ingestion layer using EC2 instances and a PostgreSQL RDS database with 500GB standard storage.
  </h1>
  <h1 class="question">
    The pilot is considered a success and your CEO has managed to get the attention of some potential investors. The business plan requires a deployment of at least 100K sensors, which needs to be supported by the backend. You also need to store sensor data for at least two years to be able to compare year-over-year improvements.
  </h1>
  <h1 class="question">
    To secure funding, you have to make sure that the platform meets these requirements and leaves room for further scaling.
    <br><br>
    Which setup will meet the requirements?
  </h1>

  <label for="optionA">
    A. Add an SQS queue to the ingestion layer to buffer writes to the RDS instance.
  </label><br>

  <label for="optionB" class="correct-answer">
    B. Ingest data into a DynamoDB table and move old data to a Redshift cluster. &#x2713;
  </label><br>

  <label for="optionC">
    C. Replace the RDS instance with a 6-node Redshift cluster with 96TB of storage.
  </label><br>

  <label for="optionD">
    D. Keep the current architecture but upgrade RDS storage to 3TB and 10K provisioned IOPS.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A web company is looking to implement an intrusion detection and prevention system into their deployed VPC.
  </h1>
  <h1 class="question">
    This platform should have the ability to scale to thousands of instances running inside of the VPC.
  </h1>
  <h1 class="question">
    How should they architect their solution to achieve these goals?
  </h1>

  <label for="optionA">
    A. Configure an instance with monitoring software and the elastic network interface (ENI) set to promiscuous mode packet sniffing to see all traffic across the VPC.
  </label><br>

  <label for="optionB">
    B. Create a second VPC and route all traffic from the primary application VPC through the second VPC where the scalable virtualized IDS/IPS platform resides.
  </label><br>

  <label for="optionC">
    C. Configure servers running in the VPC using the host-based 'route' commands to send all traffic through the platform to a scalable virtualized IDS/IPS.
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Configure each host with an agent that collects all network traffic and sends that traffic to the IDS/IPS platform for inspection. &#x2713;
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A company is storing data on Amazon Simple Storage Service (S3). The company's security policy mandates that data is encrypted at rest.
  </h1>
  <h1 class="question">
    Which of the following methods can achieve this? (Choose three.)
  </h1>

  <label for="optionA" class="correct-answer">
    A. Use Amazon S3 server-side encryption with AWS Key Management Service managed keys. &#x2713;
  </label><br>

  <label for="optionB" class="correct-answer">
    B. Use Amazon S3 server-side encryption with customer-provided keys. &#x2713;
  </label><br>

  <label for="optionC">
    C. Use Amazon S3 server-side encryption with EC2 key pair.
  </label><br>

  <label for="optionD">
    D. Use Amazon S3 bucket policies to restrict access to the data at rest.
  </label><br>

  <label for="optionE" class="correct-answer">
    E. Encrypt the data on the client-side before ingesting to Amazon S3 using their own master key. &#x2713;
  </label><br>

  <label for="optionF">
    F. Use SSL to encrypt the data while in transit to Amazon S3.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    Your firm has uploaded a large amount of aerial image data to S3. In the past, in your on-premises environment, you used a dedicated group of servers to often process this data and used Rabbit MQ — an open-source messaging system — to get job information to the servers.
  </h1>
  <h1 class="question">
    Once processed, the data would go to tape and be shipped offsite. Your manager told you to stay with the current design, and leverage AWS archival storage and messaging services to minimize cost.
  </h1>
  <h1 class="question">
    Which is correct?
  </h1>

  <label for="optionA">
    A. Use SQS for passing job messages, use CloudWatch alarms to terminate EC2 worker instances when they become idle. Once data is processed, change the storage class of the S3 objects to Reduced Redundancy Storage.
  </label><br>

  <label for="optionB">
    B. Setup Auto-Scaled workers triggered by queue depth that use spot instances to process messages in SOS. Once data is processed, change the storage class of the S3 objects to Reduced Redundancy Storage.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Setup Auto-Scaled workers triggered by queue depth that use spot instances to process messages in SQS. Once data is processed, change the storage class of the S3 objects to Glacier. &#x2713;
  </label><br>

  <label for="optionD">
    D. Use SNS to pass job messages, use CloudWatch alarms to terminate spot worker instances when they become idle. Once data is processed, change the storage class of the S3 object to Glacier.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    You've been hired to enhance the overall security posture for a very large e-commerce site.
  </h1>
  <h1 class="question">
    They have a well-architected multi-tier application running in a VPC that uses ELBs in front of both the web and the app tier, with static assets served directly from S3.
  </h1>
  <h1 class="question">
    They are using a combination of RDS and DynamoDB for their dynamic data and then archiving nightly into S3 for further processing with EMR.
  </h1>
  <h1 class="question">
    They are concerned because they found questionable log entries and suspect someone is attempting to gain unauthorized access.
  </h1>
  <h1 class="question">
    Which approach provides a cost-effective, scalable mitigation to this kind of attack?
  </h1>

  <label for="optionA">
    A. Recommend that they lease space at a DirectConnect partner location and establish a 1G DirectConnect connection to their VPC. They would then establish Internet connectivity into their space, filter the traffic in a hardware Web Application Firewall (WAF), and then pass the traffic through the DirectConnect connection into their application running in their VPC.
  </label><br>

  <label for="optionB">
    B. Add previously identified hostile source IPs as an explicit INBOUND DENY NACL to the web tier subnet.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Add a WAF tier by creating a new ELB and an Auto Scaling group of EC2 instances running a host-based WAF. They would redirect Route 53 to resolve to the new WAF tier ELB. The WAF tier would then pass the traffic to the current web tier. The web tier Security Groups would be updated to only allow traffic from the WAF tier Security Group. &#x2713;
  </label><br>

  <label for="optionD">
    D. Remove all but TLS 1.2 from the web tier ELB and enable Advanced Protocol Filtering. This will enable the ELB itself to perform WAF functionality.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    Your company is in the process of developing a next-generation pet collar that collects biometric information to assist families with promoting healthy lifestyles for their pets.
  </h1>
  <h1 class="question">
    Each collar will push 30KB of biometric data in JSON format every 2 seconds to a collection platform that will process and analyze the data, providing health trending information back to the pet owners and veterinarians via a web portal.
  </h1>
  <h1 class="question">
    Management has tasked you to architect the collection platform ensuring the following requirements are met:
    <ul>
      <li>Provide the ability for real-time analytics of the inbound biometric data</li>
      <li>Ensure processing of the biometric data is highly durable, elastic, and parallel</li>
      <li>The results of the analytic processing should be persisted for data mining</li>
    </ul>
    Which architecture outlined below will meet the initial requirements for the collection platform?
  </h1>

  <label for="optionA">
    A. Utilize S3 to collect the inbound sensor data, analyze the data from S3 with a daily scheduled Data Pipeline, and save the results to a Redshift cluster.
  </label><br>

  <label for="optionB" class="correct-answer">
    B. Utilize Amazon Kinesis to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to a Redshift cluster using EMR. &#x2713;
  </label><br>

  <label for="optionC">
    C. Utilize SQS to collect the inbound sensor data, analyze the data from SQS with Amazon Kinesis, and save the results to a Microsoft SQL Server RDS instance.
  </label><br>

  <label for="optionD">
    D. Utilize EMR to collect the inbound sensor data, analyze the data from EMR with Amazon Kinesis, and save the results to DynamoDB.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    You are designing Internet connectivity for your VPC. The web servers must be available on the Internet.
  </h1>
  <h1 class="question">
    The application must have a highly available architecture.
  </h1>
  <h1 class="question">
    Which alternatives should you consider? (Choose two.)
  </h1>

  <label for="optionA">
    A. Configure a NAT instance in your VPC. Create a default route via the NAT instance and associate it with all subnets. Configure a DNS A record that points to the NAT instance public IP address.
  </label><br>

  <label for="optionB">
    B. Configure a CloudFront distribution and configure the origin to point to the private IP addresses of your web servers. Configure a Route 53 CNAME record to your CloudFront distribution.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Place all your web servers behind ELB. Configure a Route 53 CNAME to point to the ELB DNS name. &#x2713;
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Assign EIPs to all web servers. Configure a Route 53 record set with all EIPs, with health checks and DNS failover. &#x2713;
  </label><br>

  <label for="optionE">
    E. Configure ELB with an EIP. Place all your web servers behind ELB. Configure a Route 53 A record that points to the EIP.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    Your team has a Tomcat-based Java application you need to deploy into development, test, and production environments. After some research, you opt to use
    Elastic Beanstalk due to its tight integration with your developer tools and RDS due to its ease of management.
  </h1>
  <h1 class="question">
    Your QA team lead points out that you need to roll a sanitized set of production data into your environment on a nightly basis. Similarly, other software teams in your organization want access to that same restored data via their EC2 instances in your VPC.
  </h1>
  <h1 class="question">
    The optimal setup for persistence and security that meets the above requirements would be the following:
  </h1>

  <label for="optionA">
    A. Create your RDS instance as part of your Elastic Beanstalk definition and alter its security group to allow access to it from hosts in your application subnets.
  </label><br>

  <label for="optionB">
    B. Create your RDS instance separately and add its IP address to your application's DB connection strings in your code. Alter its security group to allow access to it from hosts within your VPC's IP address block.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Create your RDS instance separately and pass its DNS name to your app's DB connection string as an environment variable. Create a security group for client machines and add it as a valid source for DB traffic to the security group of the RDS instance itself. &#x2713;
  </label><br>

  <label for="optionD">
    D. Create your RDS instance separately and pass its DNS name to your app's DB connection string as an environment variable. Alter its security group to allow access to it from hosts in your application subnets.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    You're trying to delete an SSL certificate from the IAM certificate store, and you're getting the message:
    <br><br>
    "<code>Certificate: &lt;certificate-id&gt; is being used by CloudFront.</code>"
  </h1>
  <h1 class="question">
    Which of the following statements is probably the reason why you are getting this error?
  </h1>

  <label for="optionA">
    A. Before you can delete an SSL certificate you need to set up HTTPS on your server.
  </label><br>

  <label for="optionB">
    B. Before you can delete an SSL certificate, you need to set up the appropriate access level in IAM.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Before you can delete an SSL certificate, you need to either rotate SSL certificates or revert from using a custom SSL certificate to using the default CloudFront certificate. &#x2713;
  </label><br>

  <label for="optionD">
    D. You can't delete SSL certificates. You need to request it from AWS.
  </label><br>
</form>
    <form class="question-form">
  <h1 class="question">
    A company is migrating to the cloud. It wants to evaluate the configurations of virtual machines in its existing data center environment to ensure that it can size new Amazon EC2 instances accurately. 
    <br><br>
    The company wants to collect metrics, such as CPU, memory, and disk utilization, and it needs an inventory of what processes are running on each instance. 
    <br><br>
    The company would also like to monitor network connections to map communications between servers.
    <br><br>
    Which would enable the collection of this data MOST cost effectively?
  </h1>

  <label for="optionA" class="correct-answer">
    A. Use AWS Application Discovery Service and deploy the data collection agent to each virtual machine in the data center. &#x2713;
  </label><br>

  <label for="optionB">
    B. Configure the Amazon CloudWatch agent on all servers within the local environment and publish metrics to Amazon CloudWatch Logs.
  </label><br>

  <label for="optionC">
    C. Use AWS Application Discovery Service and enable agentless discovery in the existing virtualization environment.
  </label><br>

  <label for="optionD">
    D. Enable AWS Application Discovery Service in the AWS Management Console and configure the corporate firewall to allow scans over a VPN.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    The _____ service is targeted at organizations with multiple users or systems that use AWS products such as Amazon EC2, Amazon SimpleDB, and the AWS Management Console.
  </h1>

  <label for="optionA">
    A. Amazon RDS
  </label><br>

  <label for="optionB">
    B. AWS Integrity Management
  </label><br>

  <label for="optionC" class="correct-answer">
    C. AWS Identity and Access Management &#x2713;
  </label><br>

  <label for="optionD">
    D. Amazon EMR
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    To get started using AWS Direct Connect, in which of the following steps do you configure Border Gateway Protocol (BGP)?
  </h1>

  <label for="optionA">
    A. Complete the Cross Connect
  </label><br>

  <label for="optionB">
    B. Configure Redundant Connections with AWS Direct Connect
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Create a Virtual Interface &#x2713;
  </label><br>

  <label for="optionD">
    D. Download Router Configuration
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A company wants to allow its Marketing team to perform SQL queries on customer records to identify market segments. The data is spread across hundreds of files. The records must be encrypted in transit and at rest. The Team Manager must have the ability to manage users and groups, but no team members should have access to services or resources not required for the SQL queries. Additionally, Administrators need to audit the queries made and receive notifications when a query violates rules defined by the Security team.
  </h1>
  <h1 class="question">
    AWS Organizations has been used to create a new account and an AWS IAM user with administrator permissions for the Team Manager. Which design meets these requirements?
  </h1>

  <label for="optionA">
    A. Apply a service control policy (SCP) that denies to all services except IAM, Amazon DynamoDB, and AWS CloudTrail. Store customer records in DynamoDB and train users to execute queries using the AWS CLI. Enable DynamoDB streams to track the queries that are issued and use an AWS Lambda function for real-time monitoring and alerting.
  </label><br>

  <label for="optionB">
    B. Apply a service control policy (SCP) that allows to IAM, Amazon Athena, Amazon S3, and AWS CloudTrail. Store customer records as files in Amazon S3 and train users to leverage the Amazon S3 Select feature and execute queries using the AWS CLI. Enable S3 object-level logging and analyze CloudTrail events to audit and alarm on queries against personal data.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Apply a service control policy (SCP) that denies access to all services except IAM, Amazon Athena, Amazon S3, and AWS CloudTrail. Store customer record files in Amazon S3 and train users to execute queries using the CLI via Athena. Analyze CloudTrail events to audit and alarm on queries against personal data. &#x2713;
  </label><br>

  <label for="optionD">
    D. Apply a service control policy (SCP) that allows access to IAM, Amazon RDS, and AWS CloudTrail. Load customer records in Amazon RDS MySQL and train users to execute queries using the AWS CLI. Stream the query logs to Amazon CloudWatch Logs from the RDS database instance. Use a subscription filter with AWS Lambda functions to audit and alarm on queries against personal data.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    Which of the following is NOT an advantage of using AWS Direct Connect?
  </h1>

  <label for="optionA" class="correct-answer">
    A. AWS Direct Connect provides users access to public and private resources by using two different connections while maintaining network separation between the public and private environments. &#x2713;
  </label><br>

  <label for="optionB">
    B. AWS Direct Connect provides a more consistent network experience than Internet-based connections.
  </label><br>

  <label for="optionC">
    C. AWS Direct Connect makes it easy to establish a dedicated network connection from your premises to AWS.
  </label><br>

  <label for="optionD">
    D. AWS Direct Connect reduces your network costs.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    You create a VPN connection, and your VPN device supports Border Gateway Protocol (BGP).  
    Which of the following should be specified to configure the VPN connection?
  </h1>

  <label for="optionA">
    A. Classless routing
  </label><br>

  <label for="optionB">
    B. Classfull routing
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Dynamic routing &#x2713;
  </label><br>

  <label for="optionD">
    D. Static routing
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A company runs a three-tier application in AWS. Users report that the application performance can vary greatly depending on the time of day and functionality being accessed. The application includes the following components:
    <ul>
      <li>Eight t2.large front-end web servers that serve static content and proxy dynamic content from the application tier.</li>
      <li>Four t2.large application servers.</li>
      <li>One db.m4.large Amazon RDS MySQL Multi-AZ DB instance.</li>
    </ul>
    Operations has determined that the web and application tiers are network constrained.  
    Which of the following should cost effectively improve application performance? (Choose two.)
  </h1>

  <label for="optionA">
    A. Replace web and app tiers with t2.xlarge instances
  </label><br>

  <label for="optionB" class="correct-answer">
    B. Use AWS Auto Scaling and m4.large instances for the web and application tiers &#x2713;
  </label><br>

  <label for="optionC">
    C. Convert the MySQL RDS instance to a self-managed MySQL cluster on Amazon EC2
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Create an Amazon CloudFront distribution to cache content &#x2713;
  </label><br>

  <label for="optionE">
    E. Increase the size of the Amazon RDS instance to db.m4.xlarge
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A large company has increased its utilization of AWS over time in an unmanaged way. As such, they have a large number of independent AWS accounts across different business units, projects, and environments. The company has created a Cloud Center of Excellence team, which is responsible for managing all aspects of the AWS Cloud, including their AWS accounts.  
    Which of the following should the Cloud Center of Excellence team do to BEST address their requirements in a centralized way? (Select two.)
  </h1>

  <label for="optionA">
    A. Control all AWS account root user credentials. Assign AWS IAM users in the account of each user who needs to access AWS resources. Follow the policy of least privilege in assigning permissions to each user.
  </label><br>

  <label for="optionB">
    B. Tag all AWS resources with details about the business unit, project, and environment. Send all AWS Cost and Usage reports to a central Amazon S3 bucket, and use tools such as Amazon Athena and Amazon QuickSight to collect billing details by business unit.
  </label><br>

  <label for="optionC">
    C. Use the AWS Marketplace to choose and deploy a Cost Management tool. Tag all AWS resources with details about the business unit, project, and environment. Send all AWS Cost and Usage reports for the AWS accounts to this tool for analysis.
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Set up AWS Organizations. Enable consolidated billing, and link all existing AWS accounts to a master billing account. Tag all AWS resources with details about the business unit, project and environment.  
    Analyze Cost and Usage reports using tools such as Amazon Athena and Amazon QuickSight to collect billing details by business unit. &#x2713;
  </label><br>

  <label for="optionE" class="correct-answer">
    E. Using a master AWS account, create IAM users within the master account. Define IAM roles in the other AWS accounts, which cover each of the required functions in the account. Follow the policy of least privilege in assigning permissions to each role, then enable the IAM users to assume the roles that they need to use. &#x2713;
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    Your website is serving on-demand training videos to your workforce. Videos are uploaded monthly in high resolution MP4 format. Your workforce is distributed globally, often on the move and using company-provided tablets that require the HTTP Live Streaming (HLS) protocol to watch a video. Your company has no video transcoding expertise and if required you may need to pay for a consultant.  
    How do you implement the most cost-efficient architecture without compromising high availability and quality of video delivery?
  </h1>

  <label for="optionA">
    A. Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. EBS volumes to host videos and EBS snapshots to incrementally backup original files after a few days. CloudFront to serve HLS transcoded videos from EC2.
  </label><br>

  <label for="optionB" class="correct-answer">
    B. Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. S3 to host videos with Lifecycle Management to archive original files to Glacier after a few days. CloudFront to serve HLS transcoded videos from S3. &#x2713;
  </label><br>

  <label for="optionC">
    C. A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. S3 to host videos with Lifecycle Management to archive all files to Glacier after a few days. CloudFront to serve HLS transcoded videos from Glacier.
  </label><br>

  <label for="optionD">
    D. A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. EBS volumes to host videos and EBS snapshots to incrementally backup original files after a few days. CloudFront to serve HLS transcoded videos from EC2.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    Your startup wants to implement an order fulfillment process for selling a personalized gadget that needs an average of 3-4 days to produce with some orders taking up to 6 months.
    You expect 10 orders per day on your first day, 1000 orders per day after 6 months and 10,000 orders after 12 months. Orders coming in are checked for consistency, then dispatched to your manufacturing plant for production, quality control, packaging, shipment and payment processing.
    If the product does not meet the quality standards at any stage of the process, employees may force the process to repeat a step. Customers are notified via email about order status and any critical issues with their orders such as payment failure.
    Your base architecture includes AWS Elastic Beanstalk for your website with an RDS MySQL instance for customer data and orders.
    How can you implement the order fulfillment process while making sure that the emails are delivered reliably?
  </h1>

  <label for="optionA">
    A. Add a business process management application to your Elastic Beanstalk app servers and re-use the RDS database for tracking order status. Use one of the Elastic Beanstalk instances to send emails to customers.
  </label><br>

  <label for="optionB" class="correct-answer">
    B. Use SWF with an Auto Scaling group of activity workers and a decider instance in another Auto Scaling group with min/max=1. Use SES to send emails to customers. &#x2713;
  </label><br>

  <label for="optionC">
    C. Use an SQS queue to manage all process tasks. Use an Auto Scaling group of EC2 instances that poll the tasks and execute them. Use SES to send emails to customers.
  </label><br>

  <label for="optionD">
    D. Use SWF with an Auto Scaling group of activity workers and a decider instance in another Auto Scaling group with min/max=1. Use the decider instance to send emails to customers.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    Your team has a Tomcat-based Java application you need to deploy into development, test, and production environments. After some research, you opt to use Elastic Beanstalk due to its tight integration with your developer tools and RDS due to its ease of management.
    Your QA team lead points out that you need to roll a sanitized set of production data into your environment on a nightly basis. Similarly, other software teams in your org want access to that same restored data via their EC2 instances in your VPC.
    The optimal setup for persistence and security that meets the above requirements would be the following:
  </h1>

  <label for="optionA">
    A. Create your RDS instance separately and add its IP address to your application's DB connection strings in your code. Alter its security group to allow access to it from hosts within your VPC's IP address block.
  </label><br>

  <label for="optionB">
    B. Create your RDS instance separately and pass its DNS name to your's DB connection string as an environment variable. Alter its security group to allow access to it from hosts in your application subnets.
  </label><br>

  <label for="optionC">
    C. Create your RDS instance as part of your Elastic Beanstalk definition and alter its security group to allow access to it from hosts in your application subnets.
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Create your RDS instance separately and pass its DNS name to your app's DB connection string as an environment variable. Create a security group for client machines and add it as a valid source for DB traffic to the security group of the RDS instance itself. &#x2713;
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    You have a website which requires international presence and consequently you have set it up as follows. It is hosted on 30 EC2 instances. It is in 15 regions around the globe. Each region has 2 instances. All the instances are in a public hosted zone.
    Which of the following is the best way to configure your site to maintain availability with minimum downtime if one of the 15 regions was to lose network connectivity for an extended period? (Choose 2 answers)
  </h1>

  <label for="optionA" class="correct-answer">
    A. Create a Route 53 Latency Based Routing Record set that resolves to an Elastic Load Balancer in each region and has the Evaluate Target Health flag set to true. &#x2713;
  </label><br>

  <label for="optionB" class="correct-answer">
    B. Create a Route 53 failover routing policy and configure an active-passive failover. &#x2713;
  </label><br>

  <label for="optionC">
    C. Create a Route 53 Failover Routing Policy and assign each resource record set a unique identifier and a relative weight.
  </label><br>

  <label for="optionD">
    D. Create a Route 53 Geolocation Routing Policy that resolves to an Elastic Load Balancer in each region and has the Evaluate Target Health flag set to false.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    You require the ability to analyze a customer's clickstream data on a website, so they can do behavioral analysis. Your customer needs to know what sequence of pages and ads their customer clicked on. This data will be used in real time to modify the page layouts as customers click through the site, to increase stickiness and advertising click-through.<br><br>
    Which option meets the requirements for capturing and analyzing this data?
  </h1>

  <label for="optionA">
    A. Log clicks in weblogs by URL, store to Amazon S3, and then analyze with Elastic MapReduce.
  </label><br>

  <label for="optionB">
    B. Publish web clicks by session to an Amazon SQS queue; then periodically drain these events to Amazon RDS and analyze with SQL.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Push web clicks by session to Amazon Kinesis, then analyze behavior using Kinesis workers. &#x2713;
  </label><br>

  <label for="optionD">
    D. Write click events directly to Amazon Redshift, and then analyze with SQL.
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A user wants to configure AutoScaling which scales up when the CPU utilization is above 70% and scales down when the CPU utilization is below 30%. How can the user configure AutoScaling for the above mentioned condition?
  </h1>

  <label for="optionA">
    A. Configure ELB to notify AutoScaling on load increase or decrease
  </label><br>

  <label for="optionB">
    B. Use AutoScaling with a schedule
  </label><br>

  <label for="optionC">
    C. Use AutoScaling by manually modifying the desired capacity during a condition
  </label><br>

  <label for="optionD" class="correct-answer">
    D. Use dynamic AutoScaling with a policy &#x2713;
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    You want to use AWS CodeDeploy to deploy an application to Amazon EC2 instances running within an Amazon Virtual Private Cloud (VPC).<br>
    What criterion must be met for this to be possible?
  </h1>

  <label for="optionA">
    A. The AWS CodeDeploy agent installed on the Amazon EC2 instances must be able to access only the public AWS CodeDeploy endpoint.
  </label><br>

  <label for="optionB">
    B. The AWS CodeDeploy agent installed on the Amazon EC2 instances must be able to access only the public Amazon S3 service endpoint.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. The AWS CodeDeploy agent installed on the Amazon EC2 instances must be able to access the public AWS CodeDeploy and Amazon S3 service endpoints &#x2713;
  </label><br>

  <label for="optionD">
    D. It is not currently possible to use AWS CodeDeploy to deploy an application to Amazon EC2 instances running within an Amazon Virtual Private Cloud (VPC.)
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    You have subscribed to the AWS Business and Enterprise support plan.<br>
    Your business has a backlog of problems, and you need about 20 of your IAM users to open technical support cases.<br>
    How many users can open technical support cases under the AWS Business and Enterprise support plan?
  </h1>

  <label for="optionA">
    A. 5 users
  </label><br>

  <label for="optionB">
    B. 10 users
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Unlimited &#x2713;
  </label><br>

  <label for="optionD">
    D. 1 user
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    A user has created a VPC with two subnets: one public and one private.<br>
    The user is planning to run the patch update for the instances in the private subnet.<br>
    How can the instances in the private subnet connect to the internet?
  </h1>

  <label for="optionA">
    A. The private subnet can never connect to the internet
  </label><br>

  <label for="optionB" class="correct-answer">
    B. Use NAT with an elastic IP &#x2713;
  </label><br>

  <label for="optionC">
    C. Use the internet gateway with a private IP
  </label><br>

  <label for="optionD">
    D. Allow outbound traffic in the security group for port 80 to allow internet updates
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    When I/O performance is more important than fault tolerance, which of the following configurations should be used?
  </h1>

  <label for="optionA">
    A. SPAN 10
  </label><br>

  <label for="optionB">
    B. RAID 1
  </label><br>

  <label for="optionC" class="correct-answer">
    C. RAID 0 &#x2713;
  </label><br>

  <label for="optionD">
    D. NFS 1
  </label><br>
</form>
<form class="question-form">
  <h1 class="question">
    An organization is setting up an application on AWS to have both High Availability (HA) and Disaster Recovery (DR). The organization wants to have both Recovery Point Objective (RPO) and Recovery Time Objective (RTO) of 10 minutes. Which of the below mentioned service configurations does <strong>not</strong> help the organization achieve the said RPO and RTO?
  </h1>

  <label for="optionA">
    A. Take a snapshot of the data every 10 minutes and copy it to the other region.
  </label><br>

  <label for="optionB">
    B. Use an elastic IP to assign to a running instance and use Route 53 to map the user's domain with that IP.
  </label><br>

  <label for="optionC" class="correct-answer">
    C. Create ELB with multi-region routing to allow automated failover when required. &#x2713;
  </label><br>

  <label for="optionD">
    D. Use an AMI copy to keep the AMI available in other regions.
  </label><br>
</form>


  </div>
  <button class="scroll-top" id="scrollTop">↑</button>

  <!-- ========================================================= -->
  <!-- ===== START OF THE NEW, CORRECTED JAVASCRIPT SCRIPT ===== -->
  <!-- ========================================================= -->
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      
      // --- DEDUPLICATION LOGIC ---
      const seenQuestions = new Map();
      
      // This selector finds ALL possible question containers, whether they are
      // a .question-block div or a .question-form that is NOT inside a .question-block.
      const allQuestionContainers = document.querySelectorAll(".question-block, form.question-form:not(.question-block .question-form)");

      allQuestionContainers.forEach(container => {
        const questionElement = container.querySelector(".question");
        if (!questionElement) return; // Skip if no question text is found

        const questionText = questionElement.textContent.trim();

        if (seenQuestions.has(questionText)) {
          container.remove(); // Remove the duplicate container
        } else {
          seenQuestions.set(questionText, container); // Store the first one we see
        }
      });


      // --- SCROLL TO TOP BUTTON LOGIC ---
      const scrollTopBtn = document.getElementById('scrollTop');
      window.addEventListener('scroll', function () {
        scrollTopBtn.style.display = (window.pageYOffset > 300) ? 'block' : 'none';
      });
      scrollTopBtn.addEventListener('click', function () {
        window.scrollTo({ top: 0, behavior: 'smooth' });
      });


      // --- SEARCH LOGIC ---
      const searchInput = document.getElementById('search');
      const clearSearchBtn = document.getElementById('clear-search');
      
      // Get the list of unique containers that survived deduplication
      const finalQuestionContainers = Array.from(seenQuestions.values());

      function performSearch() {
        const query = searchInput.value.toLowerCase().trim();
        finalQuestionContainers.forEach(container => {
          const questionText = container.querySelector('.question').textContent.toLowerCase();
          
          // Show if the text includes the query, hide otherwise
          container.style.display = questionText.includes(query) ? 'block' : 'none';
        });
      }

      searchInput.addEventListener('input', performSearch);

      clearSearchBtn.addEventListener('click', function () {
        searchInput.value = '';
        performSearch(); // Re-run search with empty query to show all
        searchInput.focus();
      });
    });
  </script>
  <!-- ======================================================= -->
  <!-- ===== END OF THE NEW, CORRECTED JAVASCRIPT SCRIPT ===== -->
  <!-- ======================================================= -->
</body>

</html>